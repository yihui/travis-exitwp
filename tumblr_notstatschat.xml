<?xml version="1.0" encoding="UTF-8" ?>
<!-- This is a WordPress eXtended RSS file generated from your Tumblr posts. -->
<!-- It contains information about your blog's posts, comments, and categories. -->
<!-- You may use this file to transfer that content from one site to another. -->
<!-- This file is not intended to serve as a complete backup of your blog. -->

<!-- To import this information into a WordPress blog follow these steps. -->
<!-- 1. Log into that blog as an administrator. -->
<!-- 2. Go to Manage: Import in the blog's admin panels. -->
<!-- 3. Choose "WordPress" from the list. -->
<!-- 4. Upload this file using the form provided on that page. -->
<!-- 5. You will first be asked to map the authors in this export file to users -->
<!--    on the blog.  For each author, you may choose to map to an -->
<!--    existing user on the blog or to create a new user -->
<!-- 6. WordPress will then import each of the posts, comments, and categories -->
<!--    contained in this file into your blog -->

<!-- generator="Tumblr2WordPress/0.4" created="2018-02-02 13:53"-->
<rss version="2.0"
  xmlns:content="http://purl.org/rss/1.0/modules/content/"
  xmlns:wfw="http://wellformedweb.org/CommentAPI/"
  xmlns:dc="http://purl.org/dc/elements/1.1/"
  xmlns:wp="http://wordpress.org/export/1.0/"
>
<channel>
  <title>Biased and Inefficient</title>
  <link>http://notstatschat.tumblr.com/</link>
  <description>I'm a statistical researcher in Auckland. This blog is for things that don't fit on my department's blog, &lt;a href=&quot;http://www.statschat.org.nz&quot;&gt;StatsChat.&lt;/a&gt; 
I also tweet as &lt;a href=&quot;https://twitter.com/tslumley&quot;&gt;@tslumley&lt;/a&gt;</description>
  <pubDate>Fri, 02 Feb 2018 13:53:36 -0800</pubDate>
  <generator>http://Tumblr2WordPress/0.4(tumblr2wordpress.benapps.net)</generator>
  <language>en</language>
  <wp:wxr_version>1.0</wp:wxr_version>
  <wp:base_site_url>http://notstatschat.tumblr.com/</wp:base_site_url>
  <wp:base_blog_url>http://notstatschat.tumblr.com/</wp:base_blog_url>
  <wp:category>
    <wp:category_nicename>uncategorized</wp:category_nicename>
    <wp:category_parent></wp:category_parent>
    <wp:cat_name><![CDATA[Uncategorized]]></wp:cat_name>
  </wp:category>

	<wp:tag><wp:tag_slug>edgelering</wp:tag_slug><wp:tag_name><![CDATA[edgelering]]></wp:tag_name></wp:tag>
	<wp:tag><wp:tag_slug>silly-tereo-conservation</wp:tag_slug><wp:tag_name><![CDATA[silly tereo conservation]]></wp:tag_name></wp:tag>
	<wp:tag><wp:tag_slug>dfa</wp:tag_slug><wp:tag_name><![CDATA[dfa]]></wp:tag_name></wp:tag>
  <item>
    <link>http://notstatschat.tumblr.com/post/170353005206</link>
    <pubDate>Thu, 01 Feb 2018 09:18:22 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/170353005206</guid>
    <!--<wp:post_id>170353005206</wp:post_id>-->
    <wp:post_date>2018-01-31 12:18:22</wp:post_date>
    <wp:post_date_gmt>2018-01-31 20:18:22</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Useful debugging trick</title>
    <description></description>
    <content:encoded><![CDATA[<p>If you have a thing with lots of indices, such as a fourth-order sampling probability $\pi_{ijk\ell}$ (the probability that individuals $i$, $j$, $k$ and $\ell$ are all sampled), there will likely be scenarios where it has lots and lots of symmetries.&nbsp;</p><p>A useful trick is to write a wrapper that checks them:</p><blockquote><p>FourPi&lt;-function(i,j,k,l){<br>&nbsp; &nbsp;answer &lt;- FourPiInternal(i,j,k,l)<br>&nbsp; &nbsp;sym &lt;- FourPiInternal(j,i,k,l)<br>&nbsp; &nbsp;if (abs((answer-sym)/(answer+sym))&gt;EPSILON) stop(paste(i,j,k,l))<br>&nbsp; &nbsp;answer<br>}</p></blockquote><p>Other useful tricks:</p><ul><li>The score (deriviative of loglikelihood) has mean zero at the true parameters under sampling from the model, even in finite samples</li><li>Quite a few design-based variance estimators are unbiased for the sampling variance even in small samples.&nbsp;</li><li>Many (but not all) variance estimators should be positive semidefinite even in small samples.</li><li>If you have two estimators of the same thing, do a scatterplot of them or of their estimating functions.</li></ul><p>More generally, properties of estimating functions are often easier to check in small samples than properties of the estimators.&nbsp; This is especially useful when you have an estimator that takes $\Omega\left(M^2N^2\right)$ time for large $N$ and moderate $M$, so you can’t just scale up and use asymptotics.&nbsp; If the computation time isn’t $O(N)$ or near offer, tests you can do with small samples are enormously useful&nbsp;</p>]]></content:encoded>
    <wp:post_name>170353005206</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/170018017986</link>
    <pubDate>Tue, 23 Jan 2018 13:21:08 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/170018017986</guid>
    <!--<wp:post_id>170018017986</wp:post_id>-->
    <wp:post_date>2018-01-22 16:21:08</wp:post_date>
    <wp:post_date_gmt>2018-01-23 00:21:08</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>The Ihaka Lectures</title>
    <description></description>
    <content:encoded><![CDATA[<p>They’re back!</p><p>This year our theme is visualisation. The lectures will again run on Wednesday evenings in March. The three speakers work in different areas of data visualisation: collect the complete set!</p><p><a href="https://github.com/pmur002">Paul Murrell</a> is an Associate Professor in Statistics here in Auckland. He’s a member of the R Core Development Team, and responsible for a lot of graphics infrastructure in R. The&nbsp;‘grid’ graphics system grew out of his PhD thesis with Ross Ihaka.&nbsp;</p><p><a href="http://www.dicook.org/">Di Cook</a> is Professor of Business Analytics at Monash University, and was previously a Professor at Iowa State University. She’s an expert in visualisation for data analysis: using graphics, especially interactive and dynamic graphics, to learn from data.&nbsp;</p><p><a href="http://www.thefunctionalart.com/">Alberto Cairo</a> is the Knight Chair in Visual Journalism at the University of Miami. His focus is in data visualisation for communication, especially with non-specialists.&nbsp;&nbsp;He led the creation of the Interactive Infographics Department at El Mundo (<b><a href="http://elmundo.es/">elmundo.es</a></b>, Spain), in 2000.</p><p>More details to follow.</p>]]></content:encoded>
    <wp:post_name>170018017986</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/170016876851</link>
    <pubDate>Tue, 23 Jan 2018 12:43:59 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/170016876851</guid>
    <!--<wp:post_id>170016876851</wp:post_id>-->
    <wp:post_date>2018-01-22 15:43:59</wp:post_date>
    <wp:post_date_gmt>2018-01-22 23:43:59</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>More tests for survey data</title>
    <description></description>
    <content:encoded><![CDATA[<p>If you know about design-based analysis of survey data, you probably know about the Rao-Scott tests, at least in contingency tables.&nbsp; The tests started off in the 1980s as&nbsp;“ok, people are going to keep doing Pearson $X^2$ tests on estimated population tables, can we work out how to get $p$-values that aren’t ludicrous?” Subsequently, they turned out to have better operating characteristics than the Wald-type tests that were the obvious thing to do -- mostly by accident.&nbsp; Finally, they’ve been adapted to regression models in general, and reinterpreted as tests in a marginal working model of independent sampling, where they are distinctive in that they weight different directions of departure from the null in a way that doesn’t depend on the sampling design.&nbsp;</p><p>The Rao--Scott test statistics are asymptotically equivalent to $(\hat\beta-\beta_0)^TV_0^{-1}(\hat\beta-\beta_0)$, where $\hat\beta$ is the estimate of $\beta_0$, and $V_0$ is the variance matrix you’d get with full population data. The standard Wald tests are targetting&nbsp; $(\hat\beta-\beta_0)^TV^{-1}(\hat\beta-\beta_0)$, where $V$ is the actual variance matrix of $\hat\beta$.&nbsp; One reason the Rao--Scott score and likelihood ratio tests work better in small samples is just that score and likelihood ratio tests seem to work better in small samples than Wald tests. But there’s another reason.&nbsp;</p><p>The actual Wald-type test statistic (up to degree-of-freedom adjustments) is&nbsp;$(\hat\beta-\beta_0)^T\hat V^{-1}(\hat\beta-\beta_0)$. In small samples $\hat V$ is often poorly estimated, and in particular its condition number is, on average, larger than the condition number of $V$, so its inverse is wobblier. The Rao--Scott tests obviously can’t avoid this problem completely: $\hat V$ must be involved somewhere. However, they use $\hat V$ via the eigenvalues of $\hat V_0^{-1}\hat V$; in the original Satterthwaite approximation, the mean and variance of these eigenvalues.&nbsp; In the typical survey settings, $V_0$ is fairly well estimated, so inverting it isn’t a problem. The fact that $\hat V$ is more ill-conditioned than $V$ translates as fewer degrees of freedom for the Satterthwaite approximation, and so to a more conservative test.&nbsp; This conservative bias happens to cancel out a lot of the anticonservative bias and the tests work relatively well.&nbsp;&nbsp;</p><p>Here’s an example of qqplots of $-\log_{10} p$-values simulated&nbsp; in a Cox model: the Wald test is the top panel and the Rao--Scott LRT is the bottom panel. The clusters are of size 100; the orange tests use the design degrees of freedom minus the number of parameters as the denominator degrees of freedom in an $F$ test.&nbsp;</p><figure class="tmblr-full" data-orig-height="1462" data-orig-width="1732"><img src="https://78.media.tumblr.com/f9970ca528f7ed8067b48b9f781f8ab6/tumblr_inline_p2zaz9LnDD1s1hdxy_540.png" data-orig-height="1462" data-orig-width="1732"></figure><p>So, what’s new? SUDAAN has tests they call&nbsp;“Satterthwaite Adjusted Wald Tests”, which are based on $(\hat\beta-\beta_0)^T\hat V_0^{-1} (\hat\beta-\beta_0)$.&nbsp; I’ve added similar tests to version 3.33 of the survey package (which I hope will be on CRAN soon).&nbsp; These new tests are (I think) asymptotically locally equivalent to the Rao--Scott LRT and score tests. I’d expect them to be slightly inferior in operating characteristics just based on traditional folklore about score and likelihood ratio tests being better. But you can do the simulations yourself and find out.&nbsp;</p><p>The implementation is in the regTermTest() function, and I’m calling these&nbsp;“working Wald tests” rather than&nbsp;“Satterthwaite adjusted”, because the important difference is the substitution of $V_0$ for $V$, and because I don’t even use the Satterthwaite approximation to the asymptotic distribution by default.&nbsp;</p>]]></content:encoded>
    <wp:post_name>170016876851</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/169945333636</link>
    <pubDate>Sun, 21 Jan 2018 16:39:16 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/169945333636</guid>
    <!--<wp:post_id>169945333636</wp:post_id>-->
    <wp:post_date>2018-01-20 19:39:16</wp:post_date>
    <wp:post_date_gmt>2018-01-21 03:39:16</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>As far as it goes</title>
    <description></description>
    <content:encoded><![CDATA[<p>I’ve been reading two somewhat depressing documents today.</p><p>The American Statistical Association has put out a<a href="http://www.amstat.org/asa/files/pdfs/POL-Statistics-as-a-Scientific-Discipline.pdf"> position pape</a>r titled “Overview of Statistics as a Scientific Discipline and Practical Implications for the Evaluation of Faculty Excellence“. It says, in the executive summary</p><blockquote><p>Statistics is at the same time a dynamic, stand-alone science with its own core research agenda and an inherently collaborative discipline, developing in response to scientific needs. In this sense, statistics fundamentally differs from many other domain-specific disciplines in science. This difference poses unique challenges for defining the standards by which faculty excellence is evaluated across the teaching, research, and service components.</p><p>This document strives to provide a conceptual framework and practical guidelines to facilitate such evaluations. To that end, the intended audience includes all participants in the evaluation process—provosts and deans with faculty members in statistics positions; chairs and heads of statistics, biostatistics, and non-statistics departments; and the promotion and tenure evaluation committees in academic institutions.Furthermore, this document seeks to assist statistical scientists in the negotiation of faculty positions and the articulation of their collaborative role with the subject-matter sciences.<br></p></blockquote><p>The position paper references a <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4653084/">paper</a> from a few years ago called&nbsp;“Evaluating Academic Scientists Collaborating in Team-Based Research: A Proposed Framework”. The latter is by people in biostatistics/epi (the former, rather notably, has no names from biostatistics departments).&nbsp; They both deal with the question of how to get people promoted when they do a lot of collaborative research rather than sitting in an office proving theorems.<br></p><p>Both papers are good as far as they go. But, as <a href="https://twitter.com/askdrstats/status/954773643564847104">Di Cook</a> and <a href="https://twitter.com/JennyBryan/status/954800417166082048">others</a> pointed out on Twitter, one of the places they don’t go is where most modern statistics lives. There is no mention of&nbsp;“computing”,&nbsp;“programming”, or “software”,&nbsp; “open” or&nbsp;“reproducible”. Twenty years ago, this lack would have been unfortunate but unremarkable. Today, it’s bizarre.&nbsp;</p><p>So, what should the position paper say about computing? Here’s a start:</p><ol><li>Statistical software development and publishing is also “a legitimate and essential scholarly activity” for the discipline.<br></li><li>Data analysis is part of statistics, so that research on the theory, practice, and teaching of data analysis is statistics research</li><li>Software may be important either because it contributes to the discipline of statistical computing, or because it improves the quality of research in other areas of science (or both).&nbsp;</li><li>Software often develops over time: peer review before initial publication may be neither necessary nor sufficient</li><li>Some statistical computing is crap; knowledgeable evaluation is necessary.</li><li>Recognising that expertise may be hard to find, the ASA (or other relevant bodies) should assist departments in finding reviewers to evaluate statistical computing and data science research</li><li>Department chairs should make sure expectations and criteria are clear.</li></ol><p><br></p><p><br></p><p><i>[Two clarifications:&nbsp;</i></p><p><i>for 6. A set of explicit criteria would be much better, but i think it’s easier to get agreement on particular cases than to get agreement on a set of criteria -- a workshop in 2001 didn’t manage.</i></p><p><i>for 7: what the expectations and criteria are, not what they should be-- if your department is not going to promote someone without an Annals paper, you need to know that]</i></p>]]></content:encoded>
    <wp:post_name>169945333636</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/169761989606</link>
    <pubDate>Tue, 16 Jan 2018 18:27:20 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/169761989606</guid>
    <!--<wp:post_id>169761989606</wp:post_id>-->
    <wp:post_date>2018-01-15 21:27:20</wp:post_date>
    <wp:post_date_gmt>2018-01-16 05:27:20</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>breakInNamespace</title>
    <description></description>
    <content:encoded><![CDATA[<p><i>Attention Conservation Notice: I’m putting this in a blog post in the hope it makes it easier for other people to find when they encounter the problem.&nbsp;</i></p><p>The !! and !!! quasiquotation syntax in R’s tidyverse will break if you run them through the parser and deparser. This means:</p><ol><li>Printing out the code of a function at the command line may give the wrong code<br></li><li>Functions like<b> fix()</b>, <b>fixInNamespace()</b>, and <b>edit()</b> may break functions using quasiquotation.</li><li>When you’re stepping through a function in the debugger, the code the debugger displays may be wrong</li></ol><p>I say&nbsp;“may” because it depends on your settings for saving the source code of functions. By default, the problems occur with code loaded in packages but not for code loaded by <b>source()</b>.</p><p>The symptom is that you get bizarre-looking nested calls to the ! operator. For example, from the <b>tidypredict</b> package</p><blockquote><p>reg &lt;- expr(!(!syms(reg)))</p></blockquote><p>and</p><blockquote><p>reduce(set, function(l, r) expr((!(!(!l))) * (!(!(!r)))))<br></p></blockquote><p>The output from these functions ends up with lots of exclamation marks in it, and doesn’t work.&nbsp;</p><p>Tracking this down is quite hard -- especially when you’ve been trained over the years that&nbsp;“it’s never the compiler”.&nbsp; Sometimes it is (or, rather, the interpreter).</p><p>What’s happening under the hood? The tidyeval system has a special magic interpreter to treat !! and !!! as single operators. This is possible because R does, in fact, distinguish !!x from !(!x)) at the level of the parse tree. Things break because the deparser doesn’t distinguish.</p><p>Why does the deparser not distinguish? I don’t know for sure, but I suspect it’s because the operator precedence of ! is not what many people would expect: it binds relatively loosely, so that <b>!x %in% y</b> is the same as<b> !(x %in% y)</b>. Putting in the implied parenthesis (previously) made code more readable, and it wouldn’t cause any problems under the built-in definition of ! as the logical not operator. With tidyeval, we have problems.&nbsp;</p><p>Now,&nbsp; I like quasiquotation. I’m responsible for the previous step in this direction, the <b>bquote()</b> function.&nbsp;I don’t want to get into the fight over whether !! is an abomination or a brilliantly creative work-around.</p><p>The fact that C++ overloads the bitwise left shift operator to do printing is probably a supporting argument that could be used in the fight -- but I’m not sure by which side.&nbsp; &nbsp;</p>]]></content:encoded>
    <wp:post_name>169761989606</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/169511429631</link>
    <pubDate>Wed, 10 Jan 2018 08:02:08 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/169511429631</guid>
    <!--<wp:post_id>169511429631</wp:post_id>-->
    <wp:post_date>2018-01-09 11:02:08</wp:post_date>
    <wp:post_date_gmt>2018-01-09 19:02:08</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Finding packages</title>
    <description></description>
    <content:encoded><![CDATA[<p>R, as I’ve pointed out before, has a package discovery problem.</p><p>There’s a new package, <a href="https://github.com/clauswilke/colorblindr">colorblindr</a>, which lets you see the impact of various sorts of colour-blindness on a colour palette, a very useful thing for designing good graphics. When it’s mentioned on Twitter, you see lots of people glad that such a tool is now available for R.&nbsp;</p><p><a href="https://cran.r-project.org/web/packages/dichromat/index.html">A similar tool</a> has been available on CRAN for fifteen years.&nbsp;</p><p>Now, the new package looks to have a better representation of colour vision. The dichromat package used colour-matching experiments from the 1990s and interpolates with loess; colorblindr seems to have a unified colour model.&nbsp; &nbsp;It would be nice to see a comparison of the two packages, and maybe it would show everyone should move to the new one.&nbsp; That’s what’s happened to a lot of my contributions to R.&nbsp;Progress is good.</p><p>But my point right now is there are a lot of people excited about colorblindr who didn’t know dichromat existed. Most of these are people who would have been interested in dichromat but didn’t know about it.&nbsp;&nbsp;I haven’t exactly been quiet about the package --for example, it’s in the R courses that Ken Rice and I teach around the world, and I wrote about it for the ASA Computing and Graphics newletter.&nbsp; It’s not hard to get me to talk about color spaces; some people would say it’s hard to get me to stop.</p><p>CRAN Task Views have helped a bit with the package discovery problem -- but dichromat is in the Graphics task view. It helps if a popular package suggests yours -- but dichromat is a dependency of a dependency of ggplot2. It helps if the package author is reasonably well known or the package was on CRAN early -- but again. It would help to give a clearer name, perhaps?</p><p>There must be lots of people out there who would be excited to find there was a package doing just what they needed. But there isn’t an easy way for them to notice.&nbsp;<br></p>]]></content:encoded>
    <wp:post_name>169511429631</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/169135719021</link>
    <pubDate>Sun, 31 Dec 2017 16:47:17 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/169135719021</guid>
    <!--<wp:post_id>169135719021</wp:post_id>-->
    <wp:post_date>2017-12-30 19:47:17</wp:post_date>
    <wp:post_date_gmt>2017-12-31 03:47:17</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>e-bike-onomics</title>
    <description></description>
    <content:encoded><![CDATA[<p>If you own an e-bike, you get used to certain questions: how fast does it go, how often does it need to be charged, how much did it cost?</p><p>My e-bike was a bottom-end one two years ago -- I didn’t know if I’d end up using it, so I didn’t spend more than I had to. Since then, the quality has generally gone up, and so has the price. Andrew Chen, <a href="https://www.mcdp.nz/2017/12/a-guide-to-buying-e-bike.html">who just bought one</a>, says that reliable entry-level bikes are \$2500-\$3000.&nbsp;That’s quite a bit of money, but so are other commuting methods.</p><p>In the mornings, I take a train to Newmarket then ride to the University. In the evenings I ride all the way home.&nbsp; That costs me \$1.85 for a train fare. Going by bus all the way both ways costs $6.30, so I save \$4.45 by cycling.&nbsp; Since I often stop to go shopping on the way home I’d sometimes need to pay an extra fare on the bus -- and before Auckland Transport started proper transfers, shopping would always be an extra fare.</p><p>Suppose we use \$4.45/day as the benchmark. At 5 days a week --&nbsp; reasonable, because while I miss some weekdays, I do ride at weekends too -- that’s \$1157/year. Over three years that’s over&nbsp; \$3400 -- enough for an e-bike plus maintenance -- and even the battery should last substantially longer than that.</p><p>You might think I need to add the cost of electricity: I pay about 30c per kWh; a complete charge is about 1/3 kWh; I probably average less than 3 charges per week. Over three years that’s maybe \$50.&nbsp;</p><p>Now, I’ve got some important advantages here. I can often leave work earlier or later to avoid rain -- and if necessary leave my bike there overnight. Also, I’m at the very end of a train line, so I can be sure of getting my bike on the train in the morning. Still, it’s notable how quickly a bike pays off compared to bus fares -- and the comparison to University parking permit would be even more favourable. Obviously the numbers for a conventional bike would be dramatically better -- but since I’m not going to commute on a conventional bike, that’s not really relevant.&nbsp;&nbsp;</p><p>When electric bikes are expensive but cost-saving at plausible discount rates, there’s interesting potential for employers to subsidise them.&nbsp;</p>]]></content:encoded>
    <wp:post_name>169135719021</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/168977106726</link>
    <pubDate>Wed, 27 Dec 2017 12:20:21 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/168977106726</guid>
    <!--<wp:post_id>168977106726</wp:post_id>-->
    <wp:post_date>2017-12-26 15:20:21</wp:post_date>
    <wp:post_date_gmt>2017-12-26 23:20:21</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Statistics on pairs</title>
    <description></description>
    <content:encoded><![CDATA[<p><br>I'm interested in estimation for complex samples from structured data --- clustered, longitudinal, family, network --- and so I'm interested in intuition for estimating statistics of pairs, triples, etc. &nbsp; This turns out to be surprisingly hard, so I want easy examples. &nbsp;One thing I want easy examples for is the relationship between design-weighted $U$-statistics and design-weighted versions of their Hoeffding projections. That is, if you write a statistic as a sum over all pairs of observations, you can usually rewrite it as a sum of a slightly more complicated statistic over single observations, and I want to think about whether the weighting should be done before or after you rewrite the statistic. </p><p> The easiest possible $U$-statistic is the variance<br>$$V = \frac{1}{N(N-1)}\sum_{i,j=1}^N (X_i-X_j)^2$$<br>where Hoeffding projection gives the usual form<br>$$S = \frac{1}{N-1} \sum_{i=1}^N (X_i-\frac{1}{N}\sum_{j=1}^N X_j)^2.$$</p><p>These are identical, as everyone who has heard of $U$-statistics has probably been forced to prove. &nbsp;In fact<br>$$\begin{eqnarray*}<br>S&amp;=&amp; \frac{1}{N-1}\sum_{i=1}^N (X_i - \frac{1}{N}\sum_{j=1}^N X_j)^2\\\\<br>&amp;=&amp;\frac{1}{N-1}\sum_{i=1}^N \left(\frac{1}{N}\sum_{j=1}^N (X_i-X_j)\right)^2\\\\<br>&amp;=&amp;\frac{1}{N-1}\sum_{i=1}^N \left(\frac{1}{N}\sum_{j=1}^N (X_i-X_j)\right)^2\\\\<br>&amp;=&amp;\frac{1}{N(N-1)}\sum_{i,j=1}^N (X_i-X_j)^2 + \frac{1}{N(N-1)}\sum_{(i,j)\neq(k,l)}^N (X_i-X_j)(X_k-X_l)\\\\<br>\end{eqnarray*}$$<br>with the second term zero by symmetry, because the $(i,j)$ terms cancel the $(j,i)$ terms and so on.</p><p>The idea is that we can estimate $S$ from a sample by putting in sampling weights $w_i$ where $w_i^{-1}$ is the probability of $X_i$ getting sampled, because the sums are only over one index at a time. &nbsp;We get a weighted mean with another weighted mean nested inside it. &nbsp; We can reweight $V$ with pairwise sampling weights $w_{ij}$ where $w_{ij}^{-1}$ is the probability that the pair $(i,j)$ are both sampled, because the sum is over pairs. &nbsp;</p><p>Under general sampling, we'd expect the two weighted estimators to be different because one of them depends on joint sampling probabilities and the other doesn't. Unfortunately, the variance is <b>too</b> simple. For straightforward comparisons such as simple random sampling versus cluster sampling all the interesting stuff cancels out and the two estimators are the same. We do not pass `Go' and do not collect 200 intuition points. </p><p>The next simplest example is the Wilcoxon--Mann--Whitney test. </p><p></p><h2>Setup</h2>Suppose we have a finite population of pairs $(Z, G)$ , where $Z$ is numeric and $G$ is binary, and for some crazy reason we want to do a rank test for association between $Z$ and $G$. &nbsp;In fact, we don't {\bf need} to assume we want a rank test, since the test statistics will be reasonable estimators of well-defined population quantities, but to be honest the main motivation is rank tests. &nbsp;For a test to make any sense at all, we need a model for the population, and we'll start with pairs $(Z,G)$ chosen iid from some probability distribution. Later, we'll add covariates to give a bit more structure. <p></p><p>Write $N$ for the number of observations with $Z=1$ and $M$ for the numher with $Z=0$, and write $X$ and $Y$ respectively for the subvectors of $Z$. Write $\mathbb{F}$ for the empirical cdf of $X$, $\mathbb{G}$ for the empirical cdf of $Y$, and $\mathbb{H}$ for that of $Z$. &nbsp;</p><p>The Mann--Whitney $U$ statistic (suitably scaled) is<br>$$U_{\textrm{pop}} = \frac{1}{NM} \sum_{i=1}^N\sum_{j=1}^M \{X_i&gt;Y_j\}$$<br>The Wilcoxon rank-sum statistic (also scaled) is<br>$$W_{\textrm{pop}} = \frac{1}{N}\sum_{i=1}^N \mathbb{H}(X_i) -\frac{1}{M}\sum_{J=1}^M \mathbb{H}(Y_j)$$</p><p>Clearly, $U_\textrm{pop}$ is an unbiased estimator of $P(X&gt;Y)$ if a single observation is generated with $G=0$ and $G=1$. &nbsp;We can expand $\mathbb{H}$ in terms of pairs of observations:<br>$$\mathbb{H}(x) = \frac{1}{M+N}\left(\sum_{i=1}^N \{X_i\leq x\} + \sum_{j=1}^M \{Y_j\leq x\}\right)$$<br>and substitute to get <br>$$\begin{eqnarray*}<br>W_{\textrm{pop}} &amp;= &amp;\frac{1}{N}\sum_{i=1}^N \frac{1}{M+N}\left(\sum_{i'=1}^N \{X_{i'}\leq X_i\} + \sum_{j'=1}^M \{Y_{j'}\leq X_i\}\right) \\\\<br>&amp; &amp;\qquad - \frac{1}{M}\sum_{j=1}^M \frac{1}{M+N}\left(\sum_{i'=1}^N \{X_{i'}\leq Y_j\} + \sum_{j'=1}^M \{Y_{j'}\leq Y_j\}\right)\\\\<br>&amp;=&amp; \frac{1}{N(M+N)} \sum_{i=1}^N\sum_{j'=1}^M &nbsp;\{Y_{j'}\leq X_i\}-\frac{1}{M(M+N)} &nbsp;\sum_{i'=1}^N \{X_{i'}\leq Y_j\} \\\\<br>&amp;&amp;\qquad +\frac{1}{N(M+N)}\sum_{i=1}^N\sum_{i'=1}^N \{X_{i'}\leq X_i\} - \frac{1}{M(M+N)}\sum_{j=1}^M\sum_{j'=1}^M \{Y_{j'}\leq Y_j\}\\\\<br>&amp;=&amp;\frac{M+N}{NM(M+N)}\sum_{i,j} \{X_i&gt;Y_j\} + \frac{NM(M-1)/2-MN(N-1)/2}{NM(M+N)}\\\\<br>&amp;=&amp;\frac{1}{NM}\sum_{i,j} \{X_i&gt;Y_j\} + \frac{M-N}{2(M+N)}<br>\end{eqnarray*}$$</p><p>So, &nbsp;$$U_\textrm{pop} = &nbsp;W_\textrm{pop} + \frac{M-N}{2(M+N)}$$<br>and the two tests are equivalent, as everyone already knows. Again, there's a good chance you have been forced to do this derivation, and you probably took fewer tries to get it right than I did.</p><h2>Definitions under complex sampling</h2><p>We take a sample, with known marginal sampling probabilities $p_i$ for the $X$s, $q_j$ for the $Y$s and pairwise sampling probabilities $\pi_{i,j}$. &nbsp;We write $n$ and $m$ for the number of sampled observations in each group, and relabel so that these are $i=1,\ldots,n$ and $j=1,\ldots,m$. &nbsp;We write $\hat N$ &nbsp;and $\hat M$ for the Horvitz--Thompson estimates of $N$ and $M$, and $\hat F$ for the estimate of $\mathbb{F}$ (and so on). &nbsp;That is<br>$$\hat H(z) = \frac{1}{\hat M+\hat N}\left(\sum_{i=1}^n \frac{1}{p_i} \{X_i\leq z\} + \sum_{j=1}^m \frac{1}{q_i} \{Y_j\leq z\}\right)$$</p><p>The natural estimator of $W_\textrm{pop}$ is that of <a href="https://academic.oup.com/biomet/article-abstract/100/4/831/213064">Lumley and Scott</a><br>$$\hat W = \frac{1}{\hat N}\sum_{i=1}^n \frac{1}{p_i}\hat{H}(X_i) -\frac{1}{\hat M}\sum_{J=1}^m \frac{1}{q_i}\hat{H}(Y_j)$$</p><p>A natural estimator of $U_\textrm{pop}$ is <br>$$\hat U= \frac{1}{\widehat{NM}} \sum_{i=1}^n\sum_{j=1}^m \frac{1}{\pi_{ij}}\{X_i&gt;Y_j\}$$</p><p>Now</p><ol><li>&nbsp;$\hat U$ and $\hat W$ are consistent estimators of the population values<br></li><li>&nbsp;Therefore, they are also consistent estimators of the superpopulation parameters<br></li><li>However, $\hat U$ depends explicitly on pairwise sampling probabilities and $\hat W$ does not<br></li><li>And there (hopefully) isn't enough linearity to make all the differences go away.<br></li></ol><p><br></p><h2>Expansions</h2><p>We can try to substitute the definition of $\hat H$ into the definition of $\hat W$ and expand as in the population case. &nbsp;To simplify notation I will assume that the sampling probabilities are designed or calibrated so that $N=\hat N$ and so on (or close enough that it doesn't matter).<br>$$\begin{eqnarray*}<br>\hat W &amp;= &amp;\frac{1}{N}\sum_{i=1}^n \frac{1}{p_i} \frac{1}{M+N}\left(\sum_{i'=1}^n \frac{1}{p_{i'}}\{X_{i'}\leq X_i\} + \sum_{j'=1}^m \frac{1}{q_{j'}} \{Y_{j'}\leq X_i\}\right) \\\\<br>&amp; &amp;\qquad - \frac{1}{M}\sum_{j=1}^m \frac{1}{q_{j}} \frac{1}{M+N}\left(\sum_{i'=1}^n\frac{1}{p_{i'}} \{X_{i'}\leq Y_j\} + \sum_{j'=1}^m\frac{1}{q_{j'}} \{Y_{j'}\leq Y_j\}\right)\\\\<br>&amp;=&amp; \frac{1}{N(M+N)} \sum_{i=1}^n\sum_{j'=1}^m\frac{1}{p_iq_{j'}} &nbsp;\{Y_{j'}\leq X_i\}-\frac{1}{M(M+N)} &nbsp;\sum_{i'=1}^n \sum_{j=1}^m\frac{1}{p_{i'}q_{j}} \{X_{i'}\leq Y_j\} \\\\<br>&amp;&amp;\qquad +\frac{1}{N(M+N)}\sum_{i=1}^n\sum_{i'=1}^n \frac{1}{p_{i'}p_{i}} \{X_{i'}\leq X_i\} - \frac{1}{M(M+N)}\sum_{j=1}^m\sum_{j'=1}^m \frac{1}{q_jq_{j'}} \{Y_{j'}\leq Y_j\}\\\\<br>&amp;=&amp;\frac{1}{MN}\sum_{i,j}\frac{1}{p_iq_j} \{X_i&gt;Y_j\} + \textrm{ugly expression not involving $X$ and $Y$}<br>\end{eqnarray*}$$</p><p>(The ugly expression involves the variance of the marginal sampling weights, since <br>$$2\sum_{i,i'} p_i^{-1}p_{i'}^{-1}= (\sum_i p_i^{-1})^2- 2\sum_i p_i^{-2}.$$<br>It doesn't depend on $X$ and $Y$, and it is the same in, for example, simple random sampling of individuals and simple random sampling of clusters. )</p><p>That is, the reweighted $\hat W$ is a version of $\hat U$ using the <b>product of marginal sampling probabilities </b>rather than the joint ones. &nbsp;They really are different, this time. Using $\hat W$ will give more weight to pairs within the same cluster than to pairs in different clusters. </p><p>It’s still not clear which one is preferable, eg,&nbsp; how will the power of the tests compare in various scenarios? But it’s progress.&nbsp;<br></p>]]></content:encoded>
    <wp:post_name>168977106726</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/168275023636</link>
    <pubDate>Thu, 07 Dec 2017 15:37:17 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/168275023636</guid>
    <!--<wp:post_id>168275023636</wp:post_id>-->
    <wp:post_date>2017-12-06 18:37:17</wp:post_date>
    <wp:post_date_gmt>2017-12-07 02:37:17</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>How to add chi-squareds</title>
    <description></description>
    <content:encoded><![CDATA[<p>A quadratic form in Gaussian variables has the same distribution as a linear combination of independent $\chi^2_1$ variables -- that’s obvious if the Gaussian variables are independent and the quadratic form is diagonal, and you can make that true by change of basis. The coefficients in the linear combination are the eigenvalues $\lambda_1,\dots,\lambda_m$ of $VA$, where $A$ is the matrix representing the quadratic form and $V$ is the covariance matrix of the Gaussians. I’ve <a href="http://notstatschat.tumblr.com/post/151013860496/large-quadratic-forms">written about</a> the issue of computing the eigenvalues, and how to speed this up. That still leaves you with the problem of computing tail probabilities for a linear combination of $\chi^2$s -- for $m$ at least hundreds, and perhaps thousands or tens of thousands.&nbsp;</p><p>There’s quite a bit of literature on this problem, but it mostly deals with small numbers of terms (like, say, $m=5$) and moderate p-values.&nbsp; The genetics examples involve large numbers of terms and very small p-values. So, Tong Chen did an MSc short research project with me, looking at these methods in the context of genetics. Here’s a summary of what we know (what we knew before and what he found)</p><p><b>‘Exact’ methods<br></b></p><ol><li><a href="https://www.jstor.org/stable/2347721">Farebrother</a>: based on an infinite series of $F$ densities<br></li><li><a href="https://www.jstor.org/stable/2346911">Davies</a>: based on numerical inversion of the characteristic function</li><li><a href="https://arxiv.org/abs/1208.2691">Bausch</a>: based on an algebra for sums of Gamma distributions</li></ol><p>All three of these can achieve any desired accuracy when used with infinite-precision arithmetic. Bausch’s method also has bounds on the computational effort, polynomial in the number of terms and the log of the maximum relative approximation error.</p><p>In ordinary double-precision (using Kahan summation), Bausch’s method can be accurate in the right tail for 50 or so terms. It is very inaccurate in the left tail. Achieving anything like the full theoretical accuracy of the algorithm requires multiple-precision arithmetic and seems slow compared to the alternatives. (It might be faster in Mathematica, which is what Bausch used)</p><p>Farebrother’s method and Davies’s method are usable even for a thousand terms, and achieve close to their nominal accuracy as long as the right tail probability is much larger than machine epsilon.&nbsp; Since $P(Q&gt;q)$ is computed from $1-P(Q&lt;q)$, they break down completely at right tail probabilities near machine epsilon. Farebrother’s method is faster for high precision, but only works when all the coefficients are positive.&nbsp;</p><p><b>Moment methods</b></p><p>The traditional approach is the <b>Satterthwaite</b> approximation, which approximates the distribution by $a\chi^2_d$ with $a$ and $d$ chosen to give the correct mean and variance.&nbsp; The Satterthwaite approximation is much better than you’d expect for moderate $p$-values and is computationally inexpensive: it only needs the sum and sum of squares of the eigenvalues, which can be computed more rapidly than the full eigendecomposition and can be approximated by randomised estimators.</p><p>Sadly, the Satterthwaite distribution is increasingly anti-conservative in the right tail.</p><p><a href="https://www.sciencedirect.com/science/article/pii/S0167947308005653">Liu and co-workers</a> proposed a <b>four-moment approximation</b> by a distribution of the form $a+b\chi^2_d(\nu)$, where $\nu$ is a non-centrality parameter and $a$ is an offset. This approximation is used in the R <a href="https://www.hsph.harvard.edu/skat/">SKAT</a> package. It’s a lot better than the Satterthwaite approximation, but it is still anticonservative in the right tail, even for $p$-values in the vicinity of $10^{-5}$.</p><p>The moment methods are more-or-less inevitably anticonservative in the right tail, because the extreme right tail of the linear combination is proportional to the extreme right tail of the single summand $\lambda_1\chi^2_1$ corresponding to the leading eigenvalue. (That’s how convolutions with exponential tails work.) The tail of the approximation depends on all of the eigenvalues and so is lighter.&nbsp;</p><p>Moment methods more accurate than the Satterthwaite approximation need summaries of the third or higher powers of the eigenvalues; these can’t be computed any faster than by a full eigendecomposition.</p><p><b>Saddlepoint approximation</b></p><p><a href="https://www.jstor.org/stable/2673596">Kuonen</a> derived a saddlepoint approximation to the sum. The approximation gets better as $m$ increases. Tong Chen proved it has the correct exponential rate in the extreme right tail, so that its relative error is uniformly bounded. The computational effort is linear in $m$ and is fairly small. On the downside, there’s no straightforward way to use more computation to reduce the error further.</p><p><b>Leading eigenvalue approximation</b></p><p>This <a href="https://www.biorxiv.org/content/early/2016/11/04/085639">approximates</a> $\sum_{i=1}^m\lambda_i\chi^2_1$ by a sum of $k$ terms plus a remainder<br>$$a_k\chi^2_{d_k}+\sum_{i=1}^k\lambda_i\chi^2_1$$</p><p>The remainder is the Satterthwaite approximation to the $n-k$ remaining terms; having the first $k$ terms separate is done to improve the tail approximation.&nbsp; You still need to decide how to add up these $k+1$ terms, but the issues are basically the same as with any set of $k+1$ $\chi^2$s.</p><p><b>Take-home message</b></p><ol><li>For large $m$, use the leading-eigenvalue approximation<br></li><li>For p-values much larger than machine epsilon, use the Davies or Farebrother algorithms (together with the leading-eigenvalue approximation if $m$ is large)<br></li><li>For p-values that might not be much larger than machine epsilon, use the saddlepoint approximation if its relative error (sometimes as much as 10% or so) is acceptable. There’s no need to use the leading-eigenvalue approximation if you have the full set of eigenvalues, but you might want to use it to avoid needing them all.&nbsp;<br></li><li>If you need very high accuracy for very small tail probabilities, you’ll need Bausch’s method, and multiple-precision arithmetic. With any luck you’ll still be able to use the leading-eigenvalue approximation to cut down the work.&nbsp;<br></li></ol>]]></content:encoded>
    <wp:post_name>168275023636</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/167885514256</link>
    <pubDate>Sun, 26 Nov 2017 13:37:00 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/167885514256</guid>
    <!--<wp:post_id>167885514256</wp:post_id>-->
    <wp:post_date>2017-11-25 16:37:00</wp:post_date>
    <wp:post_date_gmt>2017-11-26 00:37:00</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Secret Santa collisions</title>
    <description></description>
    <content:encoded><![CDATA[<p><i>Attention Conservation Notice: while this probability question actually came up in in real life, that’s just because I’m a nerd.</i></p><p>“Secret Santa” is a Christmas tradition for taming the gift-giving problem in offices, groups of acquaintances, etc. Instead of everyone wondering which subset of people they should give a gift to, each person is randomly assigned one recipient and has to give a gift (with a relatively low upper bound on cost) to that one person. The&nbsp;‘Secret’ part is that you don’t know who is going to give you the gift.&nbsp;</p><p>It’s not completely trivial (though it’s not that hard) to come up with a random process that assigns everyone exactly one recipient and ensures that no-one is left finding a gift for themselves.&nbsp; One procedure is to sample recipients without replacement to get a list that’s guaranteed to be one-to-one, and then just repeat the sampling until you get an allocation where no-one is assigned themselves.&nbsp;</p><p>So, a probability question: how likely is it that a random permutation will have ‘collisions’ where someone is their own Secret Santa? Or, equivalently, how many tries would you expect to need to get a working allocation? Does it depend on the number of people $n$? What about for 3600 people, as in the&nbsp;<a href="https://nzsecretsanta.nzpost.co.nz/">scheme</a> hosted by NZ Post on Twitter. &nbsp;</p><p>I knew the answer, but I didn’t know a proof, so this is a reasonably honest exploration of how you might find the answer.</p><p>First, is there some simple bound? Well, the chance that you, Dear Reader, get assigned yourself is $1/n$, so the simple Bonferroni bound is $n\times 1/n$. That doesn’t look very helpful, because we already knew 1 was an upper bound; this is a probability.&nbsp; However, we&nbsp; can recast the Bonferroni bound as the <b>expected number</b> of collisions.&nbsp; If the expected number of collisions is 1, then it’s reasonable to expect that the probability of no collisions is appreciable, and that as $n$ increases it converges to some useful number that isn’t 1 or 0.&nbsp; At this point, if you had to make a wild guess, a reasonable guess would be that the number of collisions has approximately a Poisson distribution for large $n$, so that the probability of no collisions will be approximately $e^{-1}$.</p><p>Next, try simulation. Doing $10^5$ replicates in R gives</p><figure data-orig-width="468" data-orig-height="246" class="tmblr-full"><img src="https://78.media.tumblr.com/7df8b237ccb74594a717a1448b3194cd/tumblr_inline_ozzzkswZfG1s1hdxy_540.png" alt="image" data-orig-width="468" data-orig-height="246"></figure><p>The probability is 1 if you try to do this by yourself, 1/2 if you have one friend, and converges astonishingly quickly to a fixed value.&nbsp; The red line is $1-e^{-1}$.</p><p>Now, can we prove this? The collisions aren’t independent, so we can’t quite just use a Poisson or Binomial argument. We could try the higher-order Bonferroni bounds, eg<br>$$P(\cup_i A_i)\geq \sum_i P(A_i) - \sum_{i,j} P(A_i\cap A_j).$$</p><p>The second-order bound gives $n\times 1/n-{n\choose 2}\times 1/n^2=1/2$, so we’re making progress.&nbsp; The third-order bound gives<br>$$n\times 1/n-{n\choose 2}\times 1/n^2+{n\choose 3}\times 1/n^3=1-1/2+1/6=4/6$$<br></p><p>We’re definitely making progress now, and these bounds look suspiciously like the values of the probability for $n=1,\,2,\,3$.&nbsp; Continuing the pattern, we will end up with<br>$$\sum_{k=1}^\infty (-1)^{k+1}{n\choose k} n^{-k}=\sum_{k=1}^\infty \frac{(-1)^{k+1}}{n^{-k}}$$</p><p>Comparing that to the infinite series for $\exp(x)$, it’s $1-e^{-1}$. What we haven’t got this way is whether the number of collisions is Poisson, which was where the guess originally came from. This seems to be a lot harder.&nbsp;&nbsp;</p><p>First, simulation confirms that the Poisson <b>is</b> a good approximation. That’s reassuring: it’s typically easier to prove things that are true than things that are not true.&nbsp;</p><p>I should next say that I spent quite a long time looking for&nbsp;‘coupling’ arguments to show that this assignment method gave the same large-sample distribution as some other assignment method that was obviously Poisson. I didn’t get anywhere with this, but it’s a fruitful approach for problems similar to this one. Since that didn’t work, we’re back to combinatorics.</p><p>Now, suppose we have 1 collision. That means we have an assignment of the $n-1$ other people with no collisions. So, the number of ways to have 1 collision in $n$ people is $n$ times the number of ways to have no collisions in $n-1$ people. Since the number of permutations of $n$ people is also $n$ times the number of permutations of $n-1$ people, the fraction of permutations with 1 collision in $n$ people is the same as the fraction of permutations with 0 collisions in $n-1$ people, ie, for large $n$ it’s $e^{-1}$. That agrees with the Poisson formula, so we’re definitely making progress.</p><p>If we have $k$ collisions in $n$ people then we have a set of $n-k$ people with no collisions. The number of ways we can have $n-k$ people with no collisions is about $(n-k)!e^{-1}$ and the number of ways to pick the $k$ people who have to buy themselves gifts is ${n\choose k}= n!/(k!(n-k)!)$. So, the number of ways of having $k$ collisions in $n$ is about $n!e^{-1}/k!$, and the probability of this is $e^{-1}/k!$, matching the Poisson distribution. We’ve done it!</p><p>Ok. Technically we’re not quite done, since we’d need to show the approximation error in the argument actually gets smaller with increasing $n$. But we’re done enough for me.&nbsp;</p><p>PS: If you want an easy way <b>not</b> to have to worry about collisions, just arrange the people in random order and assign each person the following person in the list (with the last person being assigned the first).</p>]]></content:encoded>
    <wp:post_name>167885514256</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/167819848006</link>
    <pubDate>Fri, 24 Nov 2017 14:57:01 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/167819848006</guid>
    <!--<wp:post_id>167819848006</wp:post_id>-->
    <wp:post_date>2017-11-23 17:57:01</wp:post_date>
    <wp:post_date_gmt>2017-11-24 01:57:01</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>When all U-shaped curves look the same to you</title>
    <description></description>
    <content:encoded><![CDATA[<p>There was (as usual) controversy about some of the NCEA maths questions this year.&nbsp; Most of the controversy was about whether they assumed knowledge that the students hadn’t been told to know, but I’m going to worry about the pseudocontext problem</p><p>One question had the set up</p><figure data-orig-width="912" data-orig-height="329" class="tmblr-full"><img src="https://78.media.tumblr.com/0b3faafaa981a2b4ba446b326be75be7/tumblr_inline_ozwfjiYTq31s1hdxy_540.png" data-orig-width="912" data-orig-height="329"></figure><p>and then (from the Herald), the question was</p><figure data-orig-width="620" data-orig-height="661" class="tmblr-full"><img src="https://78.media.tumblr.com/e6a5c7ba45a884a62a1bd950562a197f/tumblr_inline_ozwfkdgHoL1s1hdxy_540.png" data-orig-width="620" data-orig-height="661"></figure><p><br></p><p>The maths problem is fine as a quadratic equation, I suppose. But the physics is wrong and the maths isn’t how any sane person would answer the question in reality.</p><p>First, a hanging rope <b>does not</b> form a parabola -- you can tell, looking at the picture, that there’s something wrong.&nbsp; A hanging rope is steeper at the top and flatter at the bottom; the idealised physical model of a perfectly flexible rope with constant weight per unit length will form a curve called a catenary.&nbsp;</p><p>Here’s a comparison of catenary and parabola <a href="http://mathyear2013.blogspot.co.nz/2013/07/the-catenary-and-parabola.html">from a maths teaching blog</a></p><figure data-orig-width="320" data-orig-height="193" class="tmblr-full"><img src="https://78.media.tumblr.com/a17e9920ae5611bbef5025c369624bb0/tumblr_inline_ozwftimyci1s1hdxy_540.png" data-orig-width="320" data-orig-height="193"></figure><p><br></p><p>Second, if you stick a board across the bottom, the shape of the rope <b>will not stay the same</b>. The extra weight of the board will pull the rope straighter.&nbsp;</p><p>A good motivating example for a maths or stats question can be one where you learn something about the world using maths, or one where your knowledge of the world helps you with the maths.&nbsp; This question isn’t like that. It comes from a different universe.</p><p>Admittedly, good questions are hard to find. But last year’s exam had a good real-world parabola question</p><figure data-orig-width="1334" data-orig-height="632" class="tmblr-full"><img src="https://78.media.tumblr.com/7d5d6a8054afb675f52693aec87af852/tumblr_inline_ozwfj8BRUp1s1hdxy_540.png" data-orig-width="1334" data-orig-height="632"></figure><p>Unlike a hanging rope, an idealised suspension bridge does actually have a parabolic curve. And you might want to use geometry and algebra to work out a height like the one in the problem, rather than going out and measuring it.&nbsp;</p>]]></content:encoded>
    <wp:post_name>167819848006</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/167250449316</link>
    <pubDate>Wed, 08 Nov 2017 13:52:23 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/167250449316</guid>
    <!--<wp:post_id>167250449316</wp:post_id>-->
    <wp:post_date>2017-11-07 16:52:23</wp:post_date>
    <wp:post_date_gmt>2017-11-08 00:52:23</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Means of maximums</title>
    <description></description>
    <content:encoded><![CDATA[<blockquote><p>From a math point of view, it’s an interesting example of how the mean of the maximum of a set of random variables is higher than the max of the individual means -- <a href="http://andrewgelman.com/2017/10/19/think-top-batting-average-will-higher-311-pooling-point-predictions-bayesian-inference/">Andrew Gelman</a><br></p></blockquote><p>Controlling the maximum of a set of random variables is an important problem in mathematical statistics, and it’s surprising how far a comparatively crude approach can be stretched.</p><p>Suppose you have $m$ random variables $X_1$, .., $X_m$, and you want to know about the mean of the maximum.&nbsp; A very simple bound is that the whole is greater than the parts:<br>$$E[\max_i |X_i|]\leq E[\sum_i |X_i|]\leq m\max E|[X_i|].$$</p><p>That’s crude enough to be pretty useless, but suppose we looked at the squares of the $X_i$. Applying the same crude bound<br>$$E[\max_i X_i^2]\leq E[\sum_i X_i^2]\leq m\max E[X_i^2],$$<br>but now we can say<br>$$\sqrt{E[\max_i X_i^2]}\leq \sqrt{m}\max \sqrt{E[X_i^2]},$$<br>or<br>$$\|X_i\|\leq \|X_i\|_2\leq \sqrt{m}\max_i\|X_i\|_2.$$</p><p>We don’t need to to stop there:<br>$$\|X_i\|\leq \|\max_i X_i\|_4\leq \sqrt[4]{m}\max_i\|X_i\|_4$$<br>and in general<br>$$\|X_i\|\leq \|\max_i X_i\|_p\leq \sqrt[p]{m}\max_i\|X_i\|_p.$$<br><br></p><p>And it doesn’t even stop there. If we write $\psi_p(x)=\exp (\|x\|^p)$ and define a norm by<br>$$\psi_p(\|X\|_{\psi_p})=E[\psi_p(X)],$$<br>we get, eg,&nbsp;<br>$$\|X_i\|\leq \|\max_I X_i\|_{\psi_2}\leq \sqrt{\log m}\max_i\|X_i\|_{\psi_2}.$$</p><p>You still need to worry if these expectations exist, but for Normal distributions and those with lighter tails than the Normal they do.&nbsp;<br><br></p><p>These bounds are for finite sets, but they can be stretched to some infinite sets by a process called chaining.&nbsp; Suppose you have a stochastic process $X$ indexed by a set $T$ in a metric space, and you want to control $E\left[\sup|X({r})-X(s)|\right]$ over all pairs with $d(r,s)&lt;\delta$.&nbsp; Assume we have some sort of Lipschitz condition so that $E[|X({r})-Xs)|]&lt;K d(r,s)$ and (for simplicity) assume $K=1$. First, lay down a grid of points $t_{1i}$ so that for any point $s$ there is a $t_{1i}$ with&nbsp; $d(s,t_{1i})&lt;1$ and so also $E[|X(s)-X(t_{1i})|]&lt;1$. Let $N(1)$ be the number of points you needed. Now, repeat with points $t_{1/2,i}$ so that&nbsp;for any point $s$ there is a $t_{1i}$ with &nbsp;$d(s,t_{1i})&lt;1/2$, and write $N({1/2})$ for the number you needed. And so on in both directions: ...8,4,2,1,1/2 $1/4, 1/8,\dots,1/{2^k},\dots$.</p><p>For any two points $r$ and $s$ anywhere in any level of this grid, you can bound&nbsp;$E[|X({r})-Xs)|]$ by following the tree of links up to the coarsest necessary level (where the links will be of length about $\delta$) and then back down again. At each level, the number of links of length $2^{-k}$ is finite: $N(2^{-k})$, so the maximum over links of that length is bounded by $2^{-k}\sqrt{\log&nbsp;N(2^{-k})}$.&nbsp; The total path length is bounded by a multiple of $\sum_k 2^{-k}\sqrt{\log N(2^{-k})}$, with the sum taken over all links shorter than $\delta$.&nbsp;</p><p>And, if you were optimistic, you might hope that under reasonable conditions this bound for a dense grid might often transfer to a bound for the whole process. And, after a lot of painful details including genuine questions of measurability, it often does. <br><br></p><p>If we write $N(\epsilon)$ more generally for the number of points you’d need in a grid with spacing $\epsilon$, we can bound (up to a multiple) the sum by an integral<br>$$\int_0^\delta \sqrt{\log N(\epsilon)}\,d\epsilon.$$<br>Which is why integrals like that appear a lot in empirical process theory.</p>]]></content:encoded>
    <wp:post_name>167250449316</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/165788346396</link>
    <pubDate>Wed, 27 Sep 2017 19:55:17 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/165788346396</guid>
    <!--<wp:post_id>165788346396</wp:post_id>-->
    <wp:post_date>2017-09-26 23:55:17</wp:post_date>
    <wp:post_date_gmt>2017-09-27 06:55:17</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Haere mai, statistical computing folks.</title>
    <description></description>
    <content:encoded><![CDATA[<p>Later this year, Auckland is hosting the <a href="http://www.nzsa2017.com/">Asian regional meeting of the International Association for Statistical Computing</a>. &nbsp;For the benefit of conference-goers, here’s a brief introduction to the locale.&nbsp;</p><p><b>Nomenclature</b>:</p><p>The Owen G. Glenn Building (OGGB, or building 260, in university abbreviations) is named after Owen G. Glenn. He’s a New Zealand businessman and philanthropist.&nbsp;</p><p>Auckland is named after George Eden. &nbsp;The subantarctic Auckland Islands were not named after George but after his father William Eden.</p><p>New Zealand is named after the Dutch province of Zeeland; the lack of resemblance is quite striking.</p><p><strike>Formally</strike>,(Actually It’s more complicated) the country is Aotearoa New Zealand, with Māori and English names of equal status. The city has a Māori name, Tāmaki Makaurau, but its primary name is the English one. </p><p>The Māori language (te reo Māori) is fairly easy to pronounce roughly right. The consonants are the same as in Western European languages (or pinyin), except that ‘wh’ is pronounced /f/. The vowels are pure, as in Spanish or German or Italian. The bars above vowels mean they are about twice as long. There isn’t strong stress on any syllable. </p><p>People over 30 who grew up in a place with a Māori name may well use an older, anglicised pronounciation for it, but there’s been a trend away from that. In particular, weather forecasts and airport announcements will typically use something relatively close to the Māori pronounciation.</p><p><b>Mountains</b></p><p>Auckland is full of little pointy hills that look like baby volcanoes. They are baby volcanoes. One of them, <a href="http://www.maungawhau.co.nz/">Maungawhau/Mt Eden</a> is data(volcano) in R. Every few thousand years, a new one pops up at some unpredictable location in the Auckland area, erupts briefly, and then stops. There’s only a few of these volcano fields around the world — another is the (extinct) <a href="https://volcanoes.usgs.gov/observatories/cvo/cvo_boring.html">Boring Volcano Field</a> in Portland, Oregon. The Auckland one is still active and so is less boring.</p><p>The most recent and largest volcano, <a href="http://www.doc.govt.nz/rangitoto">Rangitoto</a>, is just outside the Waitemata Harbour. There are ferry rides a few times a day, and it’s a nice walk to the top. Parts of Rangitoto are still bare rock, parts are pohutukawa forest, and there’s some areas on the south side that have developed proper soil and a variety of plants. </p><p>Auckland Domain, just across the motorway from the conference, is the crater of the closest volcano; Mt Eden is a short bus ride away. </p><p><b>Peoples</b></p><p>New Zealand was the last worthwhile land mass to be settled — about 800 years ago, by Polynesians in big ocean-going canoes. You occasionally see people raising alt-theories of earlier settlement by, eg, Celts, but there’s scientific consensus and fairly wide social endorsement for the view that these people are probably racist whackjobs. </p><p>The British arrived in increasing numbers in the early nineteenth century, with the usual consequences — though the <a href="https://www.waitangitribunal.govt.nz/treaty-of-waitangi/">Treaty of Waitangi</a> was somewhat more successful than most attempts to negotiate with the British. Recently, the NZ government has settled treaty claims with many iwi (tribes, clans). </p><p>At the start of the twentieth century, about one in four residents of New Zealand was an immigrant. The proportion decreased to a minimum of about one in six in the 1940s and has been slowly increasing again. What’s different this time is where the immigrants are from: many are from the Pacific Islands and from Asia. &nbsp;Auckland, in particular, has about 40% &nbsp;immigrant proportion, &nbsp;similar to New York and London. The increase in diversity has gone reasonably well by international standards, but there are certainly some people who aren’t happy with things being different from fifty years ago. &nbsp;</p><p><b>Plants</b></p><p>The trees with dense, gray-green leaves are <a href="https://www.google.co.nz/search?q=pohutukawa&amp;source=lnms&amp;tbm=isch">pohutukawa</a>. Some of them might be flowering by the time of the conference. &nbsp;Stylised versions of the red spiky puffs of flowers are starting to displace winter-based symbols for Christmas in Auckland. You’ll probably hear people worrying about myrtle rust, a South American fungus that has recently arrived; no-one knows how much damage it will do.</p><p>Many of the conifers you see are native: rīmu, tōtara, kauri, kahikatea (native plants are typically known by their Māori names). The things like enormous fake Christmas trees are Araucarias; not native but regional — A. heterophylla, ‘Norfolk Pine’ from Norfolk Island and A. columnaris, ‘Cook Pine’, from New Caledonia. &nbsp;There are also two conifers from the Monterey area of California: “radiata” (Pinus radiata) and “macrocarpa” (Cupressus macrocarpa). They grow much more vigorously here. </p><p>The Dr Seuss trees looking like bunches of grass on top of tall trunks are Cabbage Trees (Cordyline australis). The name comes from the edibility of the new stem and the roots, rather than their appearance.</p><p>Tree ferns are native; the Waitakere hills to the west of Auckland are packed full of them. They’re culturally important: the major women’s professional sports teams are named after them, and the unfolding fern frond (the ‘koru’) is a widely-used symbol of growth.</p><p>Kauri are massively huge living-fossil conifers that used to be common in Auckland and points north. Sadly, a lot of the nearby ones were turned into houses, and they grow slowly. Some of the ones on the west side of Northland (day-trip distance) are almost as big as redwoods (Sequoiadendron). </p><p>New Zealand Flax is known and loved and/or hated by gardeners around the warm temperate world. It was a traditional fibre source, and the nectar was used as a sweetener. &nbsp;It’s not related to the `true’ flax of the northern hemisphere; it’s a lily. </p><p><b>Birds</b></p><p>New Zealand is famous for its weird native birds. The ones you see around you in Auckland mostly aren’t them. &nbsp;You can easily see a lot of stupidly-introduced English birds: sparrow, starling, pigeon, blackbird, thrush, chaffinch, goldfinch. The cute urban parrots are Australian, as are the magpies and the tiny green silvereyes. The leggy blue and black pūkeko are ‘courtesy natives’ — they arrived before Europeans but after Māori — but they are the same species as the ones all over Europe and Asia. The large black gulls actually are a native species, but the differences would only matter to another gull. </p><p>You might, in the parks near the University, see the kererū, the big native pigeon. It’s about twice the size of the feral pigeons, and colored purple, green, and white. &nbsp;There’s a few fantails (pīwakawaka) around, which are very cute. </p><p>There’s one common, distinctive, native bird. If you walk past a tree that sounds as if it’s full of old 28k dial-up modems, you have met the tūī. They’re about the size of a blackbird, with a puff of white feathers at the throat, and they’re boisterous, musical, and give the impression of being slightly drunk. </p><p>If you want to see more native birds, the day trip to <a href="http://www.tiritirimatangi.org.nz/">Tiritiri Maitangi Island</a> is highly recommended. You still won’t see kiwi (they are brown, shy, and nocturnal, so are essentially unobservable) but you will likely see saddlebacks and kākāriki and <strike>black</strike> robins and hihi and bellbirds and kēreru and maybe kōkako and takahē. </p><p><b>Death Rays from Space</b> (update)&nbsp;</p><p>Auckland has a relatively mild climate, since it’s surrounded by water, but we’re the same distance from the equator as Las Vegas or the Greek islands.&nbsp; There’s also less continental dust in the atmosphere here than a lot of places. It is surprisingly easy to get badly sunburned.&nbsp;</p><p><b>Foods</b></p><p>The best-value inexpensive food in central Auckland is in Asian restaurants, and particularly in Asian food courts. Non-foodcourt examples especially worth mentioning are <a href="https://www.zomato.com/auckland/selera-newmarket/menu">Selera</a> (Malaysian, in Newmarket) and <a href="https://www.zomato.com/auckland/chomna-lorne-street">Chom Na</a> (Thai, downtown). Another inexpensive option is fish and chips, which is as good here as anywhere in the world: it’s worth paying extra for snapper if it’s fresh. A lot of pubs also have reasonable food. </p><p>The best Indian food is in Sandringham, about 6km south, but there are some good places at the top of the hill, along K Rd (<a href="http://satya.co.nz/">Satya</a>, in particular)</p><p>Mexican food is not recommended: it tends to be either bad or expensive. Pizza mostly isn’t great (with a few exceptions). Otherwise, any restaurant that can survive in Auckland is unlikely to be terrible. </p><p>At the higher priced end of the market, there are a number of good restaurants on Fort St. <a href="https://www.imacuisine.co.nz/">Ima</a> does family-style Israeli food very well. <a href="https://www.indochinekitchen.co.nz/">Indochine Kitchen</a> is Vietnamese, a bit noisy but good flavours. &nbsp;<a href="http://www.beirut.co.nz/">Beirut</a> is posh Lebanese. <a href="https://www.cassiarestaurant.co.nz/">Cassia</a> is modern Indian food and was the Restaurant of the Year last year. There are lots of well-regarded places in Ponsonby that I don’t know much about. &nbsp;</p><p>At the top: for high-end French-style food, <a href="https://www.thegroverestaurant.co.nz/">The Grove</a> is really excellent; I’ve heard good things about <a href="https://www.thefrenchcafe.co.nz/">The French Café</a>, but have never been there; <a href="http://www.grandharbour.co.nz/">Grand Harbour</a> does Hong Kong-style seafood and is by acclaim the best Chinese restaurant in the country, but I’m not really qualified to judge whether it’s worth it. The <a href="https://www.skycityauckland.co.nz/restaurants/orbit/">revolving</a> restaurant on the SkyTower is expensive because it revolves; unlike some revolving restaurants it does actually have good food. </p><p>Finally, <a href="https://www.giapo.com/">Giapo</a>, on Fort St, does absolutely over-the-top decorated locavore gelato. You have never seen anything like it.<br></p>]]></content:encoded>
    <wp:post_name>165788346396</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/165752537951</link>
    <pubDate>Tue, 26 Sep 2017 19:21:41 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/165752537951</guid>
    <!--<wp:post_id>165752537951</wp:post_id>-->
    <wp:post_date>2017-09-25 23:21:41</wp:post_date>
    <wp:post_date_gmt>2017-09-26 06:21:41</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>A genome analogy</title>
    <description></description>
    <content:encoded><![CDATA[<p>DNA looks like a zipper. &nbsp;</p><p>If you scale it up to a fairly fine-toothed zipper with tooth spacing of about 2mm or 1/12in, the human genome would run about from Auckland to Hawai’i.</p><p>On this scale, 1 Morgan is about 230km, so you inherit contiguous genome chunks from your grandparents of about 60km.&nbsp;</p><p>The HLA region is about 6km long.</p><p>A million-variant SNPchip has markers spaced several meters apart.</p><p>A typical gene is maybe 20 meters from start to finish, but a lot of it is introns, and most exons are less than half a meter long.</p><p>And, on this scale, the original Human Genome Project cost rather less per mile than a rural two-lane road.&nbsp;</p>]]></content:encoded>
    <wp:post_name>165752537951</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/165642986646</link>
    <pubDate>Sat, 23 Sep 2017 18:23:08 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/165642986646</guid>
    <!--<wp:post_id>165642986646</wp:post_id>-->
    <wp:post_date>2017-09-22 23:23:08</wp:post_date>
    <wp:post_date_gmt>2017-09-23 06:23:08</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Bayesian surprise</title>
    <description></description>
    <content:encoded><![CDATA[<p>For reasons not entirely unconnected with NZ election polling, I’ve been thinking about surprise in Bayesian inference again: what happens when you get a result that’s a long way from what you expected in advance? Yes, your prior is badly calibrated and you should feel bad, but what should you <b>believe</b>?</p><p>A toy version of the problem is inference for a location parameter. We have a prior $p_\theta(\theta)$ for the parameter, and a model $p_X(x|\theta)$. Consider two extremes</p><ul><li>$\theta\sim N(0,1)$ and $X\sim\textrm{Cauchy}(\theta)$</li><li>$\theta\sim\textrm{Cauchy}(0)$ and $X\sim N(\theta, 1)$</li></ul><p>Suppose we take a single observation $x$ of $X$ and it’s very large. What do we end up believing about $\theta$ in each case?</p><p>Heuristically, the first case says the data can sometimes be a long way from $\theta$, but $\theta$ has to be not that far from 0. The second case says $\theta$ can sometimes be a long way from 0 but $X$ can’t be that far from $\theta$. So in the the first case the posterior for $\theta$ should be concentrated fairly near zero and in the second it should be concentrated fairly near $X$.&nbsp;That’s exactly what happens when you do the maths.&nbsp;</p><p>Under the first model, the posterior density is proportional to&nbsp;<br>$$e^{-\theta^2/2}\frac{1}{1+(x-\theta)^2}$$ and the posterior mode solves<br>$$\tilde\theta =\frac{(x-\tilde\theta)}{1+(x-\tilde\theta)^2}.$$<br>For $x\to\infty$ we can’t have &nbsp;$x-\theta$ bounded, which in turn means $\tilde\theta=O((x-\tilde\theta)^{-1})$, giving $\theta\to 0$.</p><p>Under the second model, the posterior is proportional to $$e^{(x-\theta)^2/2}\frac{1}{1+\theta^2}$$ and the posterior mode solves<br>$$x-\tilde\theta=\frac{2\tilde\theta}{1+\tilde\theta^2}.$$<br>If $x\to\infty$, the solution to this equation must have $x-\tilde\theta$ bounded, which implies $\tilde\theta\to\infty$, which implies $x-\tilde\theta\to 0$.</p><p>If the two distributions are both Normal the posterior mode will be about halfway between $x$ and 0. If they’re both Cauchy, the posterior will be bimodal, with one mode near $x$ and another near 0.&nbsp;<br></p><p>The basic observation here goes back a long way, with a relatively recent summary by O’Hagan in <a href="http://www.jstor.org/stable/2289540">JASA, 1990</a>: given a surprising observation, Bayesian inference can (sensibly) end up just ‘rejecting’ which ever of the prior and model have heavier tails.&nbsp;</p><p>Working it out for simple cases makes a nice straightforward stats theory question. &nbsp;It’s also a good low-dimensional example of the problem common in high-dimensional problems that it’s quite hard to be sure what features of your model and prior are going to matter for inference.&nbsp;</p>]]></content:encoded>
    <wp:post_name>165642986646</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/165065299921</link>
    <pubDate>Thu, 07 Sep 2017 14:13:34 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/165065299921</guid>
    <!--<wp:post_id>165065299921</wp:post_id>-->
    <wp:post_date>2017-09-06 19:13:34</wp:post_date>
    <wp:post_date_gmt>2017-09-07 02:13:34</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Visual design of diagnostics</title>
    <description></description>
    <content:encoded><![CDATA[<p>Q: Are these curves parallel?</p><figure class="tmblr-full" data-orig-height="328" data-orig-width="473"><img src="https://68.media.tumblr.com/c1f42936a6f17a19b5b856d750cbab55/tumblr_inline_ovw0lnMAF71s1hdxy_540.png" data-orig-height="328" data-orig-width="473"></figure><p>A: I mean, probably not? They look like they might be getting closer together, but if those big steps mean more uncertainty...</p><p>Q: Ok, how about with confidence intervals. Now are they parallel?</p><figure class="tmblr-full" data-orig-height="328" data-orig-width="473"><img src="https://68.media.tumblr.com/9a77d08b121d01cc0a82348be260e8c3/tumblr_inline_ovw0ng5wMs1s1hdxy_540.png" data-orig-height="328" data-orig-width="473"></figure><p>A: Um. I’m not sure that helped. Still a definite maybe</p><p>Q: Is this curve horizontal?</p><figure class="tmblr-full" data-orig-height="328" data-orig-width="473"><img src="https://68.media.tumblr.com/d9adbd25eb626f4904ff72710f042715/tumblr_inline_ovw0pgBEat1s1hdxy_540.png" data-orig-height="328" data-orig-width="473"></figure><p>A: No. It slopes down. It crosses zero somewhere around 8 or 9 years.</p><p>Q: Why do some statistics packages still provide the first graph for assessing departures from proportional hazards, rather than the second?</p><p>A: Tradition? Or textbooks?</p>]]></content:encoded>
    <wp:post_name>165065299921</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/164514374976</link>
    <pubDate>Wed, 23 Aug 2017 19:37:08 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/164514374976</guid>
    <!--<wp:post_id>164514374976</wp:post_id>-->
    <wp:post_date>2017-08-23 0:37:08</wp:post_date>
    <wp:post_date_gmt>2017-08-23 07:37:08</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Causes and counterfactuals</title>
    <description></description>
    <content:encoded><![CDATA[<p><i>Attention Conservation Notice: this was on StatsChat four years ago, but I like it as a causation example.</i></p><p>A <a href="http://www.nzherald.co.nz/nz/news/article.cfm?c_id=1&amp;objectid=11186087">story in the Herald </a>illustrates a subtle technical and philosophical point about causation. A Lotto winner says</p><blockquote><p>“I realised I was starving, so stopped to grab a bacon and egg sandwich.</p><p>“When I saw they had a Lotto kiosk, I decided to buy our Lotto tickets while I was there.</p><p>“We usually buy our tickets at the supermarket, so I’m glad I followed my gut on this one,” said one of the couple, who wish to remain anonymous.</p></blockquote><p>Assuming it was a random pick, it’s almost certainly true that if they had not bought the ticket at that Lotto kiosk at that time, they would not have won.&nbsp;&nbsp;On the other hand, if Lotto is honest, buying at that kiosk wasn’t a good <b>strategy</b> — it had no impact on the chance of winning.</p><p>There is a sense in which buying the bacon-and-egg sandwich was a <b>cause</b> of the win, but it’s not a very useful sense of the word ’cause’ for most statistical purposes.</p>]]></content:encoded>
    <wp:post_name>164514374976</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/164383348811</link>
    <pubDate>Sun, 20 Aug 2017 11:09:55 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/164383348811</guid>
    <!--<wp:post_id>164383348811</wp:post_id>-->
    <wp:post_date>2017-08-19 16:09:55</wp:post_date>
    <wp:post_date_gmt>2017-08-19 23:09:55</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Wilcoxon and polymath: another update</title>
    <description></description>
    <content:encoded><![CDATA[<p>As I wrote before, there’s a polymath (large-scale collaborative pure maths) project on transitivity of dice. <a href="https://gowers.wordpress.com/2017/07/25/intransitive-dice-vi-sketch-proof-of-the-main-conjecture-for-the-balanced-sequences-model/#more-6318">Here’s the latest update</a> from Timothy Gowers’s blog.</p><p>Suppose $X$, $Y$, and $Z$ are discrete distributions supported on $1,2,\dots,n$. &nbsp;We can ask about $P(X&lt;Y)$ and $P(Y&lt;Z)$ and $P(X&lt;Z)$, which is what the Wilcoxon/Mann-Whitney rank test does.&nbsp;</p><p>The project has basically proved that under one model for randomly choosing distributions, if $X$, &nbsp;$Y$, and $Z$ have the same mean and $P(X&gt;Y)&gt;1/2$ and $P(Y&gt;Z)&gt;1/2$, the probability of $P(X&gt;Z)&gt;1/2$ is $1/2+o(1)$. That is, if three distributions have the same mean, and the Wilcoxon test says $X$ is bigger than $Y$ and $Y$ is bigger than $Z$, you’ve got essentially no information about whether it will say $X$ is bigger than $Z$.</p><p>Gowers also says they are close to showing a converse: if the means are different, then $P(X&gt;Y)&gt;1/2$, $P(Y&gt;Z)&gt;1/2$ and $P(X&gt;Z)&gt;1/2$ are true or false they way you’d assume from the ordering of the means.&nbsp;</p><p>That is, we knew the Wilcoxon test does not give a self-consistent ordering on all distributions. Now we know that (for this particular model of discrete distributions) when it <b>does</b> give an ordering, the ordering is typically the same as the ordering by means.&nbsp;</p>]]></content:encoded>
    <wp:post_name>164383348811</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/164014429561</link>
    <pubDate>Thu, 10 Aug 2017 18:44:52 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/164014429561</guid>
    <!--<wp:post_id>164014429561</wp:post_id>-->
    <wp:post_date>2017-08-09 23:44:52</wp:post_date>
    <wp:post_date_gmt>2017-08-10 06:44:52</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>The bus bot</title>
    <description></description>
    <content:encoded><![CDATA[<p>Back in January, I spent a few hours hacking together a script to tweet summaries of the Auckland bus system, on the account <a href="https://twitter.com/tuureiti">@tuureiti</a>. People seemed to like it: the bot has 110 followers, many of whom appear to be actual people (or at least actual organisations).</p><p>A few times I’ve been asked for the source code and hadn’t gotten around to it, because the code is ugly, includes my API key, and is ugly. The second problem is fixable, so I fixed it. The code (which has been running the bot since about 04:00 UTC, August 10th), is <a href="https://github.com/tslumley/busbot">here</a>. Did I mention it was ugly?</p><p>Good takeaways from the code:&nbsp;</p><ul><li>it’s really easy to write a Twitter bot in R<br></li><li>it’s pretty easy to parse JSON in R</li><li>when you’re dealing with external servers, <b>anything</b> can fail so you need exception handling <b>everywhere</b></li><li>Rule 34a: if you can think of it, there’s an R package for it.</li><li>experimental hacked-together scripts last longer than you expected</li></ul><p>From a user viewpoint, people seem to like the morning and evening messages, and the greetings in reo Māori. &nbsp;They like the unfiltered data -- especially as Auckland Transport’s headline punctuality figure (based on the first stop only) is so far removed from the lived experience of passengers.&nbsp;</p><p>One problem with percentage on time is that people don’t have a good idea of what is a sensible target. &nbsp;Aiming for 100% on time would be a disaster: you’d need to slow down the entire system to the worst-case traffic speeds. &nbsp;One of the things I want to do with the AT transit feed is to get a better idea of what’s reasonable, so that a future @tuureiti can produce the sort of rating scale that the <a href="http://www.transportchaos.org/">‘London Transport Chaos’</a> site used to.&nbsp;</p>]]></content:encoded>
    <wp:post_name>164014429561</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/163464356371</link>
    <pubDate>Thu, 27 Jul 2017 11:34:52 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/163464356371</guid>
    <!--<wp:post_id>163464356371</wp:post_id>-->
    <wp:post_date>2017-07-26 16:34:52</wp:post_date>
    <wp:post_date_gmt>2017-07-26 23:34:52</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Tail bounds under sparse correlation</title>
    <description></description>
    <content:encoded><![CDATA[<p><i>Attention Conservation Notice: Very long and involves a proof that hasn’t been published, though the paper was rejected for unrelated reasons.&nbsp;</i></p><p>Basically everything in statistics is a sum, and the basic useful fact about sums is the Law of Large Numbers: the sum is close to its expected value. Sometimes you need more, and there are lots of uses for a good bound on the probability of medium to large deviations from the expected value.</p><p>One of the nice ones is Bernstein’s Inequality, which applies to bounded variables. If the variables have mean zero, are bounded by $\pm K$, and the variance of the sum is $\sigma^2$, then<br>$$\Pr\left(\left|\sum_i X_i\right|&gt;t\right)\leq 2e^{-\frac{1}{2}\frac{t^2}{\sigma^2+Kt}}.$$<br>The bound is exponential for large $t$ and looks like a Normal distribution for small $t$. &nbsp;You don’t actually need the boundedness; you just need the moment bounds it implies: for all $r&gt;2$, $EX_i^r\leq K^{r-2}r!E[X_i^2]/2$. That looks like the Taylor series for the exponential, and indeed it is.</p><p>These inequalities tend to only hold for sums of independent variables, or ones that can be rewritten as independent, or nearly independent. My one, which this post is about, is for what I call sparse correlation. &nbsp;Suppose you’re trying to see how accurate radiologists are (or at least, how consistent they are). You line up a lot of radiologists and a lot of x-ray images, and get multiple ratings. &nbsp;Any two ratings of the same image will be correlated; any two ratings by the same radiologist will be correlated; but&nbsp;‘most’ pairs of ratings will be independent.&nbsp;</p><p>You might have the nice tidy situation where every radiologist looks at every image, in which case you could probably use $U$-statistics to prove things about the analysis. More likely, though, you’d divide the images up somehow. For rating $X_i$, I’ll write ${\cal S}_i$ as the set of ratings that aren’t independent of $X_i$, and call it the neighbourhood of $X_i$. &nbsp;You could imagine a graph where each observation has an edge to each other observation in its neighbourhood, and this graph will be important later.</p><p>I’ll write $M$ for the size of the largest neighbourhood and $m$ for the size of the largest set of independent observations. &nbsp;If you had 10 radiologists each reading 20 images, $M$ would be $10+20-1$ and $m$ would be $10$. &nbsp;I’ll call data sparsely correlated if $Mm$ isn’t too big. If I was doing asymptotics I’d say $Mm=O(n)$</p><p>I actually need to make the stronger assumption that any two sets of observations that aren’t connected by any edges in the graph are independent: pairwise independence isn’t enough. For the radiology example that’s still fine: if set A and set B of ratings don’t involve any of the same images or any of the same radiologists they’re independent.</p><p>A simple case of sparsely correlated data that’s easy to think about (if pointless in the real world) is identical replicates. &nbsp;If we have $m$ independent observations and take $M$ copies of each one, we know what happens to the tail probabilities: you need to replace $t$ by $Mt$ and $\sigma^2$ by $M\sigma^2$ (ie, $M^2$ times the sum of the $m$ independent variances). We can’t hope to do better; it turns out we can do as well.&nbsp;</p><p>The way we get enough independence to use the Bernstein’s-inequality argument is to make up $M-1$ imaginary sets of data. &nbsp;Each set has the same joint distribution as the original variables, but the sets are independent of each other. &nbsp;Actually, what we need is not $M$ copies but $\chi$ copies, where $\chi$ is the number of colours needed for every variable in the dependency graph to have a different colour from all its neighbours. $M$ is always enough, but you can sometimes get away with fewer.</p><p>Here’s the picture, for a popular graph<br><br></p><figure data-orig-width="666" data-orig-height="573" class="tmblr-full"><img src="https://68.media.tumblr.com/1eb7385484649fe9267c716e30f66dca/tumblr_inline_otpz4zklkg1s1hdxy_540.png" alt="image" data-orig-width="666" data-orig-height="573"></figure><p>The original variables are at the top. We needed three colours, so we have the original variables and two independent copies.&nbsp;</p><p>Now look at the points numbered&nbsp;‘1′. &nbsp;Within a graph these are never neighbours because they are the same colour, and obviously between graphs they can’t be neighbours. So, all the variables labelled `1′ are independent (even though they have horribly complicated relationships with variables of different labels). There’s a version of every variable labelled ‘1′, another version labelled ‘2′, and another &nbsp;labelled&nbsp;‘3′. &nbsp;</p><p>The proof has five steps. &nbsp;First, we work with the exponential of the sum in order to later use Markov’s inequality to get exponential tail bounds. Second, we observe that adding all these extra copies makes the problem worse: a bound for the sum of all $M$ copies will bound the original sum. Third, we use the independence within each label to partially factorise an expectation of $e^{\textrm{sum}}$ into a product of expectations. We use the original Taylor-series argument based on the moment bounds to get a bound for an exponential moment. And, finally, we use Markov’s inequality to turn that into an exponential tail bound. &nbsp;The first, and last two, steps are standard, the second and third are new.&nbsp;</p><p><b>Theorem</b>: Suppose we have $X_i$, $i=1,2,\ldots,n$ mean zero with neighbourhood size $M$. Suppose that for each $X_i$<br>$$&nbsp;EX_i^r\leq K^{r-2}r!\sigma_i^2/2$$<br>and let $\sigma^2\geq\sum_{i=1}^n\sigma_i^2$<br>Then<br>$$\Pr\left(\left|\sum_i X_i\right|&gt;t\right)\leq2e^{-\frac{1}{2}\frac{t^2}{M\sigma^2+MKt}}.$$</p><p><b>Proof</b>: The $M$ copies of the variables are written $\tilde X_{ij}$ with $i=1,\dots,n$ and $j=1,\dots,M$, and the labelled versions as $X_{i(j)}$ for the copy of $X_i$ labelled $j$.&nbsp;</p><p><br>By independence of the copies from each other<br>$$\begin{eqnarray*}<br>E\left[\exp(\frac{1}{M}\sum_{j=1}^M\sum_{i=1}^n\tilde X_{ij}) \right]&amp;=&amp;\prod_{j=1}^ME\left[\exp(\frac{1}{M}\sum_{i=1}^n\tilde X_{ij}) \right]\\\<br>&amp;=&amp;E\left[\exp(\frac{1}{M}\sum_{i=1}^nX_{i}) \right]^M\\\\<br>&amp;\geq&amp;E\left[\exp(\frac{1}{M}\sum_{i=1}^nX_{i}) \right]<br>\end{eqnarray*}$$<br>so introducing the extra copies makes things worse.</p><p>Now we use the labels. We can factor the expectation into a product over $i$, since $X_{i(j)}$ with the same $j$ and different $i$ are independent.<br>$$E\left[\exp(\frac{1}{M}\sum_{j=1}^M\sum_{i=1}^n\tilde X_{i(j)}) \right]=\prod_i E\left[\exp\left(\frac{1}{M}\sum_{j=1}^M \tilde X_{i(j)} \right)\right]$$<br><br>Now we use the moment assumptions to get moment bounds for the sum<br>$$E\left[\left(\frac{1}{M}\sum_{j=1}^M\sum_{i=1}^n\tilde X_{i(j)}\right)^r\;\right]\leq M^{r-1}E\left[\left(\sum_{i=1}^n \tilde X_{i1}\right)^r\,\right]\leq M^{r-1}r!K^{r-2}\sigma^2$$</p><p>Writing $S_n$ for $\sum_{i=1}^n X_i$, and $\tilde S_n$ for $\sum_{i,j} \tilde X_{ij}$ we have (for a value $c$ to be chosen later)<br>$$\begin{eqnarray*}<br>E e^{cS_n} &amp;\leq&amp; Ee^{c\tilde S_n}\\\<br>&amp;=&amp; 1+\frac{1}{2}\sigma^2c^2\sum_{r=2}^\infty \frac{c^{r-2}E\tilde S_n^r}{r!\sigma^2/2}\\\<br>&amp;&lt;&amp;\exp\left[\frac{1}{2}\sigma^2c^2\sum_{r=2}^\infty \frac{c^{r-2}E\tilde S_n^r}{r!\sigma^2/2}\right]\\\<br>&amp;\leq&amp;\exp\left[\frac{1}{2}\sigma^2c^2\sum_{r=2}^\infty \frac{c^{r-2}M^{r-1}r!K^{n-2}\sigma^2/2}{r!\sigma^2/2}\right]\\\<br>&amp;&lt;&amp;\exp\left[\frac{1}{2}M\sigma^2c^2\sum_{r=2}^\infty(cMK)^{r-2} &nbsp;\right]\\\<br>&amp;=&amp;\exp\left[\frac{M\sigma^2c^2}{2(1-cMK)}\right]<br>\end{eqnarray*}$$</p><p>Write $\tilde K$ for $MK$ and $\tilde \sigma^2$ for $M\sigma^2$ to get<br>$$E e^{cS_n}&lt;\exp\left[\frac{\tilde\sigma^2c^2}{2(1-c\tilde K)}\right]$$<br>Markov’s inequality now says<br>$$P[S_n\geq t\tilde\sigma]\leq \frac{Ee^{cS_n}}{e^{ct\tilde\sigma}}&lt;\exp\left[\frac{\tilde\sigma^2c^2}{2(1-c\tilde K)}-ct\tilde\sigma\right],$$<br>We’re basically done: we just need to find a good choice of $c$. The calculations are the same as in Bennett’s 1962 proof of Bernstein’s inequality, where he shows that $c=t/(\tilde Kt+\tilde \sigma)$ gives<br>$$P[S_n\geq t]&lt;\exp\left[-\frac{1}{2}\frac{t^2}{\tilde\sigma^2+\tilde Kt}\right]$$</p><p>That’s an upper bound, and adding the same lower bound at most doubles the tail probability. So we are done.&nbsp;</p>]]></content:encoded>
    <wp:post_name>163464356371</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/163438730506</link>
    <pubDate>Wed, 26 Jul 2017 19:13:17 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/163438730506</guid>
    <!--<wp:post_id>163438730506</wp:post_id>-->
    <wp:post_date>2017-07-26 0:13:17</wp:post_date>
    <wp:post_date_gmt>2017-07-26 07:13:17</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Psychoactive substances and Peter Dunne</title>
    <description></description>
    <content:encoded><![CDATA[<p>New Zealand, like a lot of places, has a problem with illegal sales of potent synthetic cannabinoid receptor agonists (aka ‘synthetic cannabis’, ‘synthetic marijuana’, ‘Spice’, ‘K2′, etc, etc). Peter Dunne, as the responsible Minister, is getting a lot of criticism.&nbsp;</p><p>I don’t think Peter Dunne should be an MP. His party got 0.22% of the vote at the last election. In theory it’s conceivable he got in because he provides astonishingly good constituency service to Ohariu rather than as an edge case in the MMP voting system, but I find that hard to believe. On the other hand, I cannot think of anyone else in Parliament who has done as much to try to prevent the current drug situation.</p><p>The Psychoactive Substances Act provided a mechanism for lower-risk&nbsp;‘legal highs’ to be approved, something that didn’t exist anywhere else in the world. You could argue that just legalising cannabis and MDMA would have been better, and I would totally agree, but the law was a genuine step away from the whack-a-mole serial banning paradigm that NZ and other countries had used. When the law was being proposed I was skeptical about whether it would work, and cynical about Mr Dunne’s motives. &nbsp;In hindsight I was right to be skeptical, but I think I was wrong to be cynical.&nbsp;</p><p>What killed the new approach was controversy over animal testing. &nbsp;John Banks, of the ACT Party, wasn’t willing to support a law that killed rabbits to protect drug users. &nbsp;As a result, the government couldn’t force the law through and gave up on it, allowing it to be killed by an amendment that most of Parliament then voted to support. The Green Party gets an honourable mention for abstaining on the vote.&nbsp;The law wouldn’t have been a magical fix for the drug problem (and neither would legalisation) but it would have helped. You could also argue that a better minister would have gone down with his ship and voted against the amendment, and ok, maybe yes -- but it likely wouldn’t have helped.</p><p>The people who, often literally and explicitly, wanted drug users to die rather than rabbits have got what they wanted. Peter Dunne wasn’t one of them.&nbsp;</p>]]></content:encoded>
    <wp:post_name>163438730506</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/163424464396</link>
    <pubDate>Wed, 26 Jul 2017 11:20:34 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/163424464396</guid>
    <!--<wp:post_id>163424464396</wp:post_id>-->
    <wp:post_date>2017-07-25 16:20:34</wp:post_date>
    <wp:post_date_gmt>2017-07-25 23:20:34</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Information and control</title>
    <description></description>
    <content:encoded><![CDATA[<p>There were delays on the Auckland rail system this morning, apparently due to a train hitting a person in south Auckland. &nbsp;</p><p>It seems unreasonable to complain about the delays; Auckland Transport doesn’t have a warehouse of magic inflatable replacement trains, and owing to historic underfunding of trains, there isn’t a lot of redundancy in the physical track network. They actually did a pretty good job of moving around the trains they have, and I was only delayed about twenty minutes.</p><p>What they’re absolutely pants at is telling people what is going on. &nbsp;All the real-time train information on the stations and on Auckland Transport’s app shuts down when something goes wrong -- or, worse, continues but is inaccurate. The announcements mostly tell you to wait for announcements.&nbsp;</p><p>I follow Auckland Transport on Twitter, and they announced that the Southern and Eastern lines would be disrupted, and that you needed to take a bus to get from south Auckland to the isthmus. &nbsp;They didn’t mention the Onehunga line, which doesn’t go near where the incident happened, and which you might naively expect to be ok. &nbsp;It was shut down, running just a shuttle to Penrose. Again, in terms of &nbsp;actual physical movement of trains this makes sense, but not in terms of communication with transit users.</p><p>More broadly, trains are big things that move along tracks. &nbsp;Someone must know where the trains are and where they are heading; it would then be feasible to work out roughly when they are expected to get to other places. &nbsp;Auckland Transport do a reasonable job of this for buses, and they make the raw information available for other people to try to do better. For trains, not so much.</p><p>Seattle, where I used to live, was perhaps the first place to do real-time transit prediction. &nbsp;An evaluation of the system found that it reduced average waiting time -- but that it reduced <b>perceived</b> waiting time by about twice as much as actual waiting time. &nbsp;Knowing what’s going on helps you maintain a comforting illusion of control over your life, over and above the benefit of helping you make plans.</p>]]></content:encoded>
    <wp:post_name>163424464396</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/162792527531</link>
    <pubDate>Mon, 10 Jul 2017 07:00:28 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/162792527531</guid>
    <!--<wp:post_id>162792527531</wp:post_id>-->
    <wp:post_date>2017-07-09 12:00:28</wp:post_date>
    <wp:post_date_gmt>2017-07-09 19:00:28</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Probabilities not bounded away from zero</title>
    <description></description>
    <content:encoded><![CDATA[<p>We have a population or cohort of $N$ people divided into&nbsp;$H$ sampling strata, with a sample of size $n_h$ taken from the population $N_h$ in stratum $h$. Let $\pi_{ij}$ be the sampling probability for person $i$ in stratum $h$. When we do asymptotics we usually assume $\pi_{ih}$ are bounded away from zero. That’s not ideal for, say, case-control studies of rare diseases, where we might want asymptotic approximations based on the case incidence being small (ie,&nbsp;converging to zero).&nbsp;</p><p>In the situations where I’m interested in&nbsp;$\pi_{ih}$ being small, it’s usually small for a whole stratum. Since sampling is independent between strata, there should be a central limit theorem separately for each stratum, and we should be able to add up the limiting Normal approximations for the stratum totals to get a Normal limit for the population total estimate and the population mean estimate.&nbsp;</p><p>To formalise this, &nbsp;suppose $n_h\to\infty$ for every stratum (so that asymptotics makes sense), and that&nbsp;$\pi_{ih}N_h/n_h$ is bounded above and below, so that within each stratum the sampling probability has a finite (relative) range. As a simple example, we might have a case stratum with $\pi_i\approx 1$ and a control stratum with very small $\pi_i$.&nbsp;</p><p>[<b>Update</b>: As Stas Kolenikov points out, I’m assuming the same strata are small large along the infinite sequence, so I need something like $n_{h_1}/(n_{h_1}+n_{h_2})\to c_{h_1,h_2}\in [0,1]$ for each pair of strata. &nbsp;This isn’t a meaningful loss of generality since (a) the infinite sequence is an analytic fiction and we might as well set it up for our maximum convenience; and (b) even without assuming anything, every subsequence will have a subsubsequence along which the condition holds]</p><p><br></p><p>By standard results, $n_h^{1/2}(\bar X_{.h}-\mu_h)\stackrel{d}{\to} N(0,\sigma^2_h)$ for each stratum $h$ , and by the Skorohod representation theorem we can find an $H$-variate normal vector $\langle Z_h\rangle_{h=1}^H$ with<br>$$n_h^{1/2}(\bar X_{.h}-\mu_h)\stackrel{p}{\to} Z_h$$<br>(possibly on a different probability space), to get<br>$$\bar X_{.h}= \mu_h+ n_h^{-1/2}{Z_h}+o_p(n_h^{-1/2})$$<br>The $Z_h$ will be independent, with mean zero; write $\sigma^2_h$ for the variances.&nbsp;</p><p>[<b>Update</b>: Note that $\sigma^2_h$ is just $\mathrm{var}[Z_h]$, nothing more fundamental. Under stratified random sampling, $\sigma^2_h$ will be $\mathrm{var}[X]$ in stratum $h$ multiplied by the&nbsp;‘finite population correction” $(N_h-n_h)/N_h$, but under other sampling schemes it will be something else]</p><p>Now, <br>$$\bar X_{..} = \frac{1}{N}\sum_{h=1}^H N_h\bar X_{.h}$$<br>giving<br>\begin{align*}<br>\bar X_{..} &amp;=\sum_{h=1}^H \frac{N_h}{N}\mu_h+\frac{N_hn_h^{-1/2}}{N}Z_h+o_p\left(\frac{N_hn_h^{-1/2}}{N} \right)\\\<br>&amp;=\mu+\left(\sum_{h=1}^H\frac{N_hn_h^{-1/2}}{N}Z_h\right)+o_p\left(\sum_{h=1}^H\frac{ N_h}{N\sqrt{n_h}}\right)<br>\end{align*}</p><p><br></p><p>First, suppose $ N_h/N$ converges to a non-zero constant for each $h$. Let $n_*=\min_h n_h$ and define ${\mathcal H}=\{h: \lim n_*/n_h&gt;0\}$<br>$$\begin{eqnarray*}<br>\bar X_{..} &amp;= &amp;\mu+\left(\sum_{h=1}^H\frac{N_hn_h^{-1/2}}{N}Z_h\right)+o_p\left(\frac{\max_h N_h}{N\min_h \sqrt{n_h}}\right)\\\<br>&nbsp;&amp;= &amp;\mu+\left(\sum_{h\in{\mathcal H}}\frac{N_hn_*^{-1/2}}{N}Z_h\right)+\sum_{h\not\in{\mathcal H}} o_p(n_*^{-1/2})+o_p\left(\frac{\max_h N_h}{N\sqrt{n_*}}\right)\\\<br>&amp;=&amp; \mu+ n_*^{-1/2}Z+o_p(n_*^{-1/2}) \\\<br>\end{eqnarray*}$$</p><p>where $Z\sim N(0, \sigma^2)$ with <br>$$\sigma^2=\lim_{n_*\to\infty} \sum_{h\in{\mathcal H}} \frac{N_h^2n_*\sigma^2_h}{N^2n_h}$$</p><p>Alternatively, for case--control sampling we may have $N_h/N\to 0$ in the case stratum, but we would have $n_h$ all of the same order, and so of the same order as their total, $n$. The limiting distribution is dominated by the largest strata: define ${\mathcal H}'=\{h: \lim N_h/N&gt;0\}$ (which is non-empty as $H$ is finite)</p><p>$$\begin{eqnarray*}<br>\bar X_{..} &amp;=&amp;\mu+\left(\sum_{h=1}^H\frac{N_hn_h^{-1/2}}{N}Z_h\right)+o_p\left(\sum_{h=1}^H\frac{ N_h}{N\sqrt{n_h}}\right)\\\<br>&amp;=&amp;\mu+\left(\sum_{h\in{\mathcal H}'}\frac{N_hn^{-1/2}}{N}Z_h\right)+\sum_{h\not\in{\mathcal H}'} o_p(n^{-1/2})+o_p\left(n^{-1/2}\right)\\\\<br>&amp;=&amp; \mu+ n^{-1/2}Z+o_p(n^{-1/2})\\\<br>\end{eqnarray*}$$<br>where $Z\sim N(0, \sigma^2)$ with<br>$$\sigma^2=\lim_{n\to\infty} \sum_{h\in{\mathcal H}} \frac{N_h^2n\sigma^2_h}{N^2n_h}$$</p><p>Weaker conditions on $N_h$ and $n_h$ are clearly possible: it is only necessary to identify which terms dominate the limiting distribution of $\bar X_{..}$, since the limiting distribution of estimated stratum totals is always independent $H$-variate Normal under appropriate scaling. </p>]]></content:encoded>
    <wp:post_name>162792527531</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/162620207301</link>
    <pubDate>Wed, 05 Jul 2017 18:28:20 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/162620207301</guid>
    <!--<wp:post_id>162620207301</wp:post_id>-->
    <wp:post_date>2017-07-04 23:28:20</wp:post_date>
    <wp:post_date_gmt>2017-07-05 06:28:20</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Two-day course: survival analysis</title>
    <description></description>
    <content:encoded><![CDATA[<p><b>Tuesday 12 and Wednesday 13 September 2017, 9am-5pm.&nbsp;</b>This two-day workshop will cover data exploration, data summaries, and regression modelling for time-to-event data. &nbsp;There will be both lecture and practical sessions. </p><p>Topics:</p><ul><li>Concepts: censoring, truncation, competing risks, choice of time scale<br></li><li>Summaries: the Kaplan--Meier curve; mean, median, and proportion surviving; the hazard rate; graphical exploration</li><li>Two-sample testing: the logrank test and its strengths and weaknesses</li><li>The proportional &nbsp;hazards model: right censoring, left truncation,&nbsp;<br></li><li>Time-varying predictors<br></li><li>Modelling recurrent events</li></ul><p>Basically, I’m aiming to target the issues that make time-to-event data different from more familiar binary or continuous data. Most of the examples will be medical, but the same issues exist in engineering and in the social sciences.</p><p>Participants should be familiar with linear and logistic regression, and should bring a laptop with suitable statistical software. R is preferred, but assistance will probably be available with Stata and SAS. </p><p>See the <a href="https://www.stat.auckland.ac.nz/en/about/statistical-consulting-centre/workshops1.html">Statistical Consulting Centre page</a> for organisational details</p>]]></content:encoded>
    <wp:post_name>162620207301</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/162528447451</link>
    <pubDate>Mon, 03 Jul 2017 11:44:17 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/162528447451</guid>
    <!--<wp:post_id>162528447451</wp:post_id>-->
    <wp:post_date>2017-07-02 16:44:17</wp:post_date>
    <wp:post_date_gmt>2017-07-02 23:44:17</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Ryanair randomness</title>
    <description></description>
    <content:encoded><![CDATA[<p>[See <b>update</b> at end]</p><p>Ryanair (like many airlines) allows you to pay extra to pick better seats. &nbsp;Unlike most airlines, if you don’t pay extra they claim they will allocate your seat at random, rather than trying to keep groups together or favouring frequent-flyers or whatever.&nbsp;</p><p>There’s a <a href="http://www.ox.ac.uk/news/2017-06-29-ryanair-random-seat-allocation-not-so-random-says-oxford-university-expert-0?utm_source=t.co&amp;utm_medium=referral">press release</a> from the University of Oxford starting</p><blockquote><p>Passengers have more chance of winning the National Lottery jackpot than being allocated middle seats at random on a Ryanair flight, according to new Oxford University analysis.<br></p></blockquote><p>That’s completely wrong. &nbsp;Even ignoring the issues I’ll get to about the calculation, the probabilities are for sets of people rather than individuals, and they aren’t claiming to be based on the actual probability of getting middle seats on Ryanair.</p><p>The calculations were done for the BBC program Watchdog</p><blockquote><p> Last night, the BBC Consumer affairs programme, Watchdog, ran its own investigation to test how random the airline’s seating algorithm is.</p><p>As part of their tests, groups of four people were sent on four separate Ryanair flights. In each instance every single person was allocated a middle seat. Dr Jennifer Rogers, Director of the new Oxford University Statistical Consultancy was then invited to analyse the data, to work-out the chances of every person getting a middle seat allocated randomly.</p><p>By looking at the amount of window, aisle and middle seating available on each flight, at the time of check-in, Dr Rogers, calculated the chances of all four people being randomly given middle seats on each of the flights, to be around 1:540,000,000. The chances of winning the National Lottery jackpot are 1:45,000,000. (This means that you are 10 times more likely to win the lottery than be in a group who are all randomly allocated middle seats.)</p></blockquote><p>These calculations say that if there were no seats specifically picked by other travellers, so that all seats were available, it’s very unlikely that 16 people would all get middle seats. &nbsp;The odds are a bit bigger than I would I would have expected -- I would have thought it would be more like $1:3^{16}$, &nbsp;or about $1:43\textrm{ million}$ -- but there’s no real reason to think the calculation is wrong.</p><p>It’s the wrong calculation, though.</p><p>If most of all the aisle and window seats are sold to people willing to pay extra, then the random allocation will be mostly to middle seats, and the chance of getting 16 middle seats will be fairly high. At the extreme, if all the aisle and window seats are sold to people willing to pay extra, then the random allocation will be entirely to middle seats, and the chance of getting 16 middle seats will be 100%.&nbsp;The proportion of middle seats in a set of 16 random bookings can’t give any evidence against the claim of random allocation (unless there are empty aisle or window seats). The claim is about how passengers are allocated to the set of unsold seats; the evidence is about whether the set of unsold seats is a simple random sample -- and obviously it won’t be. That’s the whole point.&nbsp;</p><p>Watchdog also says the data show Ryanair deliberately splits up groups travelling together</p><blockquote><p>Dr Rogers also considered whether row allocation played a part in the seating arrangement, observing that passengers were often scattered throughout the plane. An individual from a group would, on average, be sat 10 rows away from someone else from their group. On two of the flights the data revealed that a passenger had been sat 26 rows away from someone else who they were flying with</p></blockquote><p>Here, at least, the available data is relevant, but it seems to support the airline’s claim. &nbsp;If you allocate passengers to (mostly middle) seats at random, you will usually break up groups travelling together. About half the time they’ll be more widely separated than average. &nbsp;That’s presumably why Ryanair does it, and why most airlines don’t. It’s a shitty way to treat your customers, but that’s Ryanair for you.&nbsp;</p><p>The problem is that&nbsp;“Ryanair is a horrible airline” isn’t news.&nbsp;“Statistics show Ryanair is horrible” isn’t really news.&nbsp;“Statistics show Ryanair is lying” is news, but based on what’s in the press release, it’s not statistics.&nbsp;</p><p><br></p><p>[<b>Update</b>: Based on the Telegraph <a href="http://www.telegraph.co.uk/travel/news/ryanair-seat-selection-cost-airlines-easyjet-ba/">story</a>, the calculations might have based on the seat availability map at the time of check-in, in which case they <b>do</b> show you can’t avoid paying by checking in early. &nbsp;Since Ryanair doesn’t claim you can, I’m not convinced this is much of an improvement. &nbsp;It still has to be the case that a good seat which gets filled is filled either by someone paying extra (and so it’s not available) or by someone allocated at random (and so people can be allocated to it at random). And it’s still the case that random allocation is a strategy for deliberating splitting up groups travelling together. &nbsp;The "26 rows” may be relevant if it’s (nearly) the maximum cabin length, but even so it’s 2 pairs of travellers out of 24 same-plane pairs, so it can’t be strong evidence.</p><p>In summary: it wouldn’t be easy for Ryanair to have a less friendly seat-allocation policy than they claim to have, and I’m not at all convinced that they do.]</p>]]></content:encoded>
    <wp:post_name>162528447451</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/161712680186</link>
    <pubDate>Mon, 12 Jun 2017 11:06:56 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/161712680186</guid>
    <!--<wp:post_id>161712680186</wp:post_id>-->
    <wp:post_date>2017-06-11 16:06:56</wp:post_date>
    <wp:post_date_gmt>2017-06-11 23:06:56</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>A possibly unsurprising bootstrap observation</title>
    <description></description>
    <content:encoded><![CDATA[<p>Suppose you have a finite population modelled as a realisation of some probability model with potentially complicated spatial structure, and a multistage sample taken with some different structure. &nbsp;For example, suppose you have a genetic linear mixed model with ancestry and relatedness structure, but you sample people by census block group and household.&nbsp;</p><p>It is either blindingly obvious or really surprising (or both?) that the sampling component of the standard error doesn’t depend on the structure of the model. We can write&nbsp;<br>$$\mathrm{var}_{Y\pi}[\hat\theta]=E_Y[\mathrm{var}_\pi[\hat\theta]]+\mathrm{var}_Y[E_\pi[\hat\theta]]$$<br>(where $Y$ expectations are over the model and $\pi$-expectations are over the sampling) and estimate the first term by (say) a survey bootstrap. The second term is just the inverse population expected Fisher information, the model variance of the estimate for the whole population (the ‘census parameter’).&nbsp;</p><p>For a simpler, concrete example, consider a linear mixed model where the population is 1000 clusters of size 100, with a cluster-specific random intercept and a covariate $x$. At sampling, the population is divided into 1000 primary sampling units of size 100, but differently: they could be independent of the model clusters, or overlapping in some way. &nbsp;We take a sample random sample of, say, 250 primary sampling units and fit the random-intercept mixed model</p><p>The variance of $\hat\beta_x$ should be the sum of the sampling variance (depending only on the primary sampling units and ignoring the model clusters) and the model-based variance from the whole population. And, in simulation, it is.&nbsp;</p><p>In most practical settings, you’d still need to use information about the sampling structure (eg weights) to estimate the model parameters and the population expected Fisher information, but the two steps are still conceptually separate. And, in practice, you’d need to work out how to get the weights into the estimating functions, which is not straightforward. But&nbsp;you really can bootstrap the sampling component of variance without worrying about the spatial structure of the model.&nbsp;</p>]]></content:encoded>
    <wp:post_name>161712680186</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/161449071226</link>
    <pubDate>Mon, 05 Jun 2017 13:45:36 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/161449071226</guid>
    <!--<wp:post_id>161449071226</wp:post_id>-->
    <wp:post_date>2017-06-04 18:45:36</wp:post_date>
    <wp:post_date_gmt>2017-06-05 01:45:36</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Stupid word games</title>
    <description></description>
    <content:encoded><![CDATA[<p>Today, Jeroen Ooms announced the appearance on CRAN of <a href="https://cran.r-project.org/package=cld2">an R package for language detection</a>, wrapping the&nbsp;“<a href="https://github.com/CLD2Owners/cld2">CLD2</a>″ compact language detector. &nbsp; Obviously, given a tool like that on a holiday long weekend, my first reaction was to try to confuse it.</p><p>Two fun games to play with a language detector:</p><ul><li>Find an obviously English sentence (ideally a quote) that it doesn’t recognise as English, and a very non-obviously English sentence that it does<br></li><li>Find two sentences with as few differences as possible, where one is recognised as English and the other not</li></ul><p>CLD2 doesn’t recognise the famous telegram about platypuses&nbsp;“Monotremes oviparous, ovum meroblastic” as English, which I suppose is fair enough.&nbsp;</p><p>It didn’t recognise Getrude Stein’s&nbsp;“Rose is a rose is a rose is a rose”, or even the shorter&nbsp;“Rose is a rose is a rose”, though it had no trouble with the start of <strike>FInnegan’s</strike>&nbsp;“Finnegans Wake” or bits of&nbsp;“Howl”. &nbsp;Even better than the Stein, though:</p><figure data-orig-width="743" data-orig-height="61" class="tmblr-full"><img src="https://68.media.tumblr.com/59ed88fe394e0436e23334799f7858a4/tumblr_inline_or1w9jON0W1s1hdxy_540.png" alt="image" data-orig-width="743" data-orig-height="61"></figure><p>There’s a linguistic discussion of this sort of sentence <a href="http://itre.cis.upenn.edu/~myl/languagelog/archives/005101.html">at Language Log</a>&nbsp;-- it’s not usual English in a lot of ways -- &nbsp;but I think it’s going to be hard to beat as a false negative.</p><p>As a false positive I tried Jabberwocky (English), and then thought of Douglas Hofstatder’s self-referential example sentences</p><figure data-orig-width="949" data-orig-height="217" class="tmblr-full"><img src="https://68.media.tumblr.com/7e1064ce6164c9b87e2ebe5fbc232ca7/tumblr_inline_or1w4rkNGD1s1hdxy_540.png" alt="image" data-orig-width="949" data-orig-height="217"></figure><p>Ok, so how far can the second one be warped and still show up as English?</p><figure data-orig-width="944" data-orig-height="125" class="tmblr-full"><img src="https://68.media.tumblr.com/0827cac73903358183f64c52099af6e7/tumblr_inline_or1w7n4f2l1s1hdxy_540.png" alt="image" data-orig-width="944" data-orig-height="125"></figure><p>That’s English, but&nbsp;“See Spot run” isn’t!</p><p>For minimal changes: changing&nbsp;“a” to&nbsp;“the”</p><figure data-orig-width="585" data-orig-height="148" class="tmblr-full"><img src="https://68.media.tumblr.com/aa8a584fdcd83ef634d4daedbde9f908/tumblr_inline_or1wb9QsgT1s1hdxy_540.png" alt="image" data-orig-width="585" data-orig-height="148"></figure><p>And as a sort of combination of the two: Chomsky’s obviously-English nonsense sentence&nbsp;“Colorless green ideas sleep furiously” is recognised as English, but so is every permutation of the same words.&nbsp;</p><p>So, is there a point to this (other than a fun way to waste half an hour)? Well, one of the important things to remember about automated classification algorithms is (as Zeynep Tukfeci puts it) <a href="https://www.ted.com/talks/zeynep_tufekci_machine_intelligence_makes_human_morals_more_important">how alien they are</a>. They can often imitate human decisions astonishingly well, but they don’t work the same way. &nbsp;If another person makes the same decisions as you, it’s a good bet there are some basically similar reasons underneath. It’s easy to believe the same is true for machines, but it isn’t.</p>]]></content:encoded>
    <wp:post_name>161449071226</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/161225885311</link>
    <pubDate>Tue, 30 May 2017 15:15:21 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/161225885311</guid>
    <!--<wp:post_id>161225885311</wp:post_id>-->
    <wp:post_date>2017-05-29 20:15:21</wp:post_date>
    <wp:post_date_gmt>2017-05-30 03:15:21</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Pipeable survey analysis in R</title>
    <description></description>
    <content:encoded><![CDATA[<p>Today, I accidentally found out about the&nbsp;<a href="https://github.com/gergness/srvyr">‘srvyr’ package</a>, which is a wrapper for my&nbsp;‘survey’ package to make it work with %&gt;% pipes and dplyr and so on.&nbsp;</p><ol><li>&nbsp;Yay!<br></li><li>R has a package discovery problem. I wouldn’t say I’m the most plugged-in of R users, but there must be a reasonable fraction who would be even less likely than me to find out about it. &nbsp;</li><li>Even though the&nbsp;‘survey’ package design sticks fairly close to&nbsp;‘tidy data’ principles, the fact that it uses different conventions from the `tidyverse’ packages means that there’s *a whole lot* of adaptor code needed. I could have fixed this by sticking to Hadley’s conventions, except that would have needed time travel.</li><li>It’s much harder, sitting at the command line, to find the code that actually does the work in a pipeable tidyverse-type package. It’s clear that many people find that a good tradeoff, but I’m not sure it is for me.&nbsp;</li></ol>]]></content:encoded>
    <wp:post_name>161225885311</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/160940652681</link>
    <pubDate>Mon, 22 May 2017 19:38:11 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/160940652681</guid>
    <!--<wp:post_id>160940652681</wp:post_id>-->
    <wp:post_date>2017-05-22 0:38:11</wp:post_date>
    <wp:post_date_gmt>2017-05-22 07:38:11</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Peer review and community endorsement</title>
    <description></description>
    <content:encoded><![CDATA[<p>In most academic fields there are journals it’s easy to publish in. Some of these are outright scams, but some are just not that fussy about the importance of results. In the experimental sciences, being able to publish negative or otherwise uninteresting results can be very important. Even in fields where ideas, rather than data, are important, being able to get research out into libraries is valuable -- &nbsp;though preprint servers such as arXiv are now filling that niche.&nbsp;</p><p>Some of the papers published in, say,&nbsp;<i>Communications in Statistics -- Simulation and Computation</i> are important and influential -- eg, Pepe and Anderson’s <a href="http://www.tandfonline.com/doi/abs/10.1080/03610919408813210">classic</a> on &nbsp;structures of marginally specified models -- but most of them will be of interest only to a few with common interests, and most of them, good or bad, are there because they’ve been rejected by other journals.&nbsp;</p><p>Publication in this sort of journal doesn’t represent any sort of endorsement of the content by the broader community in that discipline -- in fact, quite the reverse. Sometimes the lack of endorsement turns out to be to the discredit of the discipline, as with Margaret Pepe’s paper; more often, not.&nbsp;</p><p>No-one’s going to set up an elaborate hoax to get a paper published in <i>Comm Stat Simul Comp</i>, let alone one of the actual scam statistics journals, because the response would be a mixture of blank looks and yawns. The claim that a hoax paper reflected the general views of the statistical community about the overgeneralised Beta distribution would be an obvious straw man, but in addition would be really boring. As<i> Skeptic Magazine</i> has shown, if the field is gender studies the result isn’t necessarily <a href="https://academeblog.org/2017/05/20/the-hoax-that-failed-or-skeptics-who-arent-very-skeptical/">boring</a>. It’s still a straw man.&nbsp;</p>]]></content:encoded>
    <wp:post_name>160940652681</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/160578860696</link>
    <pubDate>Fri, 12 May 2017 19:50:22 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/160578860696</guid>
    <!--<wp:post_id>160578860696</wp:post_id>-->
    <wp:post_date>2017-05-12 0:50:22</wp:post_date>
    <wp:post_date_gmt>2017-05-12 07:50:22</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>A ‘polymath’ project on the Wilcoxon test?</title>
    <description></description>
    <content:encoded><![CDATA[<p>`Polymath’ is a set of projects in massive collaborative proof of mathematical results; Terry Tao and Timothy Gowers are two of the famous mathematicians involved. &nbsp;There’s a new potential project <a href="https://gowers.wordpress.com/2017/04/28/a-potential-new-polymath-project-intransitive-dice/">&nbsp;on Gowers’s blog</a>, which he describes a being related to intransitive dice. As you know, if you read this blog, (a) I prefer the term non-transitive, and (b) this means it’s about the Wilcoxon test.</p><p>The idea of the conjecture is that you define an $n$-sided die by sampling uniformly with replacement $n$ numbers from $1, 2,3,\dots,n$ as the numbers on the sides, with the constraint that the numbers have to add up to $n(n+1)/2$. Rolling such a die $M$ times samples $M$ observations from a distribution on $1,2,3,\dots, n$ with mean $(n+1)/2$. &nbsp;You construct three such dice, $A$, $B$, and $C$. &nbsp;Their distributions have the same mean, so the $t$-test would have no ability to distinguish data from the three distributions, no matter how large $M$ was. But the Wilcoxon test probably would. It’s assumed, but not yet proved, that the probability of a tie according to the Wilcoxon test goes to zero as $n\to\infty$.</p><p>The interesting conjecture is that if you see $A$ beat $B$ and $B$ beat $C$ according to the Wilcoxon test, the probability that $A$ beats $C$ goes to 1/2 as $n$ goes to infinity. &nbsp; That is, given equal means, the Wilcoxon test is basically as non-transitive as possible. &nbsp;</p>]]></content:encoded>
    <wp:post_name>160578860696</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/160182924986</link>
    <pubDate>Mon, 01 May 2017 19:02:49 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/160182924986</guid>
    <!--<wp:post_id>160182924986</wp:post_id>-->
    <wp:post_date>2017-05-01 0:02:49</wp:post_date>
    <wp:post_date_gmt>2017-05-01 07:02:49</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Value of a degree?</title>
    <description></description>
    <content:encoded><![CDATA[<p>Today was graduation day for Science students at the University of Auckland. At each graduation, the Chancellor of the University gives an introduction that includes&nbsp;(for example, <a href="https://youtu.be/aPKGc-99BSI?t=19m16s">here</a>)</p><blockquote><p>We know that, compared to those whose formal education ends in high school, graduates have lower unemployment rates, higher salaries, better career prospects, and better health outcomes.</p></blockquote><p>I’d hope that a university degree would give students the tools to think about claims like that. &nbsp;</p><p>Some of them would be able to explain why that sentence, although literally just a claim of correlation, becomes a claim of causation via the pragmatics of language. &nbsp;Others could describe the alternative explanations for the difference, whether through education selecting more intelligent people, or through wealth leading both to better access to education and to other social and financial capital. Others could work out the sort of data or experiment you’d need to make the implied causal claim valid.</p><p>I’d hope that our students could do that sort of thing, but I think it’s unfair to make them do it on graduation day.&nbsp;</p>]]></content:encoded>
    <wp:post_name>160182924986</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/158976786141</link>
    <pubDate>Thu, 30 Mar 2017 10:04:37 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/158976786141</guid>
    <!--<wp:post_id>158976786141</wp:post_id>-->
    <wp:post_date>2017-03-29 14:04:37</wp:post_date>
    <wp:post_date_gmt>2017-03-29 21:04:37</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Prerequisites</title>
    <description></description>
    <content:encoded><![CDATA[<p>This week, John Myles White tweeted</p><blockquote><p>One meme I wish would die off: the belief that we can teach high school students statistics without teaching them calculus.</p></blockquote><p>Statistics Twitter was immediately divided between “Preach it, brother!” and “Not cool, dude.” I’m mostly, but not entirely, in the latter camp.&nbsp;</p><p>Personally, I did study calculus before taking up statistics, and it helped. In fact, I studied tensor calculus, functional analysis, measure theory, group theory, and differential topology before taking up statistics. They have all helped &nbsp;-- but I’m not entirely typical.&nbsp;</p><p>I have never seen anyone suggest that topology should be a prerequisite for statistics, but a good understanding of the myriads of ways&nbsp;‘closeness’ can be defined and the implications of these differences is a basic part of applied statistics. I have never seen anyone suggest that functional analysis should be a prerequisite for statistics, but learning about Hilbert and Banach spaces helped me understand why high-dimensional models and prior distributions are so much less robust than low-dimensional ones.&nbsp;</p><p>It goes the other way, too. &nbsp;In discussions of NIH training grants in biostatistics, one hears people arguing that a biostatistician needs a solid understanding of, say, physiology or immunology or genomics, to be really useful in medical research the way Famous Professor Lastname is. It frequently turns out on further investigation that Famous Professor Lastname did a PhD in, as it might be, martingale theory or the identifiability of certain semiparametric models, and that all their biomedical knowledge has been obtained on the job. Maybe it would be better if future biostatisticians were taught this stuff in classes, but the evidence being cited isn’t supportive. A surprisingly wide range of people can usefully study statistics.</p><p>If you’re teaching a course in statistics -- high school statistics, or linear regression, or in survival analysis, or whatever -- it is important to decide whether you are going to assume understanding of calculus or matrices or sigma notation for sums, or simple high-school algebra. To some extent, the more maths you can assume, the more statistics you can cover in a fixed amount of time. If everyone in the class is comfortable with matrices, you can use matrix notation for models. &nbsp;If everyone is comfortable with calculus, you can switch between maximum likelihood and the score equations without much comment. If everyone is comfortable with that level of abstraction, you can talk about causal graphs and define confounding in terms of graph substructures.&nbsp;If everyone is comfortable with algebra, you can write $Y$ and $X$ and talk about variables in general instead of needing to use the names of actual measured variables.&nbsp;</p><p>But none of this is necessary for statistics. You can teach more statistics than most scientists need to know without using calculus, matrices, or even summation notation. We did it, in biostatistics courses in Seattle aimed at the scarily smart but math-deprived PhD students in health sciences. &nbsp;And &nbsp;in high school and first-year courses in statistics in New Zealand, we teach statistics, using permutation tests and the bootstrap to substitute for integrals over the sampling distribution, just as we teach the ideas of&nbsp;‘fair comparisons’ without using causal graphs. My mother didn’t ever study calculus, because at her high school A-level maths and A-level chemistry were in the same time slot. She subsequently learned a reasonable amount of statistics by being involved in evidence-based medicine before it was trendy.&nbsp;</p><p>The problem of prerequisites is primarily an opportunity-cost problem. Should students be forced to wait until after they’ve learned calculus to start learning statistics? Is it better to have two semesters of statistics, or one semester of statistics and one of calculus? Or linear algebra, or measure theory, or functional analysis? What about students who’d rather be found dead in a ditch than take calculus? Is statistics necessarily of no use to them?<br></p><p>My sound-bite position on this is&nbsp;“Statistics should be the last thing you learn”. That is, if you’re going to study maths, you should try to do it before you start statistics, because more maths will make statistics easier. But that’s not specific to maths. If you’re going to study genetics or computer science or the history of colonial revolutions, and you’re planning to use statistics, there’s a good chance you’d be better off waiting to learn statistics until after you know what you want to use it for.&nbsp;</p><p>But if you want to learn statistics, right now, we can find you a class that doesn’t assume you’re a mathematician or a computer scientist or an astronomer, or a doctor. Maybe someone with a different background and interests could learn faster, but a key part of good statistical practice is thinking carefully about what is the actual question at issue, and that’s not it.&nbsp;</p>]]></content:encoded>
    <wp:post_name>158976786141</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/158950018446</link>
    <pubDate>Wed, 29 Mar 2017 15:29:39 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/158950018446</guid>
    <!--<wp:post_id>158950018446</wp:post_id>-->
    <wp:post_date>2017-03-28 19:29:39</wp:post_date>
    <wp:post_date_gmt>2017-03-29 02:29:39</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Come work with us</title>
    <description></description>
    <content:encoded><![CDATA[<p>The Statistics Department at the University of Auckland is looking for three new academics. We have two entry-level positions[1], and one mid-level to senior position[2]. Formal ad <a href="https://www.opportunities.auckland.ac.nz/psp/ps/EMPLOYEE/HRMS/c/HRS_HRAM.HRS_CE.GBL?Page=HRS_CE_JOB_DTL&amp;Action=A&amp;JobOpeningId=18865&amp;SiteId=1&amp;PostingSeq=1">here</a>:</p><figure data-orig-width="1024" data-orig-height="680" class="tmblr-full"><img src="https://68.media.tumblr.com/363a02c228d128bc0c4caf2135349bc9/tumblr_inline_on97s54qhz1s1hdxy_540.png" alt="image" data-orig-width="1024" data-orig-height="680"></figure><p><i>(<a href="https://www.flickr.com/photos/yaranaika/5612799116">https://www.flickr.com/photos/yaranaika/5612799116</a>)</i></p><p>The department has a fairly broad view of what statistics is about: we have probabiliists (both theoretical and applied), researchers in mainstream statistical methods, in astrostatistics, in genomics, in stats education, in statistical ecology, in forensic statistics, and (<a href="https://www.stat.auckland.ac.nz/en/about/our-department/ihaka-lectures.html">famously</a>) in statistical computing. The department is fairly stable financially: about a third of the University’s students take an introductory stats paper, so while outside money is always a bonus, the department isn’t going to dry up and blow away without it. We’re the largest department of statistics in Australasia, and we think we’re the second-largest in the southern hemisphere (there’s a bigger one in São Paulo). The current QS rankings have us at 49, which doesn’t seem outrageously high or low.&nbsp;</p><figure data-orig-width="1024" data-orig-height="436" class="tmblr-full"><img src="https://68.media.tumblr.com/0b289503c330c02a5a403fef366aa525/tumblr_inline_on97wxk43l1s1hdxy_540.png" alt="image" data-orig-width="1024" data-orig-height="436"></figure><p><i>(<a href="https://www.flickr.com/photos/yaranaika/6194710360">https://www.flickr.com/photos/yaranaika/6194710360</a>)<br></i></p><p>As the ad says, there are specific needs that it would be great to fill: our applied probabilist and our forensics expert would like collaborators, we’ve got a big cohort of data science students looming from an agreement with a university in Chongqing, we have opportunities to expand in social stats and demography, and our postgraduate students would like more projects in finance to be available. But we’re not expecting to get a precise fit, and we’ll consider people with any research area close enough to statistics.</p><figure data-orig-width="1024" data-orig-height="680" class="tmblr-full"><img src="https://68.media.tumblr.com/de2655eba0d0031a4351afc222029c19/tumblr_inline_on98cagQTp1s1hdxy_540.png" alt="image" data-orig-width="1024" data-orig-height="680"></figure><p><i>(<a href="https://www.flickr.com/photos/yaranaika/6135487087/in/album-72157627643758946/">https://www.flickr.com/photos/yaranaika/6135487087/in/album-72157627643758946/</a>)</i></p><p>Auckland is a medium-sized (1.5 million), cosmopolitan city with about 1/3 of the population born overseas. It’s built on an isthmus between two harbours, full of tiny extinct volcanoes, so it’s physically spectacular. &nbsp;The weather is mild: never below freezing in winter, never above 30C (85F) in summer. There are the basic civilised amenities like good libraries, public transport, single-payer health care, and curry laksa. Auckland has a housing-price problem that’s so bad even the politicians have noticed, but otherwise is a nice place to live.</p><p>Nat Torkington, an NZ tech person, has <a href="http://nathan.torkington.com/blog/2016/11/09/on-moving-to-new-zealand/">a largely relevant blog post</a> on moving to NZ for American tech types. Obviously I’m biased, but it’s not as if Auckland was my only option for somewhere to work and live.&nbsp;</p><figure data-orig-width="1024" data-orig-height="683" class="tmblr-full"><img src="https://68.media.tumblr.com/31903a763f653764cd77b4c20c2a564e/tumblr_inline_on9991Rcie1s1hdxy_540.png" alt="image" data-orig-width="1024" data-orig-height="683"></figure><p><i>(<a href="https://www.flickr.com/photos/yaranaika/8152694252/in/album-72157631922413795/">https://www.flickr.com/photos/yaranaika/8152694252/in/album-72157631922413795/</a>)</i></p><p>[1] Lecturer, equivalent to hard-money tenure-track Assistant Professor in the US system<br></p><p>[2]&nbsp;Senior Lecture or Associate Professor, about the range from Associate Professor to the starting end of full Professor in the US</p><p>Photos all CC-BY-SA, by Salman Javed: &nbsp;<a href="https://www.flickr.com/photos/yaranaika/">https://www.flickr.com/photos/yaranaika/</a></p>]]></content:encoded>
    <wp:post_name>158950018446</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/158920563941</link>
    <pubDate>Tue, 28 Mar 2017 19:50:01 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/158920563941</guid>
    <!--<wp:post_id>158920563941</wp:post_id>-->
    <wp:post_date>2017-03-27 23:50:01</wp:post_date>
    <wp:post_date_gmt>2017-03-28 06:50:01</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Why I like the Convolution Theorem</title>
    <description></description>
    <content:encoded><![CDATA[<p>The convolution theorem (or theorems: it has versions that some people would call distinct species and other would describe as mere subspecies) is another almost obviously almost true result, this time about asymptotic efficiency. It’s an asymptotic version of the Cramér--Rao bound.&nbsp;</p><p>Suppose $\hat\theta$ is an efficient estimator of $\theta$ and $\tilde\theta$ is another, not fully efficient, estimator. &nbsp;The convolution theorem says that if you rule out stupid exceptions, asymptotically $\tilde\theta=\hat\theta+e$ where $e$ is pure noise, independent of $\hat\theta$. &nbsp;</p><p>The reason that’s almost obvious is that if it weren’t true, there would be some information about $\theta$ in $\tilde\theta-\hat\theta$, and you could use this information to get a better estimator than $\hat\theta$, which (by assumption) can’t happen. The stupid exceptions are things like the <a href="http://notstatschat.tumblr.com/post/118767311371/superefficiency">Hodges superefficient estimator </a>that do better at a few values of $\theta$ but much worse at neighbouring values.&nbsp;</p><p>In the usual case where everything is asymptotically Normal,&nbsp;</p><p>$$\sqrt{n}\begin{pmatrix} \hat\theta-\theta_0\\\<br>\tilde\theta-\theta_0\end{pmatrix}\stackrel{d}{\to} N\left( 0, \begin{pmatrix} \sigma^2 &amp; \sigma^2\\\<br>\sigma^2 &amp;\sigma^2+\omega^2\end{pmatrix}\right)$$</p><p>The interesting part of that equation is the off-diagonal term. It’s the way it is because any other choice would imply the existence of a linear combination of $\hat\theta$ and $\tilde\theta$ with better efficiency than $\hat\theta$. &nbsp;Rescaling to correlations, the square of the correlation between $\hat\theta$ and $\tilde\theta$ is the relative efficiency.&nbsp;</p><p>I think the convolution theorem is a useful way to think about asymptotic efficiency (and the fact that it has assumptions is a useful reminder that asymptotic efficiency is less elegant than it should be).&nbsp;</p><p>Also, more or less by the definition of influence functions, it follows that the squared correlation between the influence functions for $\hat\theta$ and $\tilde\theta$ is also the asymptotic relative efficiency. &nbsp;That’s nice because in simulations we can evaluate the influence functions at the true parameter value and don’t need to solve the estimating equations iteratively. &nbsp;</p><p>In particular, now I’m looking at relative efficiency of weighted and unweighted logistic regression in the case-control design, I can compute the correlation between the estimating functions without needing the fitted log odds ratio estimates. &nbsp;That saves iteration, but more importantly it still works if the case and control $X$ distributions completely separate in a minority of the simulations. Now, I could just estimate the two variances at the true parameter value instead (with about as much work) but working with the correlation has made it easier to prove some special cases analytically. &nbsp;</p>]]></content:encoded>
    <wp:post_name>158920563941</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/158881486676</link>
    <pubDate>Mon, 27 Mar 2017 18:59:29 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/158881486676</guid>
    <!--<wp:post_id>158881486676</wp:post_id>-->
    <wp:post_date>2017-03-26 22:59:29</wp:post_date>
    <wp:post_date_gmt>2017-03-27 05:59:29</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Flat Earthers</title>
    <description></description>
    <content:encoded><![CDATA[<p>The world isn’t a flat rectangle.&nbsp;</p><p>We’ve got to the stage where most people accept this. &nbsp;It’s especially easy in New Zealand, where we know you can fly in a wide variety of directions and still end up in Europe after about the same time in the air.&nbsp;</p><p>Since the world isn’t a flat rectangle, all flat rectangular maps have to be badly wrong somehow. &nbsp;Recently, Boston public schools have shifted from the badly-wrong Mercator projection to the differently-wrong Gall-Peters projection. That’s probably a good thing, but the coverage has been playing to some urban myths about maps.</p><p>The Mercator projection makes countries far from the equator look bigger than they are relative to countries near the equator. The myth is that this is <b>why</b> Europeans adopted it, to make Europe look big. &nbsp;To start with, the myth doesn’t make a lot of sense: even when we stipulate European ego and intellectual dishonesty, we’re left with the question of why they’d want to make their colonial conquests look small. When the British boasted of an empire&nbsp;‘on which the sun never sets’ they weren’t trying to make the British Isles look big. Anyone who knows a fisher, knows that&nbsp;“it was this big” isn’t typically an understatement.</p><p>More importantly, there’s a good reason for the Mercator projection. &nbsp;If you sail a ship along a constant compass bearing (as you do), your course is a straight line on the Mercator projection. It’s the only rectangular projection where this is true. Back in the day, that was a life-saving piece of navigation tech.&nbsp;</p><p>Because all flat maps have to lie, being perfect for compass navigation means the Mercator projection has to really suck for some other map features. It’s really bad at area. &nbsp;The Gall-Peters projection is perfect for area. Of course, being perfect for area means the Gall-Peters projection has to really suck in some other ways. It distorts shape really badly, and it distorts geopolitics pretty badly, too.&nbsp;</p><p>People who work with maps mostly think there are better compromises: either giving up on longitude lines being straight, or trading off area distortion and shape distortion, or allowing for gaps cut into the oceans. XKCD, as usual, <a href="https://xkcd.com/977/">has a good take</a>. &nbsp;</p><p>It is strange, though, that Google has forced the world into&nbsp;“Web Mercator” as a standard online projection. Few people use Google Maps to plan long sailing voyages. It would be simpler as well as arguably more accurate to use the Plate Carrée projection where vertical and horizontal coordinates map on to latitude and longitude. &nbsp;[<b>Update</b>: it turns out I’m completely wrong here. Web Mercator is important if you want to &nbsp;preserving shape in small areas, and if you also want nice rectangular nesting of map tiles. Which you do. So just use Google Earth for continent-scale maps.]</p><p>One bonus of all the bizarre projections we have is to make some nice homework exercises in differential topology: work out what the implied distance metric is on the projected space, or derive the formulas for straight lines.</p>]]></content:encoded>
    <wp:post_name>158881486676</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/158540770406</link>
    <pubDate>Sat, 18 Mar 2017 20:42:12 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/158540770406</guid>
    <!--<wp:post_id>158540770406</wp:post_id>-->
    <wp:post_date>2017-03-18 0:42:12</wp:post_date>
    <wp:post_date_gmt>2017-03-18 07:42:12</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Case-control efficiency</title>
    <description></description>
    <content:encoded><![CDATA[<p>The basic story about sampling weights and regression is fairly simple: if you don’t need the weights, using them will add noise. The standard error increase is basically proportional to the coefficient of variation of the weights, and doesn’t depend on the regression coefficients or the covariate distribution.&nbsp;</p><p>Logistic regression in a case-control sample looks superficially as if it should be the same. &nbsp;The maximum likelihood estimator is unweighted logistic regression, ignoring the weights, and it’s more efficient that the estimator using sampling weights. &nbsp;But logistic regression is different: the sampling isn’t actually non-informative, it’s just that all the bias magically ends up in the intercept, which we don’t usually care about. The efficiency gap is different, too.</p><p>This graph shows the ratio of the standard errors for a model $\mathrm{logit}\,\Pr[Y=1]=\alpha+X$, with $X\sim N(0,1)$ and various numbers of controls per case. &nbsp;In each simulation, $\alpha$ is chosen so the control sampling fraction is the same (1%), so the coefficient of variation of the weights is the same everywhere. The efficiency loss isn’t, to put it mildly</p><figure class="tmblr-full" data-orig-height="508" data-orig-width="1266"><img src="https://68.media.tumblr.com/4f09c183c64b235de69408f5ebadd7cb/tumblr_inline_on01z7e40Q1s1hdxy_540.png" data-orig-height="508" data-orig-width="1266"></figure><p>As another example, consider a variable with values 1-4. &nbsp;This graph shows control distributions in black and the implied case distributions for $\beta=1$ in red.&nbsp;</p><figure class="tmblr-full" data-orig-height="543" data-orig-width="783"><img src="https://68.media.tumblr.com/772d834ebe234d58814e044bc1d6d7ef/tumblr_inline_on025nKMIY1s1hdxy_540.png" data-orig-height="543" data-orig-width="783"></figure><p>And here are the standard error ratios</p><figure class="tmblr-full" data-orig-height="580" data-orig-width="1158"><img src="https://68.media.tumblr.com/af8c7c9ba10c376cb412c1211e0855a5/tumblr_inline_on02838bns1s1hdxy_540.png" data-orig-height="580" data-orig-width="1158"></figure><p>An explanation that predicts these results (it honestly was a prior prediction, though not preregistered), is based on multiple imputation. Suppose we didn’t know what the maximum likelihood estimator was, but we were prepared to do multiple imputation of $X$ for all the unobserved non-cases. &nbsp;If we do this right and all the assumptions hold and so on, it should recover the maximum likelihood estimator. There’s information about the non-case distribution &nbsp;of $X$ in controls, obviously. There’s also information in the cases: the density of $X$ in cases is proportional to the density in controls scaled by $e^{\beta x}$. If we impute using only the control distribution we just get the weighted estimator, so the efficiency gap must be explained by the information in cases about the control $X$ distribution.</p><p>So, one way to predict whether the gain in precision is large or small is to see if the case $X$ distribution contains large or small amounts of information about the control $X$ distribution: are there lots of cases at $X$ values with few controls?</p><p>The bar plots suggest that for positive $\beta$ and positively-skewed $X$ there should be a large gain, but it should be smaller for uniform or negatively skewed $X$, and the pattern should reverse for negative $\beta$. As it does.&nbsp;Also, as the number of controls increases, the extra information from the cases will become less important, so the efficiency gap will decrease. &nbsp;As it does.</p><p>This has been another episode of&nbsp;“Case-control logistic regression is more complicated than you think”. It’s going to be a talk in Auckland on April 5 and at U Penn on April 20.&nbsp;</p>]]></content:encoded>
    <wp:post_name>158540770406</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/158403979826</link>
    <pubDate>Wed, 15 Mar 2017 08:33:39 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/158403979826</guid>
    <!--<wp:post_id>158403979826</wp:post_id>-->
    <wp:post_date>2017-03-14 12:33:39</wp:post_date>
    <wp:post_date_gmt>2017-03-14 19:33:39</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Order and quotient topologies</title>
    <description></description>
    <content:encoded><![CDATA[<p>Over the years when I was intermittently working on the rock-paper-scissors (transitivity) problem in statistical testing, one of the confusing things was the difference between order and quotient topologies. I thought I’d write about why.</p><p>Suppose you have two-dimensional Euclidean space, with points $(x,y)$, and you decide to order points on the first coordinate, so $(x,y)\prec(z,w)$ iff $x&lt;z$. This gives you equivalence classes $(x,y)\sim(z,w)$ iff $x=z$. &nbsp;There are two obvious topologies on the set of equivalence classes.&nbsp;</p><p>The first is the quotient topology, where you just collapse each equivalence class to a point. The function doing the collapsing is $\phi(x,y)=x$ and we define a set $V$ of equivalence classes as open if there is an open set $U$ in the two-d space with $V=\phi(U)$. Thinking heuristically in terms of metrics, $x$ and $z$ are close in the quotient space if there are points $(x,y)$ and $(z,w)$ that are close in the original two-d space. &nbsp;The quotient topology depends on the original topological structure, but not on the ordering.&nbsp;</p><p>The second topology is the order topology, where the open sets of equivalence classes are generated by the open intervals in the ordering: $(a,b)=\{x: a\prec x\prec b\}$. &nbsp;This uses the ordering but not the original topological structure.</p><p>In the two-dimensional to one-dimensional example these are the same: they both give the standard topology on $\mathbb{R}$. &nbsp;This happens in a lot of examples, so it’s easy to confuse the two. In particular, it happens when the equivalence classes are a stack of nice tidy surfaces that cut across a finite-dimensional space. Since that was my mental picture of what’s going on, I didn’t realise there could even be a problem. [Update: Debreu’s Theorem, in economics, is a more sophisticated version of this picture]</p><p>&nbsp;An example where the topologies are very different is when you start with cumulative distribution functions under the supremum metric and you order by the mean. &nbsp;If $F$ is a cdf with mean $\mu$ there are cdfs with any other mean $\nu$ arbitrarily close to $F$ in the original metric. &nbsp;The quotient topology isn’t any use for studying the $t$-test. &nbsp;The order toplogy, though, is still the ordinary topology on $\mathbb{R}$.&nbsp;</p>]]></content:encoded>
    <wp:post_name>158403979826</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/158301461731</link>
    <pubDate>Sun, 12 Mar 2017 20:53:42 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/158301461731</guid>
    <!--<wp:post_id>158301461731</wp:post_id>-->
    <wp:post_date>2017-03-11 23:53:42</wp:post_date>
    <wp:post_date_gmt>2017-03-12 07:53:42</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>“Meritocracy” and “public good”</title>
    <description></description>
    <content:encoded><![CDATA[<p>Sometimes a word coined with one intended meaning ends up with a very different one, and after you have fought the good fight and run the race to the finish, you need to just give up.&nbsp;</p><p>My favourite example is&nbsp;“<a href="https://en.wikipedia.org/wiki/The_Rise_of_the_Meritocracy">meritocracy</a>”, a word coined like&nbsp;“truthiness” and&nbsp;“factoid” to satirically attack a social trend. It failed completely: while&nbsp;“truthiness” worked, and&nbsp;“factoid” still has some of its negative connotation,&nbsp;“meritocracy” now means exactly the concept that it attacked: the supposed ideal of stack-ranking based on a straightforward one-dimensional metric for merit.&nbsp;</p><p>The economist Frances Woolley has <a href="https://ideas.repec.org/p/car/carecp/06-06.html">argued persuasively</a> that&nbsp;“public good’ is another example, of a slightly different sort. Economists use the term to refer to a precise (and largely non-existent) set of things; the literate public uses it to refer to a vague but almost completely different set.&nbsp;</p><p>In the technical sense a public good is something that is&nbsp;“non-excludable” (so a group of us can’t keep it to ourselves) and&nbsp;“non-rivalrous” (so we wouldn’t even want to, because everyone using it doesn’t reduce the value to us). &nbsp;In the normal sense, a public good is something it makes sense for the public to supply collectively -- something an economist would describe in terms of&nbsp;“positive externalities”</p><p>There’s almost no overlap. I was motivated to write this post by seeing educated and literate people on Twitter recently describe mass transit, education, and libraries as public goods. It’s the first week of the academic year at the University of Auckland, which provides a dramatic example of how all three of these goods are very much both rivalrous and excludable.&nbsp;</p><p>In contrast to “meritocracy” and&nbsp;“organic”, the multiple meanings of&nbsp;“public good” are actually ambiguous and can cause real confusion. It might have been better if the term hadn’t been invented, but the only solution I can see now is for economists to abandon it for some boring acronym, as they abandoned&nbsp;“natural rate of unemployment” for&nbsp;“NAIRU.”&nbsp;</p>]]></content:encoded>
    <wp:post_name>158301461731</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/158057063216</link>
    <pubDate>Mon, 06 Mar 2017 18:58:29 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/158057063216</guid>
    <!--<wp:post_id>158057063216</wp:post_id>-->
    <wp:post_date>2017-03-05 21:58:29</wp:post_date>
    <wp:post_date_gmt>2017-03-06 05:58:29</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Hearing things</title>
    <description></description>
    <content:encoded><![CDATA[<p>I spend more time than I like on aeroplanes, so I thought I’d write something about my experiences with headphones.</p><p>1. Having something to cut out the engine noise makes a noticeable difference to how much air travel sucks. Maybe as much as 10%.</p><p>2. When I first started to teach R courses for money, in 2001, I bought a pair of Bose noise-cancelling headphones. These make a big difference, and it’s easy to listen to music with them on.</p><p>3. On the other hand, it’s harder to sleep with big padded headphones, and the rigid headband conducts noise if you lean against the seat or the wall.&nbsp;</p><p>4. Inexpensive Sennheiser noise-isolating earbuds give almost as much protection against engine noise, and probably more against other sounds (such as babies), without a rigid headband. The CX 3.00 is the current version of what I had. &nbsp;In the US, you can get them for about $45. Recommended</p><p>5. My new earbuds are noise-isolating ones from Shure (model 215), using memory foam to fit in the ears. They’re a bit of a pain to put in, but the noise reduction is noticeably better. On the other hand, they’re more expensive -- even though these are the \$200 ones, not the \$500 ones. Still: highly recommended.&nbsp;</p><p>Your Mileage May Vary: I tend to find planes too hot (especially in the US), so I prefer earbuds over big padded headphones. &nbsp;Also, the music I listen to is either classical or vocal-led, and so doesn’t depend a lot on bass. The Sennheiser earphones are set quite loud, which can be a problem if you plug them into the airline systems, since the volume control doesn’t control in-flight announcements.&nbsp;</p>]]></content:encoded>
    <wp:post_name>158057063216</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/156732017926</link>
    <pubDate>Fri, 03 Feb 2017 11:45:51 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/156732017926</guid>
    <!--<wp:post_id>156732017926</wp:post_id>-->
    <wp:post_date>2017-02-02 14:45:51</wp:post_date>
    <wp:post_date_gmt>2017-02-02 22:45:51</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>The Ihaka Lectures</title>
    <description></description>
    <content:encoded><![CDATA[<p>The Stats department at the University of Auckland is <a href="https://www.stat.auckland.ac.nz/en/about/our-department/ihaka-lectures.html">inaugurating a public lecture series</a>, named to honour Ross Ihaka, who is planning to retire this year. We’re having four lectures, with speakers chosen to represent a wide range of areas where statistical computing and graphics is important.&nbsp;</p><p>Wednesday, March 8: <b>Hadley Wickham</b> (Chief Scientist, RStudio; (honorary) Associate Professor, University of Auckland). &nbsp;Hadley did an MSc in Statistics here in Auckland and a PhD with Di Cook’s statistical graphics group at Iowa State University. &nbsp;As a researcher, he studies data analysis and visualisation. As a software developer, he produces and encourages others to produce an ecosystem of packages for data manipulation and tidying. &nbsp;As a community leader, he has been an advocate for under-represented groups and for being generally nice to people whether they deserve it or not.</p><p>Wednesday, March 15: <b>Harkanwal Singh</b> (Data Editor, New Zealand Herald) Harkanwal was New Zealand’s first full-time data journalist, a role he created for himself and persuaded his employers to accept. He and his team at the Herald have since won several Canon Media Awards. Harkanwal’s work combines analysis and programming to create interactive visualisations and present complex data in stories the reader can understand.&nbsp;</p><p>Wednesday, March 22: <b>Genevera Allen</b> (Dobelman Family Junior Chair, Rice University). Genevera works in the intersection of statistics, medicine, and applied mathematics, &nbsp;with joint appointments at Rice University and Baylor College of Medicine. &nbsp;She has appeared in Forbes&nbsp;‘30 under 30′, as the North America representative in the &nbsp;International Biometric Society’s&nbsp;“Young Statisticians Showcase”, and in the American Statistical Association’s video&nbsp;“This is Statistics’.&nbsp;</p><p>Wednesday, March 29: <b>Ross Ihaka</b> (Associate Professor of Statistics, University of Auckland). Ross doesn’t actually need an introduction. &nbsp;If you’re reading this, you've probably heard of R. I’ll just mention his Pickering Medal from the Royal Society of New Zealand (for excellence and innovation in technology). &nbsp;</p><p>Lectures start at 6:30pm, in MLT1 on the ground (not basement) floor of&nbsp;<a href="https://www.google.co.nz/maps/place/Department+of+Statistics,+University+of+Auckland/@-36.8524341,174.7658516,17z/data=!3m1!4b1!4m5!3m4!1s0x6d0d47e32eb1d09d:0x868a73f26ed3e621!8m2!3d-36.8524341!4d174.7680403">38 Princes Street</a>. &nbsp;Refreshments from 6pm in the foyer of <a href="https://www.google.co.nz/maps/place/302%2F23+Symonds+St,+Auckland,+1010/@-36.8532432,174.7663845,17z/data=!3m1!4b1!4m5!3m4!1s0x6d0d47e385a59abf:0xc16eb826595c5310!8m2!3d-36.8532432!4d174.7685732">23 Symonds St</a></p>]]></content:encoded>
    <wp:post_name>156732017926</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/156650638586</link>
    <pubDate>Wed, 01 Feb 2017 14:55:04 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/156650638586</guid>
    <!--<wp:post_id>156650638586</wp:post_id>-->
    <wp:post_date>2017-01-31 17:55:04</wp:post_date>
    <wp:post_date_gmt>2017-02-01 01:55:04</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>When the bootstrap doesn’t work</title>
    <description></description>
    <content:encoded><![CDATA[<p>The bootstrap always works, except sometimes.&nbsp;</p><p>By&nbsp;‘works’ here, I mean in the weakest senses that the large-sample bootstrap variance correctly estimates the variance of the statistic, or that the large-scale percentile bootstrap intervals have their nominal coverage. &nbsp;I don’t mean the stronger sense that someone like Peter Hall might use, that the bootstrap gives higher-order accurate confidence intervals. &nbsp;So the bootstrap&nbsp;‘works’ for the median, even though not as well as for smooth functions of the mean.</p><p>Here are the reasons I know of why the bootstrap might fail</p><p>0. <b>Correlation</b>. The one that everyone knows about nowadays. &nbsp;If your data have structure, such as a time series, a spatial map, a carefully-structured experimental design, a multistage survey, a network, then you can’t hope to get the right distribution by resampling in a way that doesn’t respect that structure.&nbsp;</p><p>1. <b>Constraints</b>: Suppose $X_n\sim N(\theta,1)$ and we know $\theta\geq 0$. The maximum likelihood estimator of $\theta$ is $\hat\theta=\max(\bar X,0)$. If $\theta&gt;0$ there isn’t a problem asymptotically (or at a more sophisticated analysis, if $\theta\gg 1/\sqrt{n}$ there isn’t). &nbsp;But if $\theta=0$ the sampling distribution of $\hat\theta$ is a 50:50 mixture of a spike at zero and the positive half of a $N(0,n^{-1})$ distribution. &nbsp;The bootstrap distribution is also a mixture of a spike at zero and and a half-normal, but the mass on the spike does not converge to 0.5 (or to anything else) as the sample size increases. The problem is that the height of the spike is $\Phi(\bar X\sqrt{n})$, so the height converges in distribution to $U(0,1)$.&nbsp;</p><p>2. <b>Extrema</b>. &nbsp;Consider $X\sim U(\theta,1)$. The bootstrap replicates $\theta^*$ have a distribution that puts mass $0.632=1-e^{-1}$ on the smallest observation, $e^{-1}(1-e^{-1})\approx 0.233$ on the second smallest, and so on geometrically. We always have $\theta^*\geq\hat\theta$, and the bootstrap distribution stays very discrete as the sample size increases.</p><p>3. <b>Lack of smoothness (cube-root asymptotics)</b>&nbsp;Tukey’s shorth, the mean of the shortest half of the data, converges to the mean at $n^{-1/3}$ rate instead of the usual $n^{-1/2}$. The same is true for the least-median-of-squares regression line, the isotonic regression estimator, and the semiparametric maximum likelihood estimator of a convex density. These all have non-Normal limiting distributions and the bootstrap fails for all of them.</p><p>4. <b>Lack of smoothness (worse)</b>&nbsp;Suppose your statistic of interest is the proportion of the distribution represented by non-zero probability mass points. The proportion of non-unique observations estimates this, and is zero for a continuous distribution. &nbsp;It’s not even close to zero for bootstraps of a sample from a continuous distribution.</p><p>5. <b>Serious outliers</b>: Suppose $X$ comes from a Cauchy distribution and you don’t realise and still try bootstrap inference for the mean. There isn’t a mean. The bootstrap intervals don’t even correctly cover the center of symmetry (where the mean would be if there was one). The problem here is that a large collection of new samples from the true distribution would contain outliers of very different sizes (some worse than the original sample). &nbsp;The bootstrap replicates contain multiple outliers the same size as in the original sample.</p><p>6. <b>Zero derivative</b>. Suppose $X\sim N(\sqrt{\theta},1)$ with $\theta\geq 0$ and your statistic is $\bar X^2.$ [edit so that $\theta$ is what $\bar X^2$ estimates] If $\theta&gt; 0$, that’s fine: $\sqrt{n}(\hat\theta-\theta)$ is asymptotically Normal and the bootstrap works. But if $\theta=0$,&nbsp;$\sqrt{n}(\hat\theta-\theta)$ converges to point mass at zero, and it’s $n(\hat\theta-\theta)$ that converges to $\chi^2_1$. &nbsp;Since $\theta^*$ is not taken from a distribution with mean zero (it has mean $\bar X$), the convergence doesn’t work. The distributions are not regular at $\theta=0$. For a couple of realistic examples, essentially this happens to the IDI statistic in medical diagnostics, and also in regression if you try to test the joint null hypothesis $\theta_1=0 \cup \theta_2=0$ using $\hat\theta_1\hat\theta_2$ as the statistic. [edit&nbsp;for&nbsp;‘pleiotropy’ in genetics]</p><p>7. <b>Sparse estimators</b>: The lasso, for example, doesn’t bootstrap. &nbsp;The problem is related to numbers 1 and 6: zero is a special value for the regression coefficients, and the distribution of estimates changes as the true regression coefficient approaches zero.</p><p>8. <b>Overfitting</b>: &nbsp;If your predictive model is overfitted, all the bootstrap replicates will also be overfitted. &nbsp;You need some way to subtract off the optimism.&nbsp;</p><p>Many of these are fixable with variations on the bootstrap: there are time-series bootstraps and survey bootstraps for problem 0, ideas like the .632 bootstrap for problem 8 and subtle penalised and thresholded bootstraps for problem 7. There are also subsampling bootstraps, which work (in theory) very generally as long as you know the convergence rate of your estimator. &nbsp;But the simple nonparametric bootstrap can fail, and it’s good to have some idea of when it does.&nbsp;</p>]]></content:encoded>
    <wp:post_name>156650638586</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/156650549921</link>
    <pubDate>Wed, 01 Feb 2017 14:52:39 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/156650549921</guid>
    <!--<wp:post_id>156650549921</wp:post_id>-->
    <wp:post_date>2017-01-31 17:52:39</wp:post_date>
    <wp:post_date_gmt>2017-02-01 01:52:39</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Te Reo Māori in schools</title>
    <description></description>
    <content:encoded><![CDATA[<p>Having Te Reo Māori taught as part of the standard curriculum in NZ schools seems like a reasonable idea to me. A few reasons:</p><p>1. Learning more than one language is good for understanding grammar and pronounciation, and it doesn’t matter a lot which one. &nbsp;“Grammar” is, almost by definition, the set of rules for correct sentences that native speakers follow most of the time without thinking, so it’s hard to talk and think about grammar sensibly if you’ve never tried to produce correct sentences in another language. </p><p>2. There isn’t a compelling single alternative purely on the basis of narrowly-defined utility. &nbsp;In eastern Canada it’s obviously a good idea to learn French. In most of the US, Spanish is the clear choice. There isn’t something like that here, as there wasn’t in Australia where I grew up.</p><p>3. Te reo is relatively easy to pronounce and hear for English speakers. &nbsp;It uses the same alphabet, it has few or no sounds not present in NZ English (word-initial [$ɾ$] and [$ŋ$] might count), and only makes one phonetic distinction that English doesn’t (long vs short vowels). Compare to Mandarin Chinese, which has tones, characters, and a bunch of alvelo-palatal and retroflex consonants, or to Thai which makes a three-way distinction between voiced, unvoiced unaspirated, and unvoiced aspirated consonants. Or even to French.</p><p>4. Formal speeches and welcomes in te reo seem to be part of life here, and they’re probably more interesting if you can understand parts of them. </p><p>5. It promotes the radical left-wing agenda of brainwashing children into believing the country had a genuine pre-European history and culture. </p><p>It should be easier to implement a language-teaching policy with the same language being used everywhere, but if people really wanted their kids to learn Spanish or Indonesian or Chinese or, whatever, Pitjantjatjara I wouldn’t be opposed to having alternatives. There currently isn’t much sign that they do. </p>]]></content:encoded>
    <wp:post_name>156650549921</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/156431074901</link>
    <pubDate>Fri, 27 Jan 2017 18:24:53 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/156431074901</guid>
    <!--<wp:post_id>156431074901</wp:post_id>-->
    <wp:post_date>2017-01-26 21:24:53</wp:post_date>
    <wp:post_date_gmt>2017-01-27 05:24:53</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Case-control sampling and pseudo-Rsquareds</title>
    <description></description>
    <content:encoded><![CDATA[<p>So, I have been asked a few times how to compute $R^2$ for models fitted to survey data. Initially the questions were about the ordinary linear-regression $R^2$, which is easy because it’s the ratio of two variances, and we can estimate variances. More recently, people have been asking about the Nagelkerke pseudo-$R^2$ in logistic regression.&nbsp;</p><p>It’s not immediately obvious how to define the Nagelkerke $R^2$ under complex sampling. My approach was to consider the Cox--Snell $R^2$ that precedes it, which is an estimate of a well-defined population quantity: $\log (1-R^2)$ is the mutual information between the predictors and outcome. &nbsp;Replacing the likelihood in the definitions by the estimated population likelihood gives a design-based definition of the Cox--Snell $R^2$, and the Nagelkerke $R^2$ is a simple rescaling. I’ve got a<a href="https://arxiv.org/abs/1701.07745"> preprint here</a>&nbsp;and <a href="https://github.com/tslumley/pseudorsq">code here</a>. [Update: the paper is out, <a href="http://onlinelibrary.wiley.com/doi/10.1111/anzs.12187/abstract">at ANZ J Stat</a>]</p><p>With any design-based statistic for logistic regression it’s natural to look at case--control designs as an example. These have highly informative sampling, so that ignoring the sampling weights will lead to bias. Usually we don’t worry about the bias, because in a logistic regression model it’s all in the intercept, and not in the other regression parameters. &nbsp;However, when you have a statistic that isn’t just a function of the regression slopes, you have to worry about whether the bias reappears.</p><p>It does.</p><p>The following picture shows a series of simulated case-control designs with $X\sim N(0,1)$, $\textrm{logit}\,P[Y=1]=-6+x$, and 1,2,5,10, or 20 controls per case. The three lines are the ordinary Nagelkerke $R^2$, Cox--Snell $R^2$, and a trendy new one due to Tjur. &nbsp;The x-axis gives the control sampling percentage, and the horizontal dashed lines are the $R^2$ values for a model fitted to the full cohort.&nbsp;</p><figure data-orig-width="1263" data-orig-height="930" class="tmblr-full"><img src="https://68.media.tumblr.com/23ec773a473da73fe01c8844d7323ee4/tumblr_inline_okfanj9ArE1s1hdxy_540.png" alt="image" data-orig-width="1263" data-orig-height="930"></figure><p>As you can see, there’s a lot of bias. &nbsp;Under case--control sampling, the standard forms of all three of these $R^2$ statistics are inflated relative to the value you would get with full-cohort data.&nbsp;</p><p>While I could imagine someone coming up with a reason to prefer the in-sample $R^2$ rather than the full-cohort one, the usual motivation for case-control sampling is to estimate the properties of the cohort and the population from which it comes, not properties of the sample. So, design-based versions of these statistics should be useful. &nbsp;My preprint shows how to define and compute design-based versions of Nagelkerke and Cox--Snell statistics. The Tjur statistic is easy: it’s the mean of $\hat Y$ in cases minus the mean in non-cases, so a design-based version just needs weighted means.</p><p>As evidence that the deisgn-based versions work, here’s the same sequence of designs with survey-weighted models and design-based $R^2$ statistics:</p><figure data-orig-width="1263" data-orig-height="930" class="tmblr-full"><img src="https://68.media.tumblr.com/e8b9c2b0135493e525737f9915fba095/tumblr_inline_okf9q9IgGc1s1hdxy_540.png" alt="image" data-orig-width="1263" data-orig-height="930"></figure><p>I was surprised that Cox and Snell didn’t notice and comment on this phenomenon, but on looking up the reference I found it’s an exercise in their book&nbsp;“Analysis of Binary Data”</p>]]></content:encoded>
    <wp:post_name>156431074901</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/156007757906</link>
    <pubDate>Wed, 18 Jan 2017 10:57:45 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/156007757906</guid>
    <!--<wp:post_id>156007757906</wp:post_id>-->
    <wp:post_date>2017-01-17 13:57:45</wp:post_date>
    <wp:post_date_gmt>2017-01-17 21:57:45</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>A bus-watching bot</title>
    <description></description>
    <content:encoded><![CDATA[<p>When it’s up, the account @<a href="https://twitter.com/tuureiti">tuureiti</a> on Twitter tweets a summary of the state of Auckland buses -- at the moment, every 15 minutes.</p><figure data-orig-width="500" data-orig-height="200" class="tmblr-full"><img src="https://68.media.tumblr.com/39c4c160c5ab6b00e5b7c9bce5a71bde/tumblr_inline_ojy1kaBwrY1s1hdxy_540.png" alt="image" data-orig-width="500" data-orig-height="200"></figure><p>Q: Can you explain that picture?</p><p>A: Every bus that the Auckland Transport GTFS feed knows about has a dot on the graph. &nbsp;The GTFS feed has a&nbsp;‘delay’ field that says how far ahead or behind schedule the bus is, separated by whether the next event is&nbsp;‘arrival’ or&nbsp;‘departure’. &nbsp;I’ve coded departures as green (‘on time’) if they are between one minute early and 5 minutes late, and arrivals as on time if they are less than 5 minutes late.&nbsp;</p><p>Q: Are there really buses more than half an hour late? Isn’t it more likely to be a data problem?</p><p>A: The extreme outliers are definitely more likely to be a data problem. &nbsp;One possibility is that the driver puts the wrong trip number into the system. Some of them will be bus failures, but those trips tend to be cancelled fairly quickly.</p><p>Q: So shouldn’t you truncate the axis a bit?</p><p>A: I already did. I might truncate it more, later.</p><p>Q: What does `tuureiti’ mean? Is it a really contrived acronym?</p><p>A: It’s the Māori word for&nbsp;‘late’, in the writing system that doesn’t use modified characters, because Twitter won’t let me. &nbsp;In the more-common writing system it would be&nbsp;‘tūreiti’.&nbsp;</p><p>Q: Why use Twitter, rather than just putting the data up on a web page?</p><p>A: It’s actually easier to use Twitter, because the University of Auckland website runs off a&nbsp;‘Content Management System’.&nbsp;</p><p>Q: Is it really hard to write a bot?</p><p>A: No. Even I can do it. And I’m old.</p><p>Q: What impressive technologies did you use to write the bot?</p><p>A: Um. R? The <b>jsonlite</b> package to read the GTFS feed, the <b>twitteR</b> package to send tweets, and <b>beeswarm</b> to draw the dotplots. It only tweets every 15 minutes; it doesn’t have to be, like, efficient or anything.</p><p>Q: Are you going to add more exciting features in the future?</p><p>A: Maybe. But I hope we’ll soon have a much better system based on Tom Elliott’s PhD research.&nbsp;</p><p>Q: Why is the&nbsp;‘on-time’ percentage so much worse than the official figures?</p><p>A: Because the official figures don’t measure punctuality after the bus actually starts its route. &nbsp;The official definition is bogus, but for better official quality-control purposes you might not want my percentage either -- mine is for all buses at all stops, but you’d probably want to restrict to the set of stops on the published timetables. &nbsp;</p><p>Q: This comes with the usual disclaimers, right?</p><p>A: Definitely. Not intended to prevent, diagnose, cure, or treat any condition. Your mileage may vary. Keep out of reach of babies and young children. Product of more than one country. May contain nuts.&nbsp;</p><p>[Update: I need to recheck the meaning of negative arrival vs departure delays: is a negative arrival delay an early arrival at a timepoint or an early departure from the last stop?]</p>]]></content:encoded>
    <wp:post_name>156007757906</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/155775534711</link>
    <pubDate>Fri, 13 Jan 2017 09:37:39 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/155775534711</guid>
    <!--<wp:post_id>155775534711</wp:post_id>-->
    <wp:post_date>2017-01-12 12:37:39</wp:post_date>
    <wp:post_date_gmt>2017-01-12 20:37:39</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Mature and premature optimisation</title>
    <description></description>
    <content:encoded><![CDATA[<p>Earlier this week I wrote some code that wasted 90% of its time moving data around in memory, because I just&nbsp;‘grew’ a long vector with the idiom&nbsp;</p><blockquote><p>stuff&lt;-c(stuff, morestuff)</p></blockquote><p>Here’s the <a href="https://github.com/tslumley/bigchisqsum/commit/25ef9dbf8219d313d2053f5baebc526835c0283f">github commit</a> that changed the code.</p><p>I’m writing about it because it illustrates a few useful points. &nbsp;First, the inefficient code was <b>absolutely</b> the right choice initially. &nbsp;I didn’t know how long each additional vector would be, and while I could have worked it out in principle, in practice I would quite likely have got it wrong. Or at least not been sure it was right.</p><p>Second, the reason I know the code was inefficient was that I <b>profiled</b> it. &nbsp;There were lots of potential inefficiencies in the code because I was trying to write it as simply as possible. &nbsp;I didn’t know in advance which (if any) would matter, and I was surprised when the profiler found most of the time was being spent in the function c( ).&nbsp;</p><p>Third, the new code illustrates an approach to use when you don’t know (or don’t want to work out) how big the pieces of vector will be. &nbsp;I work out &nbsp;a <b>crude upper bound</b> for the possible size of the vector and have a variable that points to the first unused position so I can put new numbers in the right place. Then, at the end, I return just the part of the vector that I actually used.&nbsp;</p>]]></content:encoded>
    <wp:post_name>155775534711</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/155648583811</link>
    <pubDate>Tue, 10 Jan 2017 13:56:13 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/155648583811</guid>
    <!--<wp:post_id>155648583811</wp:post_id>-->
    <wp:post_date>2017-01-09 16:56:13</wp:post_date>
    <wp:post_date_gmt>2017-01-10 00:56:13</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Fixing an infelicity in ‘leaps’</title>
    <description></description>
    <content:encoded><![CDATA[<p>The&nbsp;‘leaps’ package for R is ancient -- this is its <strike>tenth</strike>&nbsp;twentieth year on CRAN. &nbsp;It uses old Fortran code by the Australian computational statistician Alan Miller. The Fortran 90 versions are <a href="http://jblevins.org/mirror/amiller/">on the web</a>, but Fortran 90 compilation with R wasn’t portable back then, so I used the older Fortran 77 version.&nbsp;</p><p>The main point back in 1997 was to provide a version of the ‘leaps()’ function in S, which uses a branch-and-bound algorithm to do exhaustive search for the best (smallest residual-sum-of-squares) model of each size. &nbsp;I was interested in exhaustive search back then as a step towards multi-model inference -- either formal modelling averaging, as Adrian Raftery had taught us in BIOST572, or visualisation of what variables were substitutes or complements for each other. &nbsp;For example, did systolic and diastolic blood pressure tend to be in together (so maybe their difference, pulse pressure, was important), or did they tend to substitute for each other. The `plot` method in the package lets you look at this sort of thing, inspired by a talk I saw Merlise Clyde give.</p><p>However, exhaustive search isn’t as popular today -- despite the clever algorithm, it takes a long time. It has to; it’s an NP-complete problem. &nbsp;People have started using the forward and backward search algorithms in the package. I included them mostly because they were there in the Fortran code. &nbsp;You’d think that would all be straightforward, but it resulted in what is either a bug in the code, or a bug in the documentation and a feature in the code that’s less useful than it was. &nbsp;</p><p>If you look at efficient construction of an exhaustive-search algorithm, the working step is rotations to swap variables in and out of the model. &nbsp;It’s convenient to start off with a preprocessing step that fits a model with the first variable, a model with the first two variables, a model with the first three variables, and so on. &nbsp;Everything ends up getting initialised; it’s basically free; you then go on to do the search.&nbsp;</p><p>If you’re doing exhaustive search, those models would eventually be considered anyway. If you’re doing forward or backward selection they might not be. However, if you think of forward or backward selection just as a computationally-cheap approximation to exhaustive search, since you’ve got those models anyway, you might as well look at them. &nbsp;That’s what the code does. It’s possible for, say, the model with the first three variables to be better than the three-variable model found by forward selection. If it is, you get the better model.&nbsp;</p><p>In the modern world we don’t really think of forward and backward selection that way. They’re specific fitting algorithms you might use as part of a selection/regularisation strategy. They’re also things you might teach for historical reasons. &nbsp;In either case, it may matter that the sequence of models you get out is nested; you don’t want a better model, even for free.</p><p>So. Bug reports. Attempts to debug twenty-year old Fortran code written by someone else. Attempts to tweak twenty-year old Fortran code written by someone else. Searches for work-arounds. Swearing. Finally, &nbsp;version 3.0 of the leaps package, now on CRAN.&nbsp;</p><p>If you have nbest=1 (asking for just one model of each size), they will be nested properly for forward or backward selection. &nbsp;If you have nbest&gt;1 I can’t see how to avoid the extra models, but you will be warned. &nbsp;As always, this isn’t an issue for exhaustive search.</p>]]></content:encoded>
    <wp:post_name>155648583811</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/155363960126</link>
    <pubDate>Wed, 04 Jan 2017 12:36:16 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/155363960126</guid>
    <!--<wp:post_id>155363960126</wp:post_id>-->
    <wp:post_date>2017-01-03 15:36:16</wp:post_date>
    <wp:post_date_gmt>2017-01-03 23:36:16</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Learning the Monty Hall problem</title>
    <description></description>
    <content:encoded><![CDATA[<p>As Wikipedia gives it</p><blockquote><p>Suppose you're on a game show, and you're given the choice of three doors: Behind one door is a car; behind the others, goats. You pick a door, say No. 1, and the host, who knows what's behind the doors, opens another door, say No. 3, which has a goat. He then says to you, "Do you want to pick door No. 2?" Is it to your advantage to switch your choice?</p></blockquote><p>Under what Wikipedia calls&nbsp;“the standard assumptions”, you should switch. The standard assumption is that the host will always open a door, and it will always be a door with a goat behind it.</p><p>One of my colleagues doesn’t believe the standard assumptions are reasonable. What I want to look at here is what should you do based on any observed history of the game -- and if the standard assumptions are in fact true, how fast will you learn that switching is the right decision?</p><p>I’ll start out with you assuming that the host always opens a door so his only choice depending on your actions is which of the doors to open. &nbsp;If you’ve chosen the car it doesn’t matter which door he opens; if you’ve chosen a goat he has a probability $p$ of opening the door with the car. &nbsp;I’ll also pretend we only know what’s behind the door he opens, when in fact we learn about all the doors (this doesn’t matter in the long run). You should switch if $p&lt;0.5$.</p><p>We could consider just a prior supported on $p=0.5$ (door chosen at random) and $p=0$ (the standard assumption), or we can use a prior over all of $[0,\,0.5]$ or even all of $[0,\,1]$. Let’s use a uniform on all of $[0,\,1]$. &nbsp;In 2/3 of cases the car won’t be behind your door, so it’s behind his door with probability $p$, in the other 1/3 of cases you’ve got the car, so it’s behind his door with probability 0. Suppose we run the game a few times and the host never opens a door with a car behind it. The posterior distribution of $p$ (by numerical quadrature) is&nbsp;</p><figure data-orig-width="458" data-orig-height="338" class="tmblr-full"><img src="https://68.media.tumblr.com/a8debef918b15c9259a9e0e7ce5144d0/tumblr_inline_oj2w8yivwR1s1hdxy_540.png" alt="image" data-orig-width="458" data-orig-height="338"></figure><p>So if the standard assumptions are true, contestants should learn quickly that $p$ is small. &nbsp;</p><p>It’s more dramatic than that, though: under this prior you should be indifferent between switching and not switching. Seeing even <b>one</b> run where a car could have been revealed, and wasn’t, would be enough to make your posterior expected utility favour switching. &nbsp;In order not to favour switching after a few shows, you’d need a prior that was strongly in favour of either $p=0.5$ or $p&gt;0.5$. So, if the standard assumptions are true, rational players will quickly learn to switch.&nbsp;</p><p>It might be, though, that you don’t assume the host has to open a door. You’ve now got three parameters: the probability $q_1$ of opening a door if you’ve chosen the car, the probability $q_2$ of opening the door with the other goat if you’ve chosen a goat, and the probability $q_3$ of opening the door with the car if he doesn’t open the door with the goat. The standard assumptions are $q_1=1$, $q_2=1$, $q_3=0$</p><p>The Nash equilibrium, according to Wikipedia, is $q_1=0$, $q_2=0$, $q_3=0$ for the host, and not switching for the player. &nbsp;On the other hand, If the standard assumptions are true, we’ll know after seeing one game that the host isn’t playing the minimax strategy -- in fact, even the first contestant will know.&nbsp;</p><p>With this parametrisation, $q_1/3$ is the joint probability you have the car and the host reveals a goat, and $2q_2/3$ is the joint probability you have a goat and the host reveals the other goat. So, conditional on seeing a goat you should switch if $q_1&lt;2q_2$. &nbsp;A flat prior implies switching even for the first contestant. &nbsp;To get extreme, suppose we had a $\textrm{Beta}(10,1)$ prior for $q_1$ and a&nbsp;$\textrm{Beta}(1,10)$ prior for $q_2$. &nbsp;If the standard assumptions are true, we still learn that switching is better after 7 rounds more often than not, and by ten rounds almost all the time.&nbsp;</p><p>In summary, if there’s an opportunity to watch even a few rounds of the game before playing, you don’t need to argue with someone about whether the standard assumptions are true. Just watch and learn.&nbsp;</p>]]></content:encoded>
    <wp:post_name>155363960126</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/155194690691</link>
    <pubDate>Sat, 31 Dec 2016 18:18:21 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/155194690691</guid>
    <!--<wp:post_id>155194690691</wp:post_id>-->
    <wp:post_date>2016-12-30 21:18:21</wp:post_date>
    <wp:post_date_gmt>2016-12-31 05:18:21</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>The ‘iris’ data</title>
    <description></description>
    <content:encoded><![CDATA[<p>Fisher’s famous&nbsp;‘iris’ data set is a convenient example because it’s small and low-dimensional and has very marked differences between groups. These characteristics also make it a bad example (edit: at least for modern machine learning), because the behaviour of small, low-dimensional classification problems is a very poor guide to the behaviour of large or high-dimensional ones.&nbsp;</p><p>That’s all obvious. What’s less well known is that the data set is an example of pseudocontext in education. You give the students the data set, emphasising that these are real measurements (though perhaps not bothering to tell them where the Gaspé Peninsula is, or to show them pictures of the flowers). &nbsp;You tell the students that this is a classification (supervised-learning) problem: the goal is to come up with a rule to classify future iris specimens. &nbsp;Or you drop the ‘species’ variable and make it a clustering (unsupervised learning) problem to work out the species.&nbsp;</p><p>None of that is what Fisher or Anderson wanted. &nbsp;Anderson, writing about&nbsp;<a href="http://biostor.org/reference/11559/page/10">‘The Species Problem in Iris’,</a> says that there are many distinguishing characteristics in the field, but that the flowers aren’t any use in identifying dried specimens because they shrivel up</p><blockquote><p>Perianth dimensions from herbarium material are completely unreliable in these species, and for that reason have been largely omitted from the keys and descriptions</p></blockquote><p>The reason&nbsp;<a href="http://biostor.org/reference/11559/page/15">Anderson</a> and <a href="http://onlinelibrary.wiley.com/doi/10.1111/j.1469-1809.1936.tb02137.x/full">Fisher</a>&nbsp;wanted statistical analysis of the data was to address the hypothesis that Iris versicolor was a hybrid of the other two (rather than the differences from Iris virginica having accumulated by many mutations over a long period). &nbsp;The numbers of chromosomes were right for this explanation, and there were other supporting factors. &nbsp;He argued that the Iris versicolor should be intermediate between the other species, and that it should be twice as far from Iris setosa as from Iris virginica&nbsp;</p><p>Anderson’s paper included this example of multivariate data visualisation to demonstrate the goodness of fit of the hypothesis</p><figure data-orig-width="752" data-orig-height="554" class="tmblr-full"><img src="https://68.media.tumblr.com/0005021b342ce9975a50000d69fd7492/tumblr_inline_oj19wgoykd1s1hdxy_540.png" alt="image" data-orig-width="752" data-orig-height="554"></figure><p>Fisher described the problem similarly</p><blockquote><p>It is of interest in association with I. setosa and I. versicolor in that Randoph (1934) has ascertained and Anderson has confirmed that, whereas I. setosa is a “diploid” species with 38 chromosomes, I. virginica is “tetraploid”, with 70, and I. versicolor, which &nbsp;is intermediate &nbsp;in &nbsp;three &nbsp;measurements, &nbsp;though &nbsp;not &nbsp;in sepal breadth, is hexaploid. &nbsp;He has suggested the interesting possibility that I. versicolor is a polyploid hybrid of &nbsp;the two other species. We shall, therefore, consider whether, when we &nbsp;use the linear compound of the four measurements most appropriate for discriminating three such species, the mean value for I. versicolor takes an intermediate value, and, if so, whether it differs twice as much from I. setosa as from I. virginica, as might be expected, if the effects of &nbsp;genes are simply additive, in a hybrid between a diploid and a tetraploid species.&nbsp;<br></p></blockquote><p>After some analysis he found</p><blockquote><p>The differences do seem, however, to be remarkably closely in the ratio 2 : 1. Compared with &nbsp;this &nbsp;standard, I. virginica would &nbsp;appear &nbsp;to have &nbsp;exerted &nbsp;a &nbsp;slightly &nbsp;preponderant influence. The departure from expectation is, however, small, and we &nbsp;have the material for making at least an approximate test of &nbsp;significance</p></blockquote><p>So, in summary, Anderson and Fisher said</p><ul><li>Accurate multivariate flower dimensions are not necessary for discriminating these species in the wild<br></li><li>Accurate multivariate flower dimensions aren’t available in preserved specimens</li><li>However, the measurements do allow a fairly clear test of a specific genetic hypothesis</li></ul><p>Now that actual genetic information can be measured, &nbsp;it’s possible to <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2735315/">confirm that Anderson was right</a></p><blockquote><p>These data confirm Anderson's hypothesis that I. versicolor is an allopolyploid involving progenitors of I. virginica and I. setosa. The number of 18–26S rDNA loci in I. versicolor is similar to that of progenitor I. virginica, suggestive of a first stage in genome diploidization. The locus loss is targeted at the I. setosa-origin subgenome, and this is discussed in relation to other polyploidy systems.</p></blockquote><p>This sort of promiscuous hybridisation and chromosome doubling is common in crop plants -- a fact that’s important in modern agricultural genetics, since it makes the assembly phase of genome sequencing much harder. The real story of the iris data is more interesting than the discrimination problem. And it’s <b>&nbsp;</b>true.&nbsp;</p>]]></content:encoded>
    <wp:post_name>155194690691</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/153576027106</link>
    <pubDate>Thu, 24 Nov 2016 11:47:23 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/153576027106</guid>
    <!--<wp:post_id>153576027106</wp:post_id>-->
    <wp:post_date>2016-11-23 14:47:23</wp:post_date>
    <wp:post_date_gmt>2016-11-23 22:47:23</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Making survey statistics boring and inefficient</title>
    <description></description>
    <content:encoded><![CDATA[<p>Last night, Alastair Scott <a href="http://www.royalsociety.org.nz/2016/11/23/jones-medal-statistics-for-sampling-and-public-health/">was awarded</a> the Jones Medal by the Royal Society of New Zealand. The medal, named after <a href="https://en.wikipedia.org/wiki/Vaughan_Jones">Vaughan Jones</a>, is for lifetime achievement in the mathematical sciences.&nbsp;</p><p>Alastair made contributions to both theoretical and applied statistics in two main areas, as the title of this post indicates. &nbsp;With Jon Rao and others (including me), he worked on making design-based inference boring, and with Chris Wild and others, on making it inefficient.&nbsp;</p><p>At one time, design-based inference was a significant challenge. &nbsp;Complex surveys needed to be designed and analysed by specialists, and the available estimators and models were quite limited. That’s not really an issue now. There’s wide range of exploratory techniques, summary statistics, and models available even in standard software. It’s easy to add more as needed. The only real exception is models for relationships between individuals-- mixed models are still hard if the clusters in the model don’t line up with the sampling units in the design. &nbsp;Survey statisticians can now focus on the interesting and difficult aspects of survey research, not on mere computation.&nbsp;</p><p>Lots of people (notably, at Statistics Canada) worked on making survey statistics boring. Alastair’s biggest single contribution was the analysis of what we would now call <a href="https://www.jstor.org/stable/2241033?seq=1#page_scan_tab_contents">‘working score’ tests and ‘working likelihood ratio’ tests</a> under complex sampling. &nbsp;In the early 1980s, with Jon Rao, he studied the statistics you’d get by treating categorical data as a simple random sample and fitting log-linear models. They worked out the actual sampling distribution of these statistics, which isn’t the usual chi-squared distribution, and also found simple approximations to these distributions. &nbsp;The&nbsp;‘first-order’ and&nbsp;‘second-order’ Rao-Scott tests are now standard. &nbsp;After I moved to Auckland, he and I also applied the same idea to generalised linear models and the Cox model.&nbsp;</p><p>The other research contribution Alastair is best known for, this time in biostatistics, is his work with Chris Wild on s<a href="http://www.jstor.org/stable/2532141">emiparametric maximum likelihood estimation for generalised linear models under case-control sampling</a> (and other two-phase sampling). &nbsp;We’ve known since the 1970s that you can fit a logistic regression model to a case-control sample by just ignoring the sampling. &nbsp;For other models or for sampling based on predictors as well as outcome it’s more complicated. Chris and Alastair worked out a semiparametric likelihood, and showed that it was much less complicated than you might have guessed, so that estimation was actually feasible. &nbsp;It’s fairly routine for people to rediscover special cases of this estimator without knowing about the original; Alastair is nicer about this than many of us would be.&nbsp;</p><p>The Scott-Wild estimator&nbsp;‘makes survey statistics inefficient’ in the sense that it provides feasible estimators that are more efficient than the design-based estimator. &nbsp;As readers of this blog know, I’m attempting to problematise this concept of efficiency, but even if I turn out to be right it’s still an important criterion.&nbsp;</p><p>Getting a&nbsp;‘mathematics’ award like this for a statistician requires some significant theoretical contributions. It also doesn’t hurt to have a national and international reputation for being friendly and helpful.&nbsp;</p><p><b>Update</b>: from the <a href="https://www.teawamutu.nz/courier/archive/2016/2016-11-29.pdf">local newspaper</a> in the town where Alastair grew up</p><figure class="tmblr-full" data-orig-height="836" data-orig-width="797"><img src="https://67.media.tumblr.com/80a44e21f6c4026d18c4c05c3b3cadcb/tumblr_inline_ohh2gkte8W1s1hdxy_540.png" data-orig-height="836" data-orig-width="797"></figure>]]></content:encoded>
    <wp:post_name>153576027106</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/153155911601</link>
    <pubDate>Mon, 14 Nov 2016 16:36:10 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/153155911601</guid>
    <!--<wp:post_id>153155911601</wp:post_id>-->
    <wp:post_date>2016-11-13 19:36:10</wp:post_date>
    <wp:post_date_gmt>2016-11-14 03:36:10</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Brief quake summary for overseas people</title>
    <description></description>
    <content:encoded><![CDATA[<p>There was a <a href="http://info.geonet.org.nz/display/quake/2016/11/14/M7.5+Kaikoura+Quake%3A+What+we+know+so+far">pair of big earthquakes</a> in New Zealand last night (late Sunday morning UTC, just after midnight Monday NZ time).&nbsp;They were about half-way between Wellington and Canterbury, in the northeast of the South Island. There have been a lot of smaller related quakes <a href="https://twitter.com/kamal_hothi/status/797921821630705664">as well</a>.</p><p>Auckland and Dunedin are unharmed.&nbsp;</p><p>Christchurch was shaken but not seriously damaged; some people were evacuated because of the potential for a big tsunami.</p><p>There is damage in Wellington, especially in the central business district, exacerbated by even more rain and wind than is usual. Wellington is also worried about aftershocks.</p><p>Closer to the quakes, damage is severe. Buildings were destroyed, with two deaths being reported. &nbsp;The main road and rail line along the coast are broken in multiple places; they aren’t coming back soon. &nbsp;</p><p>Over a much wider area of the country, bridges, rail lines, and water supplies are being checked (a lot of NZ runs on untreated groundwater).&nbsp;</p><p><a href="http://www.radionz.co.nz/">Radio NZ</a> is the place to go for more information.&nbsp;</p><p>(And here’s a <a href="https://twitter.com/radionz/status/797999347413590017">snippet of the midnight news broadcast</a> from Wellington as the quake happened)</p>]]></content:encoded>
    <wp:post_name>153155911601</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/153024320551</link>
    <pubDate>Fri, 11 Nov 2016 17:26:11 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/153024320551</guid>
    <!--<wp:post_id>153024320551</wp:post_id>-->
    <wp:post_date>2016-11-10 20:26:11</wp:post_date>
    <wp:post_date_gmt>2016-11-11 04:26:11</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Changes in turnout and preference</title>
    <description></description>
    <content:encoded><![CDATA[<p>So, as you know, Hillary Clinton narrowly lost the Electoral College and probably narrowly won the popular vote. And there’s lots of theorising about how these huge swings came about and what they mean. An important first step is to think about how big the swings really were.</p><p>Here are some graphs of county-level votes in 2012 and 2016. &nbsp;In all the graphs, the number of votes for the candidate is <b>scaled by the 2012 tota</b>l for the county, and is then <b>weighted by that same 2012 total.</b>&nbsp;&nbsp;</p><p>Scaling is necessary because otherwise LA County dominates the map and all the smaller counties are crushed down towards the origin. &nbsp;However, using the same total for both years lets us distinguish to a limited extent between swings from one party to the other and changes in turnout. &nbsp;And using weights lets us get a better visual idea of the actual number of votes involved -- a small proportion of LA County is a lot more votes than the same small proportion of Glynn County, GA.</p><p>You know how scaling is done. Weighting is done by making the hexagons bigger. &nbsp;In these scatterplots, the area of a hexagon is proportional to the total number of 2012 votes in the hexagonal grid cell.&nbsp;</p><p><span style="font-size: 14px;">First, change in the individual parties</span><br></p><figure data-orig-width="577" data-orig-height="693" class="tmblr-full"><img src="https://65.media.tumblr.com/2b991caedff19321b2e5f3fbf9d05f6e/tumblr_inline_oggly9I9Rp1s1hdxy_540.png" data-orig-width="577" data-orig-height="693"></figure><figure data-orig-width="577" data-orig-height="693" class="tmblr-full"><img src="https://67.media.tumblr.com/4eee30390fd6c3bab01ab9af24c08272/tumblr_inline_ogglyxvDGe1s1hdxy_540.png" data-orig-width="577" data-orig-height="693"></figure><p>Turnout was down -- and down more among Democrats. But there wasn’t a huge swing.</p><p>Now the changes for the two parties. First, colored according to the partisan preference of the county itself:</p><figure data-orig-width="577" data-orig-height="693" class="tmblr-full"><img src="https://65.media.tumblr.com/8d17e0a8b8e23f033771992add163940/tumblr_inline_oggm0whr3h1s1hdxy_540.png" data-orig-width="577" data-orig-height="693"></figure><p>Republican counties tended to increase Republican turnout and decrease Democratic turnout. Democratic counties were split, but overall went down more than up.</p><p>Now, coloured by the partisan preference of the state as a whole -- which is relevant, because the Electoral College makes states matter a lot.</p><figure data-orig-width="577" data-orig-height="693" class="tmblr-full"><img src="https://66.media.tumblr.com/510d55e5cc200e13430dd31b31d136f9/tumblr_inline_oggm6i8FhK1s1hdxy_540.png" data-orig-width="577" data-orig-height="693"></figure><p>Technical details: I’m cheating a little by ignoring population changes (which will look like turnout changes), but it’s only been five years. The 2016 data are from <a href="https://github.com/mkearney/presidential_election_county_results_2016/blob/master/README.md">here</a>, via Ian Lyttle and Julia Silge on Twitter.&nbsp;The 2012 data are from <a href="https://fusiontables.google.com/DataSource?docid=1qcQLqrAIAe3RcEfdWSm_QcXMLmteVg4uSpSs1rM#rows:id=1">here</a>, posted by the Guardian data blog people. &nbsp;For reasons I don’t fully understand, there are whole bunches of records with different county numbers but the same FIPS code that the 2012 data puts in different records. In the one case I looked at in detail, they were towns in New London County, CT.&nbsp;I added them up, which seems to be what the 2016 data has. The graphs use the `<a href="https://cran.r-project.org/web/packages/hextri/vignettes/hexbin-classes.html">hextri</a>` R package. &nbsp;As an indication of what sorts of other processing was needed, the 2012 data file has 4639 records and 203 variables; the 2016 file has 18351 records and 8 variables. &nbsp;There were two 2012 counties that I couldn’t find in 2016 and one that appeared.&nbsp;</p>]]></content:encoded>
    <wp:post_name>153024320551</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/151969377696</link>
    <pubDate>Tue, 18 Oct 2016 20:06:36 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/151969377696</guid>
    <!--<wp:post_id>151969377696</wp:post_id>-->
    <wp:post_date>2016-10-18 0:06:36</wp:post_date>
    <wp:post_date_gmt>2016-10-18 07:06:36</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Cuts to ‘Growing Up in New Zealand’</title>
    <description></description>
    <content:encoded><![CDATA[<p>The NZ cohort study&nbsp;‘Growing Up in New Zealand’ is being cut from 7000 children to 2000, <a href="http://www.stuff.co.nz/science/85464263/Government-cuts-5000-children-from-Growing-up-in-NZ-longitudinal-study?utm_source=dlvr.it&amp;utm_medium=twitter">according to a story on Stuff</a> today. &nbsp;That’s unfortunate -- birth cohort studies are something New Zealand has done well in the past, and this is a cohort for the modern New Zealand.&nbsp;</p><p>Obviously, the top priority for the study will have been to fight the cuts or at least try to moderate them. The news story is presumably part of this effort, so we can still hope.&nbsp;</p><p>If maintaining the study turns out not to be allowed, the next step is to work out the most informative way to continue it.&nbsp;</p><p>First of all, there has never existed any other childhood cohort study with a substantial Māori sample. Retaining the majority of these children should be a high priority.</p><p>Second, even if <b>complete</b> data are collected only for 2000 children it should be possible to collect <b>some</b> information from the remaining kids. Every bit helps.</p><p>From a statistical viewpoint there’s then a tension between simplicity and informativeness. &nbsp;Taking a simple random sample of the participants is a simple design choice leading to simple and transparent analyses in the future, but it would be more informative to over-sample children at higher risk of adverse social and heath outcomes. &nbsp;Personally, I’d favour a carefully stratified subsample to make the most use of the information collected so far, but complicated subsampling from existing studies is one of my research areas, so I’m biased.</p>]]></content:encoded>
    <wp:post_name>151969377696</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/151740630311</link>
    <pubDate>Thu, 13 Oct 2016 19:47:38 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/151740630311</guid>
    <!--<wp:post_id>151740630311</wp:post_id>-->
    <wp:post_date>2016-10-12 23:47:38</wp:post_date>
    <wp:post_date_gmt>2016-10-13 06:47:38</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Terms to eschew</title>
    <description></description>
    <content:encoded><![CDATA[<blockquote><p>“I have discovered something else,” I continued. “By flipping the pages at random, and putting my finger in and reading the sentences on that page, I can show you what’s the matter – how it’s not science, but memorizing, in <i>every</i> circumstance. Therefore I am brave enough to flip through the pages now, in front of this audience, to put my finger in, to read, and to show you.”</p><p>Richard Feynman, at a public lecture in Brazil</p></blockquote><p>Twitter polls are, admittedly, almost useless as a data-gathering technique because of selection bias. However, if I ask my Twitter follower about definitions of basic statistical terms, the selection bias should work in favour of correct answers. I asked about terms for skewness and kurtosis.</p><p>Consider a unimodal distribution with a short tail on the left and a long tail on the right, such as a Poisson or Gamma distribution. It’s unambiguous that these distributions are positively skewed, but a lot of intro stats courses don’t use this terminology: they describe distributions as&nbsp;‘right’ or&nbsp;‘left’ skewed. Are the Poisson and Gamma distributions&nbsp;‘left’ skewed because the bulk of the distribution is shifted left relative to symmetry, or&nbsp;‘right’ skewed because the long tail is on the right? You probably know this one. And there’s a reasonable chance you’re wrong. About 1/3 of respondents said&nbsp;‘left’ and 2/3 said&nbsp;‘right’.&nbsp;</p><p>Consider a $t$ distribution or the logistic distribution. &nbsp;It’s unambiguous that these distributions are&nbsp;‘heavy-tailed’, but a lot of courses use the terms&nbsp;‘platykurtic’ and&nbsp;‘leptokurtic’. &nbsp;The former is from the Greek&nbsp;πλατύς, wide; the latter from the Greek λεπτός, light or thin. &nbsp;I asked which was which. Again, you probably know this one, and again there’s a reasonable chance you’re wrong. &nbsp;About 1/3 of respondents said&nbsp;‘leptokurtic’ and 2/3 said&nbsp;‘platykurtic’.</p><p>The correct answers are&nbsp;‘right skewed’ and&nbsp;‘leptokurtic’, but what the poll illustrates is that these are bad terms. You can’t derive them from first principles, all you can do is remember how their creators derived them. That’s a convenient property for setting easy-to-mark exams, but less so when teaching people to think about data.&nbsp;</p>]]></content:encoded>
    <wp:post_name>151740630311</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/151013860496</link>
    <pubDate>Wed, 28 Sep 2016 05:40:26 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/151013860496</guid>
    <!--<wp:post_id>151013860496</wp:post_id>-->
    <wp:post_date>2016-09-27 9:40:26</wp:post_date>
    <wp:post_date_gmt>2016-09-27 16:40:26</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Large quadratic forms</title>
    <description></description>
    <content:encoded><![CDATA[<p><i>Attention Conservation Notice: there probably aren’t as many as half a dozen groups in the world who actually have this much genome sequencing data. Everyone else could wait to see if something better comes up.&nbsp;</i></p><p>If you follow me on Twitter, you will have seen various comments about eigenvalues, matrices, and other linear algebra over the past months. Here, finally, is the Sekrit Eigenvalue Project.&nbsp;</p><p>A quadratic form in Normally distributed variables is of the form $z^TAz$ where $z$ is a vector of $n$ standard Normals and $A$ is an $n\times n$ matrix. Obviously the $z$s might not be independent or unit variance; almost equally obviously you can fix that by changing $A$. If $A$ is (a multiple of) a projection matrix of rank $p$, the distribution is (a &nbsp;multiple of) $\chi^2_p$, but not otherwise.</p><p>These quadratic forms come up naturally in two settings in statistics. The first is misspecified likelihood ratio tests. If you do a second-order Taylor series expansion of the difference in log likelihoods you find that the linear term vanishes because you’re at the maximum, and the quadratic term annoyingly fails to collapse into the $\chi^2_p$ of correctly-specified nested models. &nbsp;The other setting is when you take a set of asymptotically Normal test statistics and profane the <a href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem">Gauss-Markov theorem</a> by deliberately combining them with something other than their true inverse variance matrix as weights.&nbsp;</p><p>Two popular use cases for the latter setting are the Rao-Scott tests in survey statistics and the SKAT (sequence kernel association test) in genomics. In the Rao-Scott test the individual statistics are observed minus expected counts for cells in a table; in the SKAT test they are covariances between individual rare variants and phenotype. &nbsp;In both settings the true variance matrix of the individual statistics tends to be poorly estimated, so you do better by using something simpler: the Rao--Scott test uses the matrix that would be the variance under iid sampling, the SKAT test uses a set of weights that ignore correlation and upweight less-common variants.&nbsp;</p><p>The null distribution of the resulting test is a linear combination of $\chi^2_1$ variables, where the coefficients in the linear combination are the eigenvalues of $A$. That’s not in the tables in the back of the book, but there are several <a href="http://www.jstor.org/stable/2347778?seq=1#page_scan_tab_contents">perfectly</a> <a href="http://www.jstor.org/stable/2346911">good</a> <a href="http://www.jstor.org/stable/2673596">ways</a> of working it out given the eigenvalues.</p><p>In some genomic problems, though, the matrix $A$ is big, with $n$ easily being as much as several thousands now, and much larger in the future. Computing the eigenvalues takes time. Even computing the matrix $A$ is not trivial -- the time complexity of both tasks scales as the cube of sample size (or number of genetic variants). &nbsp;That is, the time complexity for computing the sampling distribution goes up faster than the data size, even though the test statistic takes time proportional to the data size.</p><p>My Cunning Plan™ is to take just the largest $k$ eigenvalues (say 100 or so), and to use the cheap and dirty Satterthwaite approximation to handle the rest. The Satterthwaite approximation uses a multiple of a single $\chi^2$ distribution with degrees of freedom chosen to get the mean and variance right for the null distribution. It works better than it deserves for $p$-values in the traditional $10^{-2}$ range, but the wheels fall off before you get to $10^{-6}$.&nbsp;</p><p>That is, instead of<br>$$\sum_{i=1}^n\lambda_i\chi^2_1$$<br>I’m using<br>$$\sum_{i=1}^k\lambda_i\chi^2_1+a_k\chi^2_{\nu_k}$$<br>where <br>$$a_k=\left(\sum_{j=k+1}^n\lambda_i^2\right)/\left(\sum_{j=k+1}^n\lambda_i\right)$$<br>and<br>$$\nu_k=\left(\sum_{j=k+1}^n\lambda_i\right)^2/\left(\sum_{j=k+1}^n\lambda_i^2\right)$$</p><p>&nbsp;I hoped it would still be good enough to mop up the residual eigenvalues, and it is. The combination works really surprisingly well, as far into the tail as I’ve tested it. A <a href="https://projecteuclid.org/euclid.aoap/1177005712">bit more research</a> shows there’s a reason: &nbsp;the extreme right tail of a convolution of &nbsp;densities with basically exponential tails behaves like the component with the heaviest tail. Since multiples of $\chi^2_1$ variables have basically exponential tails, and the heaviest tail corresponds to the largest multiplier, the leading eigenvalue approximation should do well in the extreme right tail -- where we care most for genomics.&nbsp;</p><p>Here’s an example with simulated sequence data from Gary Chen’s <a href="https://github.com/gchen98/macs">Markov Coalescent Simulator</a>: 5000 people and 4120 SNPs, so $n=4120$ and there are 4120 eigenvalues. I’m looking at the quantile where the Satterthwaite approximation on its own gives a $p$-value of $10^{-6}$; that’s the value at 0 eigenvalues used. The dashed line shows the result from the full eigendecomposition.</p><figure data-orig-width="336" data-orig-height="299" class="tmblr-full"><img src="https://66.media.tumblr.com/eb3b166353ae8979bcfc4919aaeb2c94/tumblr_inline_o535hp28rM1s1hdxy_540.png" alt="image" data-orig-width="336" data-orig-height="299"></figure><p>I still need the hundred or so largest eigenvalues. There are at least two ways to get these with much less effort than a full eigendecomposition: <a href="https://en.wikipedia.org/wiki/Lanczos_algorithm">Lanczos</a>-type algorithms and<a href="http://arxiv.org/abs/0909.4061"> stochastic Singular Value Decomposition</a>. &nbsp;They both work, about equally fast and about equally accurately, to reduce the time complexity from $n^3$ to $n^2k$ for $k$ largest eigenvalues. </p><p>This <b>isn’t</b> just a low-rank approximation: the $a_k\chi^2_{\nu_k}$ term is important. If we took a low-rank approximation to $A$ with no remainder term, we would need to take $k$ large enough that the sum of first $k$ eigenvalues was close to the sum of all the eigenvalues. The leading-eigenvalue approximation is much better. Here’s an example comparing them, where I cheat (but also make things more comparable) by working out the full eigendecomposition first. Using just the first $k$ eigenvalues in a low-rank approximation to $A$ we need $k&gt;2000$, but using the first $k$ eigenvalues plus a remainder term we only need about $k=50$. &nbsp;A low-rank approximation needs $k\approx 350$ to do even as well as the leading-eigenvalue approximation with $k=0$! &nbsp;</p><p>In fact, you can do pretty well even using a remainder term that gets only the mean right: replace the remainder term by $b_k\chi^2_{n-k}$ with $$b_k=\frac{1}{n-k}\sum_{j=k+1}^n\lambda_i.$$</p><p>In this graph the red line shows what happens (for a single example) using no remainder term, the blue line uses the remainder term with the right mean, and the black line uses the proposed remainder term with the right mean and variance.</p><figure data-orig-width="427" data-orig-height="167" class="tmblr-full"><img src="https://65.media.tumblr.com/72b4ca6ed02774db1a27b65728cdafed/tumblr_inline_o5rebiY7Uv1s1hdxy_540.png" alt="image" data-orig-width="427" data-orig-height="167"></figure><p>What’s more, in the genomic setting, $A$ is a weighted covariance for a $m\times n$ <b>sparse</b> genotype matrix. While $A$ isn’t sparse and doesn’t have a sparse square root, it can be written as the product of a sparse matrix and a projection matrix of low rank, so that multiplication by $A$ can be done <b>fast</b>. A ‘matrix-free’ version of the algorithm, which uses $A$ only via matrix-vector multiplications, has a time complexity of something like $O(\alpha mnk+np^2k)$ where $\alpha$ is the fraction of non-zero elements in the matrix of genotypes (typically only a few percent) and $p$ is the number of adjustment variables in the model. &nbsp;</p><p>For 5000 (simulated) individuals and 20,000 variants the sparse version of the new approximation is 3.5 orders of magnitude faster than using a full eigendecomposition. &nbsp;More importantly, taking advantage of sparse genotypes makes it possible to look at really large classes of &nbsp;potentially-functional variants that wouldn’t otherwise be feasible -- classes defined by epigenetic markers or by structural features of the chromosome.</p><p><a href="https://github.com/tslumley/bigQF">Software here.</a>&nbsp; ASHG abstract <a href="https://ep70.eventpilot.us/web/page.php?page=IntHtml&amp;project=ASHG16&amp;id=160122763">here</a>.</p>]]></content:encoded>
    <wp:post_name>151013860496</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/150767812506</link>
    <pubDate>Thu, 22 Sep 2016 22:23:54 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/150767812506</guid>
    <!--<wp:post_id>150767812506</wp:post_id>-->
    <wp:post_date>2016-09-22 3:23:54</wp:post_date>
    <wp:post_date_gmt>2016-09-22 10:23:54</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>The hard problem of AI and other stories</title>
    <description></description>
    <content:encoded><![CDATA[<p>Another occasional SF/F post.</p><p><a href="https://www.amazon.com/Melissa-Scott/e/B000AQ3XVW">Amazon now has a lot of older Melissa Scott novels in Kindle format.</a> &nbsp;In the old days, Melissa Scott was known for forthrightly LBGTQ fiction. After a decade or two, there’s been enough social progress for that to not be the most obvious thing about her writing. &nbsp;</p><p>The ‘hard problem of consciousness’ is a term of art in philosophy of mind. It’s either the most important question about intelligence, or a purely linguistic distraction from the real issues. What I call&nbsp;‘the hard problem of AI’ is something different: when does a program count as people -- and how does that compare to the status of a racial underclass? Melissa Scott’s&nbsp;“Dreaming Metal” is about this problem, as is the earlier&nbsp;“Dreamships.”</p><p>“The Kindly Ones” is about honour and honour killings and a traditional society in danger of being wiped out&nbsp;.The first few times I read it, I didn’t realise that the gender of one of the main viewpoint characters is never specified.&nbsp;<br></p><p>“Burning Bright” is about love, and political intrigue, and role-playing games in a culture where they’re an important spectator sport.&nbsp;<br></p><p>“Trouble and her Friends” is a queer cyberpunk political mystery. As with cyberpunk in general, the positive aspects are a bit dated. Sadly, many of the negative aspects are still right on target.&nbsp;<br></p><p>“Point of Hopes” is a young-adult police procedural romance set in a fantasy version of the Dutch Golden Age. It has the best&nbsp;‘separate but equal’ gender roles I’ve seen: for astrological reasons, local government and shopkeepers tend to be women,and military and long-distance traders tend to be men, but with exceptions in both cases. &nbsp;There’s a set of sequels -- they’re a bit too similar in plot if you read them all, but any two would be good.&nbsp;</p>]]></content:encoded>
    <wp:post_name>150767812506</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/150103235781</link>
    <pubDate>Thu, 08 Sep 2016 14:43:40 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/150103235781</guid>
    <!--<wp:post_id>150103235781</wp:post_id>-->
    <wp:post_date>2016-09-07 19:43:40</wp:post_date>
    <wp:post_date_gmt>2016-09-08 02:43:40</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Come work with us</title>
    <description></description>
    <content:encoded><![CDATA[<p>The Statistics department at the University of Auckland is advertising for a Professional Teaching Fellow, following the retirement of existing staff. This is a full-time, permanent, academic staff position in a department that understands how much its success depends on high-quality teaching.</p><figure class="tmblr-full" data-orig-height="683" data-orig-width="1024"><img src="https://65.media.tumblr.com/096a9ed682d1f6a0d98fd51f5f38d8ff/tumblr_inline_od5zc7im991s1hdxy_540.png" data-orig-height="683" data-orig-width="1024"></figure><p><i><a href="http://www.flickr.com/photos/craigsyd/12241996833/">Aerial view of Auckland</a> by Flickr user Craig, annotated to show Stats Dept</i></p><p>The formal ad is on <a href="https://www.seek.co.nz/job/31711551">Seek</a>.&nbsp;</p><blockquote><p>The department is seeking to appoint a highly organised, energetic and collegial person for the role of Professional Teaching Fellow. The successful candidate will join the team responsible for the organisation and delivery of first year courses in statistics. Duties may include teaching other courses in the department.&nbsp;</p><p>The department has a reputation for and commitment to teaching excellence. You will be encouraged to develop content and materials, to investigate novel strategies and technologies to enhance student learning and engagement, and to support diverse styles of teaching and learning and delivery of high quality course materials.</p><p>You will have a postgraduate degree in statistics or a closely related subject. You will also have superb teaching skills and experience of using statistical techniques, either in research or consulting with a significant component of statistics.</p></blockquote><figure class="tmblr-full" data-orig-height="856" data-orig-width="1280"><img src="https://67.media.tumblr.com/bb12bf4d0422bc14a747e338e2060fd8/tumblr_inline_od5zioqc0l1s1hdxy_540.png" data-orig-height="856" data-orig-width="1280"></figure>]]></content:encoded>
    <wp:post_name>150103235781</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/150049122281</link>
    <pubDate>Wed, 07 Sep 2016 10:55:42 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/150049122281</guid>
    <!--<wp:post_id>150049122281</wp:post_id>-->
    <wp:post_date>2016-09-06 15:55:42</wp:post_date>
    <wp:post_date_gmt>2016-09-06 22:55:42</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>On permuting all the things</title>
    <description></description>
    <content:encoded><![CDATA[<p>I wanted to list all the numbers whose digits were some permutation of 2,2,5,5,9,9, and find how many of them were multiples of 11, and how many were prime. (Because of Evelyn Lamb’s comment on the prime number&nbsp;295259 produced by the <a href="https://twitter.com/_primes_">prime numbers twitter bot</a>)</p><p>It takes some thought to work out how to list those numbers exactly once (because of the duplicated digits) but no thought at all to work out how to generate a random sample of them and discard duplicates.</p><p>So:</p><pre>nn&lt;-unique(replicate(10000,
+ 	{
+ 	as.integer(paste0(sample(c(2,2,5,5,9,9)),collapse=""))
+ 	}
+ ))
</pre><p>Now, there can’t be more than, say, 200 such numbers, so the expected number of copies of each one is at least 50, and the probability of missing a number is at most $200\times e^{-50}$, which is less than $200\times 2^{-50}$, which is tiny.</p><p>The code takes a small fraction of a second to run, and less time to check for divisibility by 11. &nbsp;Finding the primes takes a bit longer, because I have to realise that my list of small primes (used in the survey package to construct Hadamard matrices) doesn’t include 2. But it’s still pretty fast.</p><p>There are 90 distinct numbers. 40% of them are divisible by 11, and 11 of them are prime.</p><p>Random generation and discarding duplicates is a useful technique for smallish discrete problems.&nbsp;</p>]]></content:encoded>
    <wp:post_name>150049122281</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/149923845091</link>
    <pubDate>Sun, 04 Sep 2016 21:21:07 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/149923845091</guid>
    <!--<wp:post_id>149923845091</wp:post_id>-->
    <wp:post_date>2016-09-04 2:21:07</wp:post_date>
    <wp:post_date_gmt>2016-09-04 09:21:07</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>The lithium-powered space bike</title>
    <description></description>
    <content:encoded><![CDATA[<p>Q: So, it’s been about 11 months since you got your fancy electric-assist bike</p><p>A: Yes, that’s right</p><p>Q: Have you given up yet?</p><p>A: No, it’s still fun.</p><p>Q: Even with the rain?</p><p>A: Combining Doppler radar and the detailed weather forecasts has mostly kept me dry</p><p>Q: And getting killed by cars?</p><p>A: So far, still at less than 1 event.&nbsp;</p><p>Q: How do you feel about busy two-lane roundabouts?</p><p>A: I have a Theory.&nbsp;</p><p>Q: Do tell!</p><p>A: Busy roundabouts rely on informal negotiation over the details of the&nbsp;‘give way to the right’ rule. Since cyclists are excluded from the negotiation, we inevitably offend either the drivers to the right or those behind us.</p><p>Q: Can’t they just four-letter-word themselves?</p><p>A: They are driving deadly weapons.</p><p>Q: Ah. Yes.</p><p>A: It’s not so bad most of the time. Sometimes, though...</p><p>Q: Ok. What else</p><p>A: You know those orchids that are designed to attract insects..</p><p>Q: To trap them for pollination? Yes.</p><p>A: Some intersections have things that look like safe green bike boxes, but on lanes with induction loops</p><p>Q: So you’re trapped there until a&nbsp;‘real’ vehicle comes along?</p><p>A: That’s right. There’s one on the Grafton bridge!</p><p>Q: But that’s bus-only on weekdays!</p><p>A: I think cyclists are supposed to sit there until &nbsp;the weekend.&nbsp;</p><p>Q; You should come up with a cute name for that sort of thing</p><p>A;&nbsp;“trollbooths”?</p><p>Q: Very droll</p><p>A:Seriously, if electric bikes were cheaper, they’d be really useful for younger people.</p><p>Q: But isn’t the speed dangerous?</p><p><span style="font-size: 14px;">A: There are plenty of roads in my neighbourhood where I can go faster than that just from gravity.</span></p><p>Q: So, what’s the advantage?</p><p>A: Better acceleration makes it easier to deal with traffic. You wobble less going up steep hills, and you’ve got more attention to give to traffic.</p>]]></content:encoded>
    <wp:post_name>149923845091</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/149591245506</link>
    <pubDate>Sun, 28 Aug 2016 18:54:35 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/149591245506</guid>
    <!--<wp:post_id>149591245506</wp:post_id>-->
    <wp:post_date>2016-08-27 23:54:35</wp:post_date>
    <wp:post_date_gmt>2016-08-28 06:54:35</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>“The” multiple comparisons problem</title>
    <description></description>
    <content:encoded><![CDATA[<p>Andrew Gelman posted recently with the title&nbsp;“<a href="http://andrewgelman.com/2016/08/22/bayesian-inference-completely-solves-the-multiple-comparisons-problem/" title="Permanent Link to Bayesian inference completely solves the multiple comparisons problem">Bayesian inference completely solves the multiple comparisons problem</a>”. Bayesians have been making a claim that sounds like this for many years, so it would be easy to misunderstand and think he was making a much weaker claim than he actually is.&nbsp;</p><p>There are at least two multiple comparisons problems, andI’d like to suggest some terminology:</p><ul><li>The first-person multiple comparisons problem: I have data relevant to a collection of parameters $\{\theta_i\}_{i=1}^N$ and I want to make sure I arrive at sensible beliefs or take sensible decisions even if $N$ is quite large<br></li><li>The second-person multiple comparisons problem: You want me to publish my findings in such a way that <b>you</b>&nbsp;arrive at sensible beliefs or take sensible decisions even if $N$ is quite large</li></ul><p>The first-person problem is, fairly uncontroversially, solved automatically by Bayesian inference. &nbsp;Frequentists aren’t bad at it either.</p><p>The second-person problem isn’t <b>automatically</b> solved by Bayesian inference, I’ve <a href="http://notstatschat.tumblr.com/post/78570650254/my-likelihood-depends-on-your-frequency-properties">written about this</a>&nbsp;(and as a commenter points out, Xiao-Li Meng has written related things). Andrew Gelman has said as much, in his&nbsp;‘Garden of Forking Paths’ metaphor for researcher degrees-of-freedom.&nbsp;</p><p>The new claim is for a particular (important) case of the problem: Prof Gelman shows that a fairly strong but reasonable prior will ensure that badly-underpowered studies almost never draw false positive conclusions at 95% posterior probability. In that sense, if everyone switched to Bayesian inference there would be a lot fewer bogus newspaper stories. &nbsp;There would be a higher rate of false negatives, but in the fields he cares most about there’s a huge asymmetry between underclaiming and overclaiming, so it doesn’t matter.&nbsp;</p>]]></content:encoded>
    <wp:post_name>149591245506</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/149213130846</link>
    <pubDate>Sat, 20 Aug 2016 19:44:27 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/149213130846</guid>
    <!--<wp:post_id>149213130846</wp:post_id>-->
    <wp:post_date>2016-08-20 0:44:27</wp:post_date>
    <wp:post_date_gmt>2016-08-20 07:44:27</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Like a crossword</title>
    <description></description>
    <content:encoded><![CDATA[<p>The philosopher of science Susan Haack has a lovely analogy for the interconnectedness of scientific ideas: the crossword puzzle. &nbsp;We’re talking something along the lines of the New York Times crossword, not a British-style cryptic: the clues for each entry are often insufficient taken one at a time, but a false answer is likely to be revealed by its failure to fit with crossing answers.</p><p>Chris McDowall recently reminded me of the <a href="https://en.wikipedia.org/wiki/Phantom_time_hypothesis">Phantom Time Hypothesis</a>, my favourite engagingly batshit historical theory. &nbsp;The&nbsp;‘hypothesis’ says the years 614CE to 911CE didn’t exist: the calendar was adjusted from the early 7th century CE to the year 1000, by a conspiracy involving the Holy Roman Emperor, the Pope, and possibly the Byzantine Emperor. &nbsp;As Wikipedia dryly notes&nbsp;“The proposal has found no favour among mainstream medievalists.”</p><p>The Phantom Time Hypothesis rises above many conspiracy theories by leading to testable (falsifiable) predictions. &nbsp;An interesting exercise for a science class would be to come up with a list. Here are some suggestions:</p><ul><li>Halley’s comet, solar eclipses, and other recurring astronomical events: times of these would be out by 297 years</li><li>The prophet Muḥammad was born about forty years before the missing centuries, and events such as the Hijra and the conquest of Arabia occurred early in the phantom time. It’s one thing to say that a relatively boring part of European history didn’t happen; it takes ... um, chutzpah is probably the wrong word... something more to delete nearly the first three centuries of Islam.</li><li>Tree rings: wooden objects created in the sixth and tenth centuries CE would have incompatible tree ring histories, as would world-wide cooling events due to large volcanic eruptions.</li><li>Chinese history: The Tang dynasty and the An Lushan rebellion (the second-deadliest war in history) are going to take some effort to paper over -- and they interacted with the Arab Caliphates in Central Asia.</li><li>Radiocarbon dating&nbsp;</li><li>Paper: the Middle East developed paper making (probably from the Chinese) during the period</li><li>Scientific development in the Arabic world:&nbsp;Muhammad ibn Musa al-Khwarizmi, after whom the algorithm is named, was just one of the scientists and scholars of the early Islamic Golden Age</li><li>The slippage between the Gregorian and Julian calendars, which were set to agree at the time of the Council of Nicea (325CE). Under the hypothesis, they would have agreed in the first century CE instead.&nbsp;</li></ul>]]></content:encoded>
    <wp:post_name>149213130846</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/149016120136</link>
    <pubDate>Tue, 16 Aug 2016 16:41:58 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/149016120136</guid>
    <!--<wp:post_id>149016120136</wp:post_id>-->
    <wp:post_date>2016-08-15 21:41:58</wp:post_date>
    <wp:post_date_gmt>2016-08-16 04:41:58</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Hot hand sampling</title>
    <description></description>
    <content:encoded><![CDATA[<p>NY magazine<a href="http://nymag.com/scienceofus/2016/08/how-researchers-discovered-the-basketball-hot-hand.html"> 'Science of Us' column</a> has a story about the 'hot hand' in basketball -- the theory that players sometimes have a run of good shooting and then a run of bad shooting. &nbsp;Andrew Gelman <a href="http://andrewgelman.com/2012/03/16/hot-hand-debate-is-warming-up/">has</a> <a href="http://andrewgelman.com/2014/03/11/myth-myth-myth-hot-hand/">written</a> <a href="http://andrewgelman.com/2014/08/12/understanding-hot-hand-myth-hot-hand-time/">extensively</a> <a href="http://andrewgelman.com/2015/09/30/hot-hand-explanation-again/">on</a> this <a href="http://andrewgelman.com/2015/10/18/explaining-to-gilovich-about-the-hot-hand/">issue</a>, but the story covers something fairly subtle and people were arguing about it on Twitter</p><p>So, basically, the early research showed there was very little deviation from independence, and the&nbsp;‘hot hand’, if not completely a myth, was a much weaker effect than people had imagined.</p><p>The new idea is that you should expect to see negative correlation in finite samples. Which sounds weird, but is actually true and is less weird than the story tries to say. I had an idea about what might explain this, then decided I was wrong and the issue was one of the subtle problems of <a href="https://en.wikipedia.org/wiki/Penney%27s_game">overlapping sequences</a>, but actually when I looked up the <a href="http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2627354">paper</a> I was wrong again and the first idea was more or less right.</p><p>Suppose we have tossed four coins. &nbsp;What’s the expected proportion of heads (over this entire sequence) that are followed by another head? (Note that I’m <b>not</b> asking for the probability that a head will be followed by another head, because for stylised probability coins this is always 50%.) &nbsp;The proportion tends to be less than 50% basically because we’ve taken out one of the heads as the start of the sequence, so on average less than half of the <b>remaining</b> tosses will have been heads.&nbsp;</p><p>Some sequence of results will have no H. The proportion is not defined for these sequences, so we consider only sequences with $h&gt;0$ heads. &nbsp;Pick a sequence, and then pick one of the heads. What is the probability it was followed by another head? Well, there are $h-1$ heads to spread over the $4-1=3$ other positions, so by simple random sampling without replacement, it should be $(h-1)/3$. &nbsp;(We have to be a little careful when $h=1$, since the only head might be last, but the proportion is zero when $h=1$ anyway). &nbsp;For $h$ heads and $n$ tosses, the proportion will be $(h-1)/(n-1)$. And since the expected value of $h$ is about $n/2$ you’d expect&nbsp;$(h-1)/(n-1)&lt;h/n$ in some average sense.&nbsp;</p><p>This intuition also explains why the phenomenon goes away with larger $n$: the proportion of heads followed by heads will be $1/2+O_p(n^{-1})$. &nbsp;Also, in order to see this in data, the number of sequences $M$ has to be much larger than the sequence length $n$, since the sampling error is $O(M^{-1/2})$. But it could be a thing.&nbsp;</p><p>And it turns out that it is a thing. The combinatorics is messy, and gets much worse if you want longer sequences than two, but the empirical autocorrelation of a Bernoulli sequence does indeed have a negative bias of about $1/n$. Here’s a histogram with $n=20$, $M=10^5$:</p><figure class="tmblr-full" data-orig-height="504" data-orig-width="504"><img src="https://67.media.tumblr.com/8c4b9b6ab0a709686ad41d2cfeeed066/tumblr_inline_obzjmvuJlM1s1hdxy_540.png" data-orig-height="504" data-orig-width="504"></figure>]]></content:encoded>
    <wp:post_name>149016120136</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/148957352886</link>
    <pubDate>Mon, 15 Aug 2016 13:05:04 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/148957352886</guid>
    <!--<wp:post_id>148957352886</wp:post_id>-->
    <wp:post_date>2016-08-14 18:05:04</wp:post_date>
    <wp:post_date_gmt>2016-08-15 01:05:04</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Simulations and modes of convergence</title>
    <description></description>
    <content:encoded><![CDATA[<p>We often have theory that says&nbsp;$$\sqrt{n}(\hat\theta_n-\theta)\stackrel{d}{\to}N(0,\sigma^2),$$<br>and then do simulations to see how well the asymptotic approximation applies. After doing so, we often present tables of the empirical mean and standard deviation of $\hat\theta_n.$ This doesn’t make a lot of sense.</p><p>Knowing that&nbsp;$\sqrt{n}(\hat\theta_n-\theta)\stackrel{d}{\to}N(0,\sigma^2)$ doesn’t tell us anything about the moments of $\hat\theta_n$ for any finite $n$. Convergence in distribution does not imply convergence in mean. For example, &nbsp;$\hat\theta_n$ could be maximum likelihood estimates in a logistic regression model. These &nbsp;have no finite moments for any finite $n$, because they are infinite with positive probability.&nbsp;</p><p>However,&nbsp;the asymptotic result does tell us that the quantiles of $\hat\theta_n$, suitably scaled, should converge to those of the approximating Normal distribution. The median of $\hat\theta_n$ should converge to $\theta$; the MAD of $\hat\theta_n$ (after scaling by $\sqrt{n}$) should converge to $\sigma$, and the probability that $(\hat\theta_n-1.96\times\widehat{se}[\hat\theta_n],\,\hat\theta_n+1.96\times\widehat{se}[\hat\theta_n])$ includes $\theta$ should converge to 95%.</p><p>Knowing how good the asymptotic approximation is for these quantile-based statistics is usually sufficient to tell us if the approximation is useful in practice. We usually don’t care about the <b>mean </b>of $\hat\theta_n$&nbsp;in any substantive way, since if we did, we’d be more worried when it didn’t have one.</p><p>I think the usefulness of&nbsp;‘robust statistics’ gets oversold a lot, but this really is a case where it’s meaningful and true to say that the median of an approximately-Normal variable is a better summary of the location parameter than the mean is. &nbsp;We should be presenting robust (ie, weakly continuous) summaries of the results of simulations motivated by asymptotics unless there’s a specific reason to care about moments.</p>]]></content:encoded>
    <wp:post_name>148957352886</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/148387479840</link>
    <pubDate>Wed, 03 Aug 2016 18:30:03 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[video]]></category>
		<category domain="category" nicename="video"><![CDATA[video]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/148387479840</guid>
    <!--<wp:post_id>148387479840</wp:post_id>-->
    <wp:post_date>2016-08-02 23:30:03</wp:post_date>
    <wp:post_date_gmt>2016-08-03 06:30:03</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title></title>
    <description></description>
    <content:encoded><![CDATA[
              
<video  id='embed-5a74dddf496c6847634743' class='crt-video crt-skin-default' width='400' height='400' poster='http://78.media.tumblr.com/tumblr_obbm1tqpiK1sueztx_smart1.jpg' preload='none' muted data-crt-video data-crt-options='{"autoheight":null,"duration":75,"hdUrl":false,"filmstrip":{"url":"http:\/\/66.media.tumblr.com\/previews\/tumblr_obbm1tqpiK1sueztx_filmstrip.jpg","width":"200","height":"200"}}' >
    <source src="https://notstatschat.tumblr.com/video_file/t:1muNFDtrOyKmI8WNQ8qzjQ/148387479840/tumblr_obbm1tqpiK1sueztx" type="video/mp4">
</video>
              <p>Uncertainty in a spatial field -- a set of images from a Gibbs sampler. The data and model are from <a href="https://www.jstatsoft.org/article/view/v055i13">Lee (2013</a>) &nbsp;‘CARBayes: An R Package for Bayesian Spatial<br>Modeling with Conditional Autoregressive Priors’, showing standardised incidence rates for respiratory disease hospitalisations in the Glasgow area. &nbsp;The image takes 300 samples from the MCMC output and interpolates five images between each one in a straight line (on the log scale).</p><p>Video created with the `animation’ package.&nbsp;</p>      ]]></content:encoded>
    <wp:post_name>148387479840</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/148340217981</link>
    <pubDate>Tue, 02 Aug 2016 20:21:08 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/148340217981</guid>
    <!--<wp:post_id>148340217981</wp:post_id>-->
    <wp:post_date>2016-08-02 1:21:08</wp:post_date>
    <wp:post_date_gmt>2016-08-02 08:21:08</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Etymology</title>
    <description></description>
    <content:encoded><![CDATA[<p><b>Penguin</b>: the name is supposed to come from the Welsh <b>pen gwyn</b>, meaning&nbsp;‘white head’. Since penguins have <b>black</b> heads, and do not live within 10,000 km of Wales, it is difficult to see how this theory arose.</p>]]></content:encoded>
    <wp:post_name>148340217981</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/148129715451</link>
    <pubDate>Fri, 29 Jul 2016 14:55:27 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/148129715451</guid>
    <!--<wp:post_id>148129715451</wp:post_id>-->
    <wp:post_date>2016-07-28 19:55:27</wp:post_date>
    <wp:post_date_gmt>2016-07-29 02:55:27</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>A modest proposal: Lazy Ambiguous Single Transferable Vote</title>
    <description></description>
    <content:encoded><![CDATA[<p>We’re about to have another outbreak of voting here in NZ as well. The local government elections use STV, and Graeme Edgeler <a href="http://publicaddress.net/legalbeagle/stv-qa/">explains it here</a>. In particular, he explains how indicating preferences for all the candidates, even ones you don’t want to win, is desirable.</p><p>Because Twitter is Twitter, &nbsp;a discussion of this came up with Rob Salmond’s proposal that you should be able to vote 1,2, 3, &lt;meh&gt;, 35,36 for, say, the District Health Board elections where there are a few good candidates who are worth voting for, a couple of antifluoride or antivax extremists who need to be voted against, and a lot of boring and irrelevant candidates. &nbsp;Or, Michael Calhoun suggested 1,2,3, blank, -2, -1 with negative numbers indexing from last.</p><p>Can this actually be done? What would it look like? Could a system allow a voter to indicate negative preferences without even a first preference?&nbsp;</p><p>As a service to democracy I present <b>LAST Vote</b>: Lazy Ambiguous Single Transferable Vote.&nbsp;</p><ul><li>Voters may rank candidates 1,2,3,.. as far as they want,with 1 being the first preference<br></li><li>Voters may rank candidates -1,-2,-3 as far as they want, with -1 being the last preference</li><li>Candidates with no ranking are less preferred than those with positive rankings but more preferred than those with negative rankings.&nbsp;</li></ul><p><b>The goal for the counting system is that your vote is equivalent to a set of fractional votes with all the possible permutations of the unranked candidates. &nbsp;</b></p><p>With computers this is directly feasible. &nbsp;When your voting paper has exhausted its set of positive rankings, your vote is distributed evenly across all the unranked candidates still being considered. When none of your unranked candidates are still being considered, your paper goes to the least-negative of your negative rankings.&nbsp;</p><p>You wouldn’t want to do it by hand. &nbsp;The number of possible sets of unranked candidates is large -- for more than 21 candidates there are more possible sets than eligible NZ voters. Can we get a simpler approximation?</p><p>If there were more statisticians in the world, a useful approximation might be to pick a single ordering at random, independently for each voting paper, perhaps using the <a href="http://notstatschat.tumblr.com/post/129692767316/nz-flag-referendum-pseudorandom-numbers">Official Electoral Pseudorandom Number Generator</a>, and rely on the Law of Large Numbers. The approximation also scales up smoothly to full accuracy -- you can generate two orders at random, or ten, for each voting paper. &nbsp;But generating random numbers by hand at that scale isn’t feasible.</p><p>With a single random ordering, the random numbers could be printed on the voting paper -- a set of numbers or letters that gives the default ordering for candidates you don’t rank. Postal ballots already have unique information on them, so this is possible in principle. Arguably it also complies with the intent of the voter better, since the voter can see where their 4th preference would go and change it if necessary.</p><p>It’s tempting to think, as I originally did, that you can collapse together all the voting papers that have exhausted their positive rankings and not got to their negative rankings, but that doesn’t preserve the stated aims. I think a better crude approximation would be to allocate the papers that have exhausted their positive preferences to candidates at random, and then go through and re-allocate at random any that were given to a candidate who had a negative preference. This doesn’t give the same results even in some average sense, but it would be feasible to do by hand, in parallel across voting booths.</p>]]></content:encoded>
    <wp:post_name>148129715451</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/148121123596</link>
    <pubDate>Fri, 29 Jul 2016 11:20:47 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/148121123596</guid>
    <!--<wp:post_id>148121123596</wp:post_id>-->
    <wp:post_date>2016-07-28 16:20:47</wp:post_date>
    <wp:post_date_gmt>2016-07-28 23:20:47</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>One scoRe years</title>
    <description></description>
    <content:encoded><![CDATA[<p>It’s always nice when even imperfect metrics make you look good.&nbsp;<br></p><p>The <a href="http://spectrum.ieee.org/static/interactive-the-top-programming-languages-2016">new programming-language rankings</a> in IEEE Spectrum are out. I don’t think I believe their weighting system, but it has R in 5th place! Since last year, we’ve just edged out C#.</p><p>Bob Muenchen l<a href="http://r4stats.com/2016/06/08/r-passes-sas-in-scholarly-use-finally/">ooks at statistical software citations</a> on Google Scholar, and finds that R is narrowly in front of SAS -- and though SPSS is well ahead, it’s headed down. I’m not sure how much I believe this metric either, since everything except SAS and SPSS is heading up, even Minitab. The trends in software popularity are probably getting confounded with trends in citation behaviour and changes in what Google can index.</p><p>In three weeks time or less I will have been an R user for twenty years: my <a href="http://tolstoy.newcastle.edu.au/R/testers/0229.html">first message to the r-alpha mailing list</a> was on August 19, 1996, just before version 0.10 came out.</p><p>I didn’t expect R to start being used by molecular biologists, ecologists, and linguists. And I think I can speak for everyone at the time in saying we didn’t foresee R being used at Google. &nbsp;</p>]]></content:encoded>
    <wp:post_name>148121123596</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/146886495511</link>
    <pubDate>Mon, 04 Jul 2016 19:27:47 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/146886495511</guid>
    <!--<wp:post_id>146886495511</wp:post_id>-->
    <wp:post_date>2016-07-04 0:27:47</wp:post_date>
    <wp:post_date_gmt>2016-07-04 07:27:47</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>How do we prove the Central Limit Theorem?</title>
    <description></description>
    <content:encoded><![CDATA[<p>More precisely, in a course in mathematical statistics that’s trying not to assume more mathematics than necessary, how do we prove it? &nbsp;</p><p>A (Weak) Law of Large Numbers is easy: Markov’s inequality, then Chebyshev’s Inequality, not needing anything more than the simplest manipulation of expectations. &nbsp;The CLT is hard.</p><p>The standard approach is to use characteristic functions: prove Levy’s Continuity Theorem, work out what the characteristic function of an iid sum looks like, and then work out the characteristic function of a Normal. &nbsp;Clean and simple, but it requires dragging in complex numbers. &nbsp;If you’re a time-series person you might think every statistician needs to understand Fourier transforms, and you might even have a point, but it seems like overkill for the CLT.</p><p>David Pollard (”A User’s Guide to Measure-Theoretic Probability”) uses the Lindeberg trick. If you have a sequence $X_n$ that you want to show the CLT for, start with a sequence $Z_n$ of Normal random variables that have the same means and variances as $X_n$. &nbsp;The CLT for $Z_n$ is trivial, and the Lindeberg trick lets you swap $X_n$ for $Z_n$ one term at a time.</p><p>If what you want is a rigorous, elementary proof, that’s the way to go. The only thing I don’t like about it is shared with the Fourier approach: it doesn’t seem to show you what’s actually happening.&nbsp;</p><p>If you want to motivate the CLT, you show the distributions of sums of increasing numbers of random variables. &nbsp;Each convolution rubs some of the individuality off the distribution, and asymptotically there’s nothing left but some generic shape. &nbsp;That doesn’t happen at all with the Lindeberg trick, and you can’t really see it happening with the Fourier approach.&nbsp;</p><p>Two approaches that do seem to show what’s going on are based on entropy and on moments:</p><p>The Normal distribution has the highest entropy of any continuous distribution with given mean and variance; a precise characterisation of its boringness. Since convolutions increase entropy (and enough convolutions increase entropy even after rescaling to constant variance), &nbsp;the distribution of a sum has to keep getting closer to a Normal distribution. &nbsp;There’s a <a href="http://www.stat.yale.edu/~arb4/publications_files/EntropyAndTheCentralLimitTheoremAnnalsProbability.pdf">proof by Andrew Barron</a>, for example.</p><p>Another tempting approach is to prove that the moments of your centered and scaled sum converge to the moments of a Normal distribution. This, again, fits what is going on nicely, and for the first few moments is straightforward. The problem is that you need to know the moments of a Normal distribution uniquely determine the distribution. While that’s true, I haven’t been able to find an elementary proof of it. You could probably cobble something together with a truncation argument and the Weierstrass approximation theorem, but it might be messy.&nbsp;</p>]]></content:encoded>
    <wp:post_name>146886495511</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/145438762776</link>
    <pubDate>Sun, 05 Jun 2016 17:12:15 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/145438762776</guid>
    <!--<wp:post_id>145438762776</wp:post_id>-->
    <wp:post_date>2016-06-04 22:12:15</wp:post_date>
    <wp:post_date_gmt>2016-06-05 05:12:15</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Computing the (simplest) sandwich estimator incrementally</title>
    <description></description>
    <content:encoded><![CDATA[<p>The biglm package in R does {incremental,online,streaming} linear regression for data potentially larger than memory. This isn’t rocket science: accumulating $X^TX$ and $X^TY$ is trivial; the package just goes one step better than this by using Alan Miller’s incremental $QR$ decomposition code to reduce rounding error in ill-conditioned problems.&nbsp;</p><p>The code also computes the Huber/White heteroscedasticity-consistent variance estimator (sandwich estimator). Someone wants a reference for this. There isn’t one, because it’s too minor to publish, and I didn’t have a blog ten years ago. &nbsp;But I do now. So:</p><p>The Huber/White variance estimator $A^{-1}BA^{-1}$, where $A^{-1}=(X^TX)^{-1}$ and $B=\left(X^T(Y-\hat\mu)\right)^{\otimes 2}$</p><p>The $(i,j)$ element of $B$ is <br>$$\sum_{k=1}^N x_{ki}(y_{k}-x_{k}\hat\beta)x_{kj}(y_{k}-x_{k}\hat\beta)$$</p><p>Multiplying this out, we get<br>$$\sum_{k=1}^N x_{ki}x_{kj}y_k^2$$<br>and about $2p$ terms that look like<br>$$\sum_{k=1}^N&nbsp;x_{ki}x_{kj}x_{k\ell}y_k\hat\beta_{\ell}$$<br>and about $p^2$ terms that look like<br>$$\sum_{k=1}^N x_{ki}x_{kj}x_{k\ell}x_{km}\hat\beta_{\ell}\hat\beta_m$$<br></p><p>We can move the $\beta$s outside the sums, so the second sort of terms look like</p><p><br></p><p>$$\hat\beta_{\ell}\left(\sum_{k=1}^N x_{ki}x_{kj}x_{k\ell}y_k\right)$$</p><p><br></p><p>and the third sort look like</p><p><br></p><p>$$\hat\beta_{\ell}\left(\sum_{k=1}^N x_{ki}x_{kj}x_{k\ell}x_{km}\right)\hat\beta_m$$</p><p>Now if we define $Z$ to have columns $x_ix_j$ and $x_iy$ (for all $i,\,j$), the matrix $Z^TZ$ contains all the $x$ and $y$ pieces needed for $B$. &nbsp;The obvious thing to do is just to accumulate $Z^TZ$ in R code, one chunk at a time.</p><p>If you were too convinced of your own cleverness you might realise that $(X,Z)$ could be fed into the $QR$ decomposition as if it were $X$, and that you’d get $Z^TZ$ For Free! Where&nbsp;‘for free’ means at $O((p^2)^3)$ extra computing time plus the mental anguish of reconstructing $Z^TZ$ from the $QR$ decomposition. &nbsp;It’s not a big deal, since the computation is dominated by the $O(np)$ cost of reading the data, but it does look kinda stupid in retrospect.</p><p>I suppose that means I’ve learned something in ten years.</p>]]></content:encoded>
    <wp:post_name>145438762776</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/145386255856</link>
    <pubDate>Sat, 04 Jun 2016 15:49:08 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/145386255856</guid>
    <!--<wp:post_id>145386255856</wp:post_id>-->
    <wp:post_date>2016-06-03 20:49:08</wp:post_date>
    <wp:post_date_gmt>2016-06-04 03:49:08</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Are there any news?</title>
    <description></description>
    <content:encoded><![CDATA[<p>I’ve <a href="http://notstatschat.tumblr.com/post/76138681207/this-is-a-wug-now-you-have-two-of-them">written before</a> about &nbsp;treating ‘data’ as a plural count noun or a mass noun. In most settings I’m happy with either:&nbsp;‘this data’ or&nbsp;‘these data’;&nbsp;‘data is’ or&nbsp;‘data are’.&nbsp;</p><p>There are settings where the count version doesn’t feel right to me. I might write “We don’t have much data on that issue”, but never&nbsp;“We don’t have many data on that issue”. Perhaps the most extreme is the opposite of&nbsp;‘more data’:&nbsp;‘fewer data’ just seems wrong. So, I did research!</p><p>(1) Google n-grams</p><figure class="tmblr-full" data-orig-height="391" data-orig-width="992"><img src="https://65.media.tumblr.com/303eacfb5c7986af0c04aef7b3b60525/tumblr_inline_o889xvrCei1s1hdxy_540.png" data-orig-height="391" data-orig-width="992"></figure><p>(2) Google trends</p><figure class="tmblr-full" data-orig-height="471" data-orig-width="883"><img src="https://66.media.tumblr.com/af944216c2b919c0c5c4909c256e6719/tumblr_inline_o88aj1MiBH1s1hdxy_540.png" data-orig-height="471" data-orig-width="883"></figure><p>(3) A Twitter poll</p><figure class="tmblr-full" data-orig-height="210" data-orig-width="582"><img src="https://67.media.tumblr.com/c2f2e6172d853e4f88cef400901c35c0/tumblr_inline_o88a2n9sHw1s1hdxy_540.png" data-orig-height="210" data-orig-width="582"></figure><p>So, people who agree with me are ahead, but by less than I would have guessed.&nbsp;</p><p>However, the Twitter poll also produced a number of comments implying that people were picking&nbsp;‘fewer’ on theoretical grounds rather than what they actually think sounds right. I suspect some of the&nbsp;‘fewer’s are hypercorrections.&nbsp;</p><p>Incidentally, both options would be clearly wrong if&nbsp;‘data’ were being treated as singular, further support for the position that no-one actually does this.</p>]]></content:encoded>
    <wp:post_name>145386255856</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/142811881871</link>
    <pubDate>Fri, 15 Apr 2016 09:50:51 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/142811881871</guid>
    <!--<wp:post_id>142811881871</wp:post_id>-->
    <wp:post_date>2016-04-14 14:50:51</wp:post_date>
    <wp:post_date_gmt>2016-04-14 21:50:51</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Size matters</title>
    <description></description>
    <content:encoded><![CDATA[<figure class="tmblr-full" data-orig-height="587" data-orig-width="1189"><img src="https://40.media.tumblr.com/9a4b5f1bcfc38773934c03a629627dd7/tumblr_inline_o5n7kp2Ou21s1hdxy_540.png" data-orig-height="587" data-orig-width="1189"></figure><p>There’s a lovely demonstration of simple neural networks at <a href="http://playground.tensorflow.org/">playground.tensorflow.org</a>, which I recommend to anyone interested in teaching or studying them. It shows the inputs, the hidden nodes, and the output classification, and how they change with training. &nbsp;You can add more neurons or more layers interactively, and fiddle with the training parameters. I wish something like this had been available in the early 90s when I was learning about neural networks.&nbsp;</p><p>On the other hand, what it shows is the sort of neural network I learned about in the early 1990s (yes, it has regularisation, but that’s in the mysterious options at the top, not in the fun-to-play-with toys). Back then, &nbsp;neural networks were interesting but not terribly impressive as classifiers. We’d already stopped thinking they really worked like biological neurons, and as black-box semiparametric binary regression models they were only ok. &nbsp;</p><figure data-orig-height="448" data-orig-width="267"><img src="https://40.media.tumblr.com/206ca0bc424a4a0504fb2c28ba1c0ed7/tumblr_inline_o5n8c5lQdD1s1hdxy_540.png" data-orig-height="448" data-orig-width="267"></figure><p>Modern neural nets suddenly started to give seriously impressive results for image classification just a few years ago, and that’s because size matters. The thousand-fold increase in computation power and training data over the past couple of decades is what neural networks needed. The tensorflow playground shows how the components of a neural network work, but I don’t think they don’t help much with understanding how image classification works in really big networks. After all, Marvin Minsky, one of the inventors of these&nbsp;‘perceptrons’, famously planned for substantial progress in computer vision as a project for the summer of 1966, as <a href="http://www.xkcd.com/1425/">the XKCD cartoon</a>&nbsp;notes.</p><p>We’ve had this sort of teaching problem in statistics for years. Textbooks were (are?) full of tiny data sets, small enough that computations can be done on a hand calculator or the data entered by hand into a computer. &nbsp;Students are told to work through all the computational steps; it’s supposed to give them a feeling for the data. What it gives them (apart from a revulsion for data) is a sense of the behaviour of very simple statistics on very small data sets, and increased difficulty in understanding the law of large numbers, the central limit theorem, the curse of dimensionality, and other important facts that don’t show up in a $20x3$ data table.</p>]]></content:encoded>
    <wp:post_name>142811881871</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/142563218201</link>
    <pubDate>Sun, 10 Apr 2016 21:40:25 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/142563218201</guid>
    <!--<wp:post_id>142563218201</wp:post_id>-->
    <wp:post_date>2016-04-10 2:40:25</wp:post_date>
    <wp:post_date_gmt>2016-04-10 09:40:25</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Sufficiently advanced technology</title>
    <description></description>
    <content:encoded><![CDATA[<p>As I said on Twitter, I found out this week (1) that there are cheap variable resistors sensitive to acetone (or ethanol) and (2) that many people, even scientists, don’t think this is amazing.&nbsp;</p><p>Multi-atom molecules are hugely bigger than electrons, but hugely smaller than the bulk semiconductor. They shouldn’t be able to affect resistance.</p><p>Other technologies for interfacing chemistry and electronics tend to be based on light absorption (breathalysers, blood oxygen detectors, some DNA sequences) or on electrons/protons released in chemical reactions (other breathalysers, other DNA sequencers) or in past days on formation of ions in solution. It’s reasonably clear how these translate between the molecular scale and the electron scale.&nbsp;</p><p>Looking on Google, it seems that the details of gas-sensitive resistors <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3292102/">aren’t completely understood</a> (or weren’t in 2009). &nbsp;The basic idea is that oxygen atoms attach to the surface of semiconductor grains and suck electrons. Reducing gases such as acetone displace some of the oxygen atoms, and don’t suck electrons, leaving more electrons free for conduction. Since semiconductors can be tuned to have just barely enough free electrons to conduct, the surface effect of the oxygen suckage can be enough to matter.</p><p>The application that brought this up was measuring breath ketones for someone on a low-carb diet, with a DIY device that cost an order of magnitude less than the commercial version. That’s clever, but I’m still more impressed that there’s a way to measure acetone concentrations with a simple resistor.&nbsp;</p>]]></content:encoded>
    <wp:post_name>142563218201</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/141887342331</link>
    <pubDate>Tue, 29 Mar 2016 20:27:14 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/141887342331</guid>
    <!--<wp:post_id>141887342331</wp:post_id>-->
    <wp:post_date>2016-03-29 0:27:14</wp:post_date>
    <wp:post_date_gmt>2016-03-29 07:27:14</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>The Great Kiwi Cherry Ripe Scandal</title>
    <description></description>
    <content:encoded><![CDATA[<p><i>In which I unnecessarily calculate a simple probability by maths when I’ve already done it by simulation.</i></p><p>You can just see it on a maths teaching blog as a Bad Example</p><blockquote><p>1. A company is making packs of eight chocolate bars chosen independently and with equal probability from five types:&nbsp;“Cherry Ripe","Dairy Milk","Crunchie", "Caramello", and "Flake".&nbsp;</p><p>(a) What is the probability that a pack will contain seven or more Cherry Ripes?</p><p>(b) What is the probability that a pack will contain seven or more of the same type of bar?</p></blockquote><p>New Zealand currently <a href="http://www.stuff.co.nz/business/78315308/cadbury-says-its-random-we-say-its-a-one-in-ten-thousand-lottery">has</a> a <a href="http://www.stuff.co.nz/national/78298099/cadbury-customers-not-happy-with-favourites-easter-egg">case</a> of Nature imitating Bad Art. &nbsp;There were two answers given in the media, which differed by two orders of magnitude. &nbsp;I used <a href="http://rpubs.com/tslumley/166331">simulation</a> to try to settle the problem reliably and fairly comprehensibly.</p><p>But there’s still the maths exam question angle. So.&nbsp;</p><p>(a) The number $N$ of Cherry Ripes in a pack is distributed as $Binomial(8,1/5)$, independently for each pack. &nbsp;We want $$P(N\geq 7)=P(N=7)+P(N=8)= (8\times 0.8^1\times 0.2^7)+0.2^8=33\times 5^{-8}$$<br>or, &nbsp;$P(N\geq 7)\approx 8.5\times 10^{-5}$</p><p>(b) Getting 7 or more of two types of bar in the same package is impossible, so the five events&nbsp;‘7 or more Cherry Ripe”,&nbsp;“7 or more Flake” etc are mutually exclusive. Their union is the event&nbsp;“7 or more of the same type”, so this has probability $5\times 33\times 5^{-8}= 165\times 5^{-8}\approx 4\times 10^{-4}$.&nbsp;</p>]]></content:encoded>
    <wp:post_name>141887342331</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/141830859731</link>
    <pubDate>Mon, 28 Mar 2016 22:46:12 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/141830859731</guid>
    <!--<wp:post_id>141830859731</wp:post_id>-->
    <wp:post_date>2016-03-28 2:46:12</wp:post_date>
    <wp:post_date_gmt>2016-03-28 09:46:12</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Mostly dead</title>
    <description></description>
    <content:encoded><![CDATA[<blockquote><p><a href="http://www.imdb.com/name/nm0001597/">Inigo Montoya</a>: He's dead. He can't talk. <br><a href="http://www.imdb.com/name/nm0000345/">Miracle Max</a>: Whoo-hoo-hoo, look who knows so much. It just so happens that your friend here is only MOSTLY dead. There's a big difference between mostly dead and all dead. Mostly dead is slightly alive.&nbsp;<br></p></blockquote><p>In the cardiovascular-research trade, there’s a minor but persistent issue of nomenclature. When your heart stops beating and you fall over dead, should that be called&nbsp;“Sudden Cardiac Death” or&nbsp;“Sudden Cardiac Arrest”? The case for the former is obvious; the case for the latter is basically that it sounds insufficiently serious when you then talk about the small fraction of ‘non-fatal sudden death.’&nbsp;</p><p>My colleagues in Seattle have one of the largest cohorts of people who have survived sudden cardiac death, largely because Seattle has one of the<a href="https://en.wikipedia.org/wiki/Seattle_%26_King_County_Emergency_Medical_Services_System#Cardiac_Arrest"> best survival rates f</a>or sudden cardiac death of anywhere in the world. &nbsp;It’s roughly 10% -- but in the subset of people who have&nbsp;‘Bystander Witnessed VF/VT Due to Heart Disease’ it’s about 50% who leave hospital alive, and it keeps getting better.&nbsp;</p><p>Here’s a slightly outdated comparison across cities (for 2008), from <a href="https://en.wikipedia.org/wiki/Seattle_%26_King_County_Emergency_Medical_Services_System">Wikipedia</a></p><figure class="tmblr-full" data-orig-height="1196" data-orig-width="2264"><img src="https://41.media.tumblr.com/bc2bc98f59b0475549a4817bb06acf6e/tumblr_inline_o4qrlwEnM11s1hdxy_540.png" data-orig-height="1196" data-orig-width="2264"></figure><p>In the wake of the <a href="http://www.stuff.co.nz/business/industries/78300012/defibrillators-removal-from-bunnings-store-stuns-heart-attack-widow">Bunnings</a> <a href="http://www.nzherald.co.nz/business/news/article.cfm?c_id=3&amp;objectid=11612423">defibrillator</a> controversy, it’s worth looking at how Seattle/King County does this. There are two components: very widespread training in CPR (50% of the population have had a CPR course) and an ambulance service funded to provide rapid support (including defibrillation) from paramedics trained in the latest techniques.&nbsp;</p><p>Both components are important: CPR is rarely enough to revive you, but it can keep you only mostly dead while you wait for the ambulance. Defibrillation has a good chance of rebooting your heart as long as it’s in one of the&nbsp;‘shockable rhythms’ -- the heart muscle cells are contracting properly, but not in the coordinated fashion needed to pump blood. &nbsp;The chance of this goes down rapidly with time, but with CPR it only deteriorates about half as fast.&nbsp;</p><p>An automatic defibrillator won’t do as well as a paramedic with 2000 hours of training, but it won’t do any harm. It won’t shock someone with a proper heartbeat, and (less importantly) it won’t give a shock &nbsp;if the &nbsp;heartbeat is flatlined or otherwise not fixable electrically. You don’t want to rely on it, but it’s a good stopgap until the professionals arrive.&nbsp;</p><p>In a perfect world it’s probably more cost-effective to train more people in CPR and to shorten ambulance response times -- as Leonard Cobb and Mickey Eisenberg did in Seattle -- because cardiac arrests can happen anywhere and it’s hard to get good coverage of defibrillators. But if you’ve got a defibrillator available, refusing to allow it to be used seems, well, &nbsp;heartless.&nbsp;</p>]]></content:encoded>
    <wp:post_name>141830859731</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/141630353371</link>
    <pubDate>Fri, 25 Mar 2016 12:58:58 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/141630353371</guid>
    <!--<wp:post_id>141630353371</wp:post_id>-->
    <wp:post_date>2016-03-24 16:58:58</wp:post_date>
    <wp:post_date_gmt>2016-03-24 23:58:58</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Artistic verisimilitude</title>
    <description></description>
    <content:encoded><![CDATA[<figure data-orig-width="600" data-orig-height="522" class="tmblr-full"><img src="https://41.media.tumblr.com/af68e83839120b657d4ec2a4da42e1cb/tumblr_inline_o4khhg7XsK1s1hdxy_540.png" data-orig-width="600" data-orig-height="522"></figure><p>Scene: the back room of a gas station not far from a Research 1 university, late twentieth century.&nbsp;</p><p>“Chris? You got a minute”<br></p><p>It’s Chris’s ‘lunch’ break, which by natural justice and state law should be the time for daydreaming about attractive members of the appropriate sex while trying to start an assignment on Banach spaces.&nbsp;</p><p>“The boss read an in-flight magazine again”&nbsp;<br></p><p>Chris sighs. Having the owner away for a week was good, but he always seemed to get...ideas</p><p>“He says we need visual analytics to optimize our product mix. Yeah, yeah, I know, we’re a gas station. We sell regular and premium. We don’t even got diesel. You’re studying analytics, right?”</p><p>“Not analytics, analysis. It’s ... no, ok... what do you need?”<br></p><p>“A graph. Number of gallons, Total cost. Then use analysis to work out the price”<br></p><p>“But we know the price! It’s on the sign, in large friendly letters. You just made me change it.”<br></p><p>“Yeah, yeah, whatever. He wants a graph, we give him a graph. He’s paying a bonus for it.”<br></p><p>“That’s different. Ok. You got graph paper”<br></p><p>“Graph paper?”<br></p><p>“You know, blue ink, little squares?”<br></p><p>“Oh, that stuff. My kids have some. Maybe I could get Sandy to do it?”<br></p><p>Chris saw the bonus slipping away.</p><p>“We got anything with squares?”<br></p><p>“Well, there’s the grid that Pat uses for D&amp;D layouts, Would that..”<br></p><p>“Perfect. Just give me a few minutes. He’ll have the best-fitting line graph ever seen in the industry”</p><p>“You want help with measuring?”</p><p>&nbsp;“I’m not wasting time measuring. Like I said, we know the price.”<br></p>]]></content:encoded>
    <wp:post_name>141630353371</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/141386721761</link>
    <pubDate>Mon, 21 Mar 2016 09:06:08 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/141386721761</guid>
    <!--<wp:post_id>141386721761</wp:post_id>-->
    <wp:post_date>2016-03-20 13:06:08</wp:post_date>
    <wp:post_date_gmt>2016-03-20 20:06:08</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>The conservative Bonferroni correction</title>
    <description></description>
    <content:encoded><![CDATA[<p>It seems to be a surprise to most people (certainly to me) how sharp the Bonferroni correction is when the number of tests is large. Unless the correlation between tests is really, high, the actual family-wise Type I error rate is very close to the nominal rate $\alpha/k$.</p><p>Part of the issue is confusing prior distributions on effect sizes (which can be quite strongly correlated) with null sampling distributions (which tend to be weakly correlated in the extreme tails). But part of it is just that the correction looks like it should be too strong.&nbsp;</p><p>I was reminded of this by Hilda Bastian’s tweet saying that the use of the Bonferroni Inequality for confidence intervals was due to Olive Jean Dunn, and reading her <a href="https://en.wikipedia.org/wiki/Olive_Jean_Dunn">Wikipedia entry</a>. Dunn says:</p><blockquote><p>“In working on the various confidence intervals for $k$<i>&nbsp;</i>means, I thought of the Bonferroni inequality ones quite early, but since they were so simple I thought they couldn't possibly be of any use. I spent a long time trying to prove that the confidence intervals which would be used in the case of independent variables could also be used or dependent variables. After failing to find a general proof for this, I finally noticed that the simple Bonferroni intervals were nearly as short".</p></blockquote><p>The Bonferroni correction is conservative because controlling family-wise Type I error is a conservative goal, but (usually) not because it comes from an inequality. But you’re in good company if that surprises you.</p>]]></content:encoded>
    <wp:post_name>141386721761</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/141133813166</link>
    <pubDate>Wed, 16 Mar 2016 19:29:28 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/141133813166</guid>
    <!--<wp:post_id>141133813166</wp:post_id>-->
    <wp:post_date>2016-03-15 23:29:28</wp:post_date>
    <wp:post_date_gmt>2016-03-16 06:29:28</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Trace estimators and impact factors</title>
    <description></description>
    <content:encoded><![CDATA[<p>For a Secret Project™, I needed a quick estimator of the trace of a matrix. To be precise, I have a rectangular matrix $A$ and I needed $\mathop{tr}(B)$ and $\mathop{tr}(B^2)$ where $B=A^TA$. That sounds easy, but $A$ is big enough that you don’t want to compute $A^TA$.&nbsp;</p><p>The first one actually is easy: $$\mathop{tr}(B)=\sum_{ij}(A_{ij})^2.$$ The second one is harder. &nbsp;I tried a sampling approach: estimating a sample of the entries of $B$ and using $$\mathop{tr}(B^2)=\sum_{ij} (B_{ij})^2.$$ &nbsp;The performance was not great, even when I tried stratified sampling -- getting all the diagonal entries of $B$ and a random sample of the off-diagonal entries.</p><p>I should have Googled before thinking, rather than after thinking. There is a standard solution: Hutchinson’s randomised trace estimator. If $z$ is a vector whose entries are independent, mean zero, unit variance, then $$E[z^TBz]=\mathop{tr}(B).$$&nbsp;<br>So, if you have $k$ random vectors $z_1,\dots,z_k$, a trace estimator is&nbsp;<br>$$\widehat{\mathop{tr}}(B^2)=\frac{1}{k} \sum_i z_i^TB^2z_i=\frac{1}{k} \sum_i z_iA^TAA^TAz_i=\frac{1}{k} \sum_i \left\|A^TAz_i\right\|_2^2$$<br></p><p>Hutchinson used Rademacher (random $\pm 1$) variables for the entries of $z$. You could also use standard Normals and it wouldn’t make much difference -- it would affect what theoretical tools you could use to get tail bounds. &nbsp;There’s a slight benefit in having the $z_i$ be uncorrelated, so in one application where it was convenient I used vectors $Az$ produced by the subsampled randomised Hadamard transform, but when $A$ is sparse and that isn’t convenient I just used standard Normals. You don’t get high accuracy -- the central limit theorem says the error is of order $k^{-1/2}$ -- but you get a reasonable estimator quickly.</p><p>The Hutchinson trace estimator has also been influential in thinking about degrees of freedom for semiparametric smoothers, as Grace Wahba mentions in her <a href="http://arxiv.org/abs/1303.5153">chapter</a> of “Past, Present, Future of Statistical Science”.</p><p>Since I knew $\mathop{tr}(B)$ I could improve on the Hutchinson estimator by ratio estimation from survey sampling. If you estimate $\mathop{tr}(B)$ using the same $z$, a better estimator of $\mathop{tr}(B^2)$ is<br>$$\widehat{\mathop{tr}}_{\mathrm{ratio}}(B^2)=&nbsp;\widehat{\mathop{tr}}(B^2) \frac{\mathop{tr}(B)}{ \hat{\mathop{tr}}(B)}$$<br>The sampling errors in the two estimators are correlated, so the error in the ratio is smaller -- by about 50\% in my case.&nbsp;</p><p>So, impact factors? Hutchinson’s <a href="http://www.tandfonline.com/doi/abs/10.1080/03610919008812866">paper</a>&nbsp;“A stochastic estimator of the trace of the influence matrix for laplacian smoothing splines” is in the journal&nbsp;‘Communications in Statistics - Simulation and Computation’, well known for not being especially picky or well read. It still has quite a few gems, such as Hutchinson’s paper and Pepe &amp; Anderson’s <a href="http://www.tandfonline.com/doi/abs/10.1080/03610919408813210#.Vuj8wBiriko">paper</a> about conditioning on $X$ at all times or just the current time in longitudinal data analysis. &nbsp;“This paper is crap because it’s in a crap journal” has the same form as the classical argumentum ad hominem. It saves effort, but it’s still a fallacy.<br></p>]]></content:encoded>
    <wp:post_name>141133813166</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/140955047906</link>
    <pubDate>Sun, 13 Mar 2016 19:31:16 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/140955047906</guid>
    <!--<wp:post_id>140955047906</wp:post_id>-->
    <wp:post_date>2016-03-12 22:31:16</wp:post_date>
    <wp:post_date_gmt>2016-03-13 06:31:16</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>A gene for celibacy?</title>
    <description></description>
    <content:encoded><![CDATA[<figure data-orig-width="598" data-orig-height="115" class="tmblr-full"><img src="https://36.media.tumblr.com/b56eec54495702a0d0f47682187f556d/tumblr_inline_o3yrbgV8qs1s1hdxy_540.png" alt="image" data-orig-width="598" data-orig-height="115"></figure><p>Tyson is, unusually for him, completely wrong here.&nbsp;</p><p>I’ll ignore the use of&nbsp;“gene” to mean&nbsp;“allele”, since that’s a plain-English abuse of notation as harmless as calling Pluto a planet. Put more precisely, he’s saying&nbsp;“if you have a genetic variant (allele) that substantially increases your tendency to celibacy, you didn’t inherit it”&nbsp;</p><p>The first problem is a statistical one. It would be surprising if you inherited a&nbsp;‘celibacy gene’, but it would also be surprising if you got it by de novo mutation. &nbsp;A typical person has about 100 point mutations not inherited from their parents. Most of these will have no effect, and it’s very unlikely for any of them to have such a subtle effect that you’d be normal enough to follow Neil deGrasse Tyson on Twitter but would have a substantially reduced interest in sex. &nbsp;But if we’re stipulating that you have a&nbsp;‘celibacy gene’, we’re conditioning on one of the two having happened.&nbsp;</p><p>From the point of view of reproductive fitness, a ‘celibacy gene’ is pretty much like a&nbsp;‘gay gene’. As Jeremy Yoder (an evolutionary biologist and gay) <a href="http://denimandtweed.jbyoder.org/2011/06/the-intelligent-homosexuals-guide-to-natural-selection-and-evolution-with-a-key-to-many-complicating-factors/">writes</a>, gay men average about 80% fewer children than straight men, and there is evidence of a genetic component and no shortage of potential explanations. Nothing inconceivable here, move on.&nbsp;</p><p>But there are more dramatic examples than `celibacy genes’. There is a recessive ‘gene for not having children’ that is carried by about 1 in 25 people of northern European ancestry, and it’s one where the molecular mechanism is well understood and until recently the effect was almost 100%. People with two copies of the deletion in amino acid 508 of the CFTR gene are less likely to have children now (especially the men) and were much less likely to do so in past generations</p><p>Loss of function in CFTR gives you cystic fibrosis. Before modern medicine people with cystic fibrosis died as infants. Until the development of intracytoplasmic sperm injection, men with cystic fibrosis were almost always infertile. But if you have cystic fibrosis, you almost certainly inherited it.&nbsp;</p>]]></content:encoded>
    <wp:post_name>140955047906</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/140322145631</link>
    <pubDate>Wed, 02 Mar 2016 21:14:15 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/140322145631</guid>
    <!--<wp:post_id>140322145631</wp:post_id>-->
    <wp:post_date>2016-03-02 0:14:15</wp:post_date>
    <wp:post_date_gmt>2016-03-02 08:14:15</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Truthy and Sciency</title>
    <description></description>
    <content:encoded><![CDATA[<p>There was a story at the<a href="http://www.nzherald.co.nz/health-wellbeing/news/article.cfm?c_id=1501238&amp;objectid=11596733"> New Zealand Herald</a>, republished without attribution from <a href="https://theconversation.com/people-in-their-nineties-reveal-the-secrets-to-ageing-well-52237">TheConversation</a>, under the headline&nbsp;<b>“People in their 90s reveal secret to ageing well”</b>. &nbsp;</p><p>By and large, it’s pretty good example of what The Conversation is trying to do, but there are some strange bits, such as</p><blockquote><p>"Regular exercise changes our epigenome, activating genes that improve muscle function"&nbsp;</p></blockquote><p>and</p><blockquote><p>Few participants smoked, avoiding the known epigenetic effects of cigarette smoke including lung damage, increased risk of dementia and cancer.<br></p></blockquote><p>The first is basically true, but&nbsp;“Regular exercise improves muscle function” seems more natural and is much more strongly supported by research. For smoking it’s even worse: it’s well established that some of the carcinogenic effects of tobacco smoke are mutagenic rather than epigenetic.&nbsp;</p><p>In something that’s supposed to be about healthy ageing, phrasing everything in terms of epigenetics makes it sound more like a<a href="https://en.wikipedia.org/wiki/Troy_McClure"> Troy Maclure</a> video:</p><blockquote><p>You might remember me from posts such as “Genomics: salvation of millions”, “Interferon: Take that, viruses”, and “Humours: Are yours balanced?”</p></blockquote><p>Exercise, a good diet, and not smoking are excellent strategies for a long and healthy life, but almost none of the supporting evidence comes from epigenomics, even though that makes it sound more sciencey</p>]]></content:encoded>
    <wp:post_name>140322145631</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/140201371641</link>
    <pubDate>Mon, 29 Feb 2016 20:18:20 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/140201371641</guid>
    <!--<wp:post_id>140201371641</wp:post_id>-->
    <wp:post_date>2016-02-28 23:18:20</wp:post_date>
    <wp:post_date_gmt>2016-02-29 07:18:20</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Coding linear splines</title>
    <description></description>
    <content:encoded><![CDATA[<p><i>Attention conservation notice: anyone who would actually use this could just sit down and do the algebra almost as quickly.</i></p><p>The best-known splines are cubic: a cubic spline with knots $x_1,\;x_2,\dots,\;x_m$ is a piecewise-cubic polynomial $f(x)$ where $f$, $f’$, and $f’’$ are continuous at the knots. &nbsp;The name is from the engineer’s drafting tool, a flexible metal strip that -- in the infinitely-thin, uniformly flexible asymptote -- will form a curve held down at the knots and otherwise minimising bending energy $\int f’’(x)^2\,dx$ to give a cubic spline.</p><p>Polynomial splines of degree $k$ generalise the cubic spline: they are piecewise polynomial, with $f$ and $k-1$ derivatives continuous at the knots. They have the nice feature of being expressible as linear combinations of a set of basis functions, so that the best-fitting spline with a given set of knots can be computed by ordinary regression. The degenerate case with $k=1$ is the linear spline, a continuous piecewise-linear function[1].</p><p>Linear splines don’t fit as well as quadratic or cubic, and don’t look as pretty: they have visible corners at the knots. &nbsp;However, they fit a lot better than step functions, and the linear coefficients are easily interpreted.&nbsp;</p><p>There are two useful parametrisations for linear regression spline bases. In the first, the coefficients are the slopes in the segments $[-\infty,\,x_1]$, $[x_1,x_2]$ and so on. &nbsp;Alternatively, we can take the first coefficient to be the slope in $[-\infty, x_1]$, the second to be the difference in slope at $x_1$, the third to be the difference in slope at $x_2$, and so on.&nbsp;</p><p>For the first parametrisation, the basis is</p><blockquote><p>z1(t) = min(t, x1)<br>z2(t) = max(x1, min(t, x2))<br>z3(t) = max(x_2, min(t, x3))<br>and so on</p></blockquote><p>For the second, the basis is</p><blockquote><p>z1(t) = t<br>z2(t) = max(0, t-x1))<br>z3(t) = max(0, t-x2))<br>and so on</p></blockquote><p>Neither of these is the b-spline basis given by the R function bs(), but both are probably more useful in practice.&nbsp;</p><p><br></p><p>[1] One could make a case for step functions as 0-degree splines, where 0 of $f$, $f’$, $f’’$,$\dots$ are continuous. Mathematicians might agree or might think this was trolling.&nbsp;</p>]]></content:encoded>
    <wp:post_name>140201371641</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/140122962896</link>
    <pubDate>Sun, 28 Feb 2016 15:49:34 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/140122962896</guid>
    <!--<wp:post_id>140122962896</wp:post_id>-->
    <wp:post_date>2016-02-27 18:49:34</wp:post_date>
    <wp:post_date_gmt>2016-02-28 02:49:34</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Cheap tricks</title>
    <description></description>
    <content:encoded><![CDATA[<p>If you’re interested in thinking about evidence and belief and rhetoric, it’s convenient to have uncontroversial examples of&nbsp;‘cheap tricks’ that directly affect attitudes. Ian Gordon has provided one, with his<a href="https://soundcloud.com/laztozia/vaders-redemption-the-imperial-march-in-a-major-key"> translation of the ‘Imperial March’ from Star Wars into a major key.&nbsp;</a></p><p>The tune is recognisably still film music by John Williams in a military style, but it’s happy, and lively, and on the edge of self-parody: somewhere between ‘The Great Escape’ and&nbsp;‘Chicken Run’.</p><p>As an experiment in music, it’s pure fun. As an example of how easy it is to press certain mental buttons, it’s a bit scary.&nbsp;</p>]]></content:encoded>
    <wp:post_name>140122962896</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/140020946461</link>
    <pubDate>Fri, 26 Feb 2016 22:35:15 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/140020946461</guid>
    <!--<wp:post_id>140020946461</wp:post_id>-->
    <wp:post_date>2016-02-26 1:35:15</wp:post_date>
    <wp:post_date_gmt>2016-02-26 09:35:15</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Two cheers for crowdfunding</title>
    <description></description>
    <content:encoded><![CDATA[<p>A successful large crowdfunding effort in a medium-sized community is, ipso facto, widely popular. If roughly 40,000 people have donated to buy a beach and sandbar, they’re going to be proud of themselves and not want the effort criticised. And it’s hard to argue that the two million dollars spent on <a href="http://www.radionz.co.nz/news/national/297288/success-crowd-funding-campaign-buys-beach">Awaroa</a> beach is a worse use of money than, say, &nbsp;the twenty million spent in a typical week on the lottery. The government did end up kicking in $350,000, but the property must be worth nearly that much to the nation.&nbsp;</p><p>Crowdfunding is like the lottery in another way. As long as it’s mostly for fun, there’s no problem, but if we start relying on it we’re in big trouble. Public funding through government is, and should be, mostly boring; crowdfunding isn’t. Crowdfunding will work better for buying a popular camping beach than for the sort of inaccessible and unknown forest or swamp that’s actually of conservation value. It’s intrinsically impossible for more than a few crowdfunding campaigns to get the wide media attention that Awaroa beach it, and there’s no reason to expect the ones that do get this support to be the most valuable.&nbsp;</p><p>I do need to distinguish big-ticket public-good crowdfunding from two other types that are more valuable. One is old-fashioned fundraising through personal contacts -- asking people you know to ask people they know to support a cause that’s important to you. The&nbsp;‘Twitter Aunties’ <a href="https://givealittle.co.nz/cause/kapawhaea">givealittle page</a> for Te Whare Marama refuge is an example-- the internet makes it easier (and potentially more anonymous), but the idea isn’t new. &nbsp;The other is the Kickstarter model of finding interested buyers for products that don’t yet exist, in order to raise initial capital. Kickstarting is new, and it while it doesn’t always work, it’s a good option.&nbsp;</p><p>So, two cheers for crowdfunding. The internet has made it easier to connect willing donors and buyers with causes and products, and that’s entirely good. We can have a few lucky fundraising schemes that make it to the big time, and that’s mostly good. &nbsp;But crowdfunding decisions can rarely manage the tedious and careful accounting of alternatives that lie behind sensible public spending by governments or charitable NGOs.&nbsp;</p><p>Personally, I don’t even try to do any serious analysis when thinking about crowdfunding appeals, and I expect most people are the same -- and as long as the money isn’t being subtracted from other sources of public-good funding, that’s not a problem.</p>]]></content:encoded>
    <wp:post_name>140020946461</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/138845422266</link>
    <pubDate>Sun, 07 Feb 2016 21:30:59 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/138845422266</guid>
    <!--<wp:post_id>138845422266</wp:post_id>-->
    <wp:post_date>2016-02-07 0:30:59</wp:post_date>
    <wp:post_date_gmt>2016-02-07 08:30:59</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>No-one’s forcing you to read the Herald</title>
    <description></description>
    <content:encoded><![CDATA[<blockquote><p>“No-one’s forcing you to read the Herald”<br><i>(various sources on the internet)</i></p></blockquote><p>It’s true. No-one is forcing me to read the NZ&nbsp;Herald. In fact, I went forty years without reading it and no-one criticised at all. &nbsp;</p><p>No-one forced me to move to New Zealand, either. </p><p>You probably wouldn’t want the Herald to be your only source of news, but for someone living, working, and voting in Auckland, the Herald is the best available paper. In many ways it’s not as good as the New York Times or the Independent, the Grauniad or the Washington Post, but the New Zealand coverage in those papers really sucks. </p><p>The Herald has Harkanwal Singh’s data journalism. It has important reporting by David Fisher and Matt Nippert, Kirsty Johnston, Martin Johnston and Jamie Morton, and probably other people I’d know to mention if I read bylines more carefully. </p><p>It also has a tendency to re-print the worst British ‘science news’ clickbait, to ignore inflation and population size in comparisons, to believe what the Government says, and to alternate between celebration and pearl-clutching about Auckland housing prices. </p><p>I rarely complain online about the Seattle Times, the South China Morning Post, or L’Osservatore Romano, because, basically, I don’t care about them and they don’t care about me. The Herald, I do care about. It’s what we’ve got, it has good and bad days, and &nbsp;I’d like there to be more good days. &nbsp;<br></p>]]></content:encoded>
    <wp:post_name>138845422266</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/138778963791</link>
    <pubDate>Sat, 06 Feb 2016 20:39:25 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/138778963791</guid>
    <!--<wp:post_id>138778963791</wp:post_id>-->
    <wp:post_date>2016-02-05 23:39:25</wp:post_date>
    <wp:post_date_gmt>2016-02-06 07:39:25</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Stochastic SVD</title>
    <description></description>
    <content:encoded><![CDATA[<p>Suppose you have an $m\times n$ matrix $A$ of rank $k$. If $\Omega$ is an $n\times k$ matrix with iid standard Gaussian entries, then $\Omega$ will have rank $k$ with probability 1, &nbsp;$A\Omega$ will have rank $k$ with probability 1, and so $A\Omega$ spans the range of $A$. That’s all easy.</p><p>More impressively, if $A=\tilde A+\epsilon$ where $\tilde A$ has rank $k$ and $\epsilon$ has small norm, and if $\Omega$ has $k+p$ columns, $A\Omega$ spans the range of $\tilde A$ with high probability, for surprisingly small values of $p$. &nbsp;If $Q$ comes from a $QR$ decomposition of $A\Omega$, then $Q^TA$ has approximately the same $k$ largest singular values as $A$ (or, equivalently, as $\tilde A$).&nbsp;</p><p>The same trick works with lots of other random $\Omega$, for fixed $p$. If we are prepared to take $p=\log k$ it even works for a&nbsp;‘structured’ random $\Omega=SHR$ where $R$ applies random signs to each row, $H$ does the Hadamard Transform, and $S$ takes a random sample of $k\log k$ rows of a matrix. &nbsp;The point of this choice of $\Omega$ is that $\Omega A$ can be computed in $mn\log n$ time &nbsp;(with a small constant, and for any $k$) using the Fast Hadamard Transform, rather than the $mnk$ time of explicit matrix multiplication.&nbsp;<br></p><p>The reference you want is <a href="http://arxiv.org/abs/0909.4061">here</a>:&nbsp;“Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions” by Halko, Martinsson, and Tropp.&nbsp;</p><p>One way to think about what’s happening is a combination of&nbsp;<a href="http://notstatschat.tumblr.com/post/129047372011/high-dimensional-space-is-big">“Space is Big. Really Big.”</a>&nbsp;with a version of the Law of Large Numbers. The columns of $A$ are $n$ points in $m$-dimensional space, and if $m\gg\log n$ they are really sparse. Because they are really sparse, capturing one eigenvector of $A$ is pretty much independent of capturing another one. $\Omega$ doesn’t have any preferences for whether it captures eigenvectors with large or small eigenvalues, but $A\Omega$ magnifies the larger ones. As the paper notes, multiplying by $(AA^T)^q$ for some small $q$ improves things even further. &nbsp;If you only took $k$ dimensions you’d still have a good chance of missing some of the $k$ main eigenvectors, but using $k+p$ dimensions decreases that chance -- more or less exponentially in $p$ because independence. &lt;waves hands&gt; The actual proofs, of course, are more complicated and use some fairly deep facts.</p><p>The stochastic SVD is no faster on average than Lanczos-type algorithms, but it’s enormously easier to program correctly and comes with simple probabilistic error bounds that are known to be sharp. As with the Lanczos-type algorithms, it can be made much faster if $A$ is sparse or otherwise structured so that multiplication by $A$ is fast. &nbsp;The stochastic algorithm is also easier to parallelize and can be modified to scan $A$ only a few times, or even only once.&nbsp;<br></p>]]></content:encoded>
    <wp:post_name>138778963791</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/137738257766</link>
    <pubDate>Thu, 21 Jan 2016 20:41:22 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/137738257766</guid>
    <!--<wp:post_id>137738257766</wp:post_id>-->
    <wp:post_date>2016-01-20 23:41:22</wp:post_date>
    <wp:post_date_gmt>2016-01-21 07:41:22</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Is it that time of day?</title>
    <description></description>
    <content:encoded><![CDATA[<p><a href="http://mindingdata.com/2016/01/21/analyzing-the-rock-playlist/">Wade at Minding Data wrote</a> about a local NZ radio station</p><blockquote><p>One of the main criticisms of The Rock, is that even if it doesn’t play the same song between 9 – 5, it still plays the same song everyday, often at the same time. To be fair to them, it’s probably no different to the criticism hurled at any popular radio station really. Anecdotal, I used to listen to the radio as I was getting up in the morning, and I used to swear that for weeks on end, I would be getting up to the same song.</p></blockquote><p>He scraped <a href="http://mindingdata.com/2016/01/21/analyzing-the-rock-playlist/">data</a> from their website and did some analysis, but didn’t end up answering the original question. So I thought I’d have a go. My code is <a href="https://gist.github.com/tslumley/040749c559bb71d1921b">here</a>.</p><p>The first step is to get the time of day for each song, which is just string processing. Next, I turned it into an angle, like the hour hand of a 24-hour clock. Midnight is straight up, the morning is on the right, noon is straight down, the afternoon is on the left.&nbsp;</p><p>Each song play is then represented by a point on the unit circle, or more usefully by a vector from the center to that point. You can add up plays of the same song to get a longer vector. If they tend to be at the same time of day, the vector will be long and point in the direction of that time of day; if they are spread evenly across the day, the vectors will cancel and the resulting total vector will be short. Actually I did this for groups, not individual songs, so the sample sizes would be more useful. The darkness of each point is proportional to the number of plays for songs from that group</p><figure data-orig-width="377" data-orig-height="365" class="tmblr-full"><img src="https://40.media.tumblr.com/4830e310e7528ccc218514683ac986c3/tumblr_inline_o1akx6s7iR1s1hdxy_540.png" alt="image" data-orig-width="377" data-orig-height="365"></figure><p>There looks to be a pattern. Unfortunately, the data aren’t complete and aren’t spread uniformly through 24 hours anyway. The big splurge of data to the mid-afternoon is because there are more songs there in the data set -- that’s why the colour scale matches the distance so neatly.</p><p>I can subtract off the mean vector from each individual song or group vector to remove the bias and recenter the distribution</p><figure data-orig-width="377" data-orig-height="365" class="tmblr-full"><img src="https://41.media.tumblr.com/24c7a7ef2756033b2ca2740a4e7e8642/tumblr_inline_o1al2bFz2M1s1hdxy_540.png" alt="image" data-orig-width="377" data-orig-height="365"></figure><p>This is less interesting, though there still look to be some patterns. &nbsp;At this point I did a set of per-group t-tests, and didn’t understand the results. The problem with this second graph is that groups with lots of plays will tend to have points that are further out -- the sum of $n$ mean-zero variables has standard deviation proportional to $\sqrt{n}$.&nbsp;</p><p>As the next step, I scaled by $1/\sqrt{n}$ for each group</p><figure data-orig-width="377" data-orig-height="365" class="tmblr-full"><img src="https://40.media.tumblr.com/11f6dfc54d85db9c4d681d36b320a16e/tumblr_inline_o1al7yoB4a1s1hdxy_540.png" alt="image" data-orig-width="377" data-orig-height="365"></figure><p>There’s a lot less going on. &nbsp;The signal for INXS may be real: they were played just three times, but at 5:12am, 5:27am, and 7:22am (on different days). Otherwise, the phrase&nbsp;“robustly null” springs to mind.</p><p>If I had to invent a moral for this exercise to justify it being a blog post, it might be the importance of subtracting off the uninteresting signals, even in drawing a graph, to let you see the interesting signal</p>]]></content:encoded>
    <wp:post_name>137738257766</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/137273290271</link>
    <pubDate>Thu, 14 Jan 2016 21:35:39 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/137273290271</guid>
    <!--<wp:post_id>137273290271</wp:post_id>-->
    <wp:post_date>2016-01-14 0:35:39</wp:post_date>
    <wp:post_date_gmt>2016-01-14 08:35:39</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Talks in the near future</title>
    <description></description>
    <content:encoded><![CDATA[<p>University of Pennsylvania Biostatistics <a href="http://www.med.upenn.edu/apps/thyme/index.php?event_action=View&amp;eid=58074&amp;instance=2016-2-2">Seminar</a>,&nbsp;Feb 2:&nbsp;"Difficult questions about difficult endpoints: Carryover effects on incident hypertension"</p><p>Wombat 2016 <a href="http://wombat2016.org/">workshop</a> “Making Data Analysis Easier”, Melbourne Zoo, Feb 18-19:&nbsp;“Domesticating survey data analysis”</p><p>Victoria University of Wellington, Feb 25 or 26. Probably similar to talk at Penn.&nbsp;</p>]]></content:encoded>
    <wp:post_name>137273290271</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/137235870091</link>
    <pubDate>Thu, 14 Jan 2016 09:09:52 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/137235870091</guid>
    <!--<wp:post_id>137235870091</wp:post_id>-->
    <wp:post_date>2016-01-13 12:09:52</wp:post_date>
    <wp:post_date_gmt>2016-01-13 20:09:52</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>What does ‘design-consistent’ even mean?</title>
    <description></description>
    <content:encoded><![CDATA[<p>In classical survey statistics you have a fixed finite population of size $N$ and a (possibly unequal-probability, multistage) sample of size $n$. Useful asymptotics requires an infinite sequence of populations and samples chosen so that approximation errors from neglecting terms that decrease in $n$ and $N$ are practically unimportant in the real data when they are asymptotically negligible in the infinite sequence.&nbsp;</p><p>For&nbsp;‘model consistency’ this is easy. &nbsp;An estimator $\hat\theta_n$ is model consistent if $\hat\theta_n\stackrel{p}{\to}\theta_0$ when the population of size $N$ is a sample from a model $P_\theta$ with parameter $\theta=\theta_0$, for all designs obeying regularity conditions to be described in the proof.&nbsp;</p><p>For design consistency the heuristic is that $\hat\theta_n$ is close to the&nbsp;‘census parameter’ $\tilde\theta_N$, the result of estimating $\theta$ using the complete population. So, roughly, we want $\hat\theta_n-\tilde\theta_N\stackrel{p}{\to}0$ for any sequence of populations. The problem with using this as a definition is that there are no design-consistent estimators. For example, if $\tilde\theta$ is the mean of a variable $X$ and $\hat\theta$ the Horvitz-Thompson estimator, we will not get design consistency if the spread of $X$ increases too fast with $N$ and $n$. “Any sequence of populations” is too broad a condition.</p><p>One approach is to put explicit conditions on the sequence of fixed populations. For example, in the case of the mean, we could require<br>$$\limsup\frac{1}{N}\sum_{i=1}^N \left(x_{N,i} - &nbsp;\frac{1}{N}\sum_{j=1}^N x_{N,j}\right)^2&lt;\infty$$<br>where $x_{N,i}$ is the $i$th observation in the population of size $N$ and the double-sum expression is the thing that would be the variance if $x_{N,i}$ were random variables, which they aren’t.</p><p>If you take this approach, you end up writing down a lot of conditions that look like moments or tail bounds for random variables, but aren’t because the things aren’t random variables. I think it’s simpler to just write design-consistency in terms of model consistency for misspecified models.&nbsp;</p><p>That is, we consider the event $\hat\theta_n-\tilde\theta_n\stackrel{p}{\to}0$ and require it to occur for almost all sequences of populations drawn from any fixed distribution $P$ in a `large enough’ set ${\cal P}$. &nbsp;</p><p>So,&nbsp;‘large enough’? At the minimum we want a&nbsp;‘nonparametric’ set of distributions: one that may satisfy tail conditions but whose tangent space is all zero-mean square-integrable functions. The interesting question is about dependence: when is it sufficient to consider only sequences of populations generated iid from some distribution $P$ and when is this an important loss of generality? The reason this isn’t obvious is that the population data vector can contain variables such as&nbsp;‘latitude’ and&nbsp;‘longitude’, which can be related in arbitrarily complex ways to the variables of interest.&nbsp;</p><p>My feeling is that iid sampling of populations is fine when you are only interested in marginal models or in small-area estimation, but that explicit consideration of large-scale spatial structure requires putting population structure into the generating model. But I don’t know.&nbsp;</p>]]></content:encoded>
    <wp:post_name>137235870091</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/137194475311</link>
    <pubDate>Wed, 13 Jan 2016 16:00:30 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/137194475311</guid>
    <!--<wp:post_id>137194475311</wp:post_id>-->
    <wp:post_date>2016-01-12 19:00:30</wp:post_date>
    <wp:post_date_gmt>2016-01-13 03:00:30</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Another view of the ‘nearly true’ model</title>
    <description></description>
    <content:encoded><![CDATA[<p>Ok, so to recap, we have a large model (such as ‘we know the marginal sampling probabilities’) and a small model (such as the subset of the large model with $\mathop{logit}\,P[Y=1]=x\beta$). &nbsp;Under the large model, we would use the estimator $\hat\beta_{L}$, but under the small model there is a more efficient estimator $\hat\beta_S$. That is, under the small model<br>$$\sqrt{n}(\hat\beta_S-\beta_0)\stackrel{d}{\to}N(0,\sigma^2)$$<br>and<br>$$\sqrt{n}(\hat\beta_L-\beta_0)\stackrel{d}{\to}N(0,\sigma^2+\omega^2)$$</p><p>We’re worried that the small model might be slightly misspecified. One test of model misspecification is based on $D=\hat\beta_S-\hat\beta_L$. &nbsp;Under the small model, $\sqrt{n}D\stackrel{d}{\to}N(0,\tau^2)$ for some $\tau^2$. This test isn’t a straw man -- for example, DuMouchel and Duncan recommended it in the context of survey regression in a <a href="http://www.stat.cmu.edu/~brian/905-2008/papers/DumouchelDuncan-JASA-1983.pdf">1983 JASA paper</a>.</p><p>If we assume that $\hat\beta_S$ is (locally, semiparametric) efficient in the small model then $\tau=\omega$. &nbsp;Now suppose the small model is slightly untrue so that $\sqrt{n}D\stackrel{d}{\to}N(\Delta,\omega^2)$ with $\Delta&gt;0$. If, say, $\Delta=\omega$, then approximately<br>$$\hat\beta_S\sim N(\omega, \sigma^2)$$<br>and<br>$$\hat\beta_L\sim N(0, \sigma^2+\omega^2)$$<br>so the two estimators have the same asymptotic mean squared error. Since $\hat\beta_L$ is asymptotically unbiased it would probably be preferred, but the test based on $D$ has noncentrality parameter 1 and very poor power. If we relied on the test, we would probably end up choosing $\hat \beta_S$</p><p>So the test based on $D$ is not very useful if we want to protect against small amounts of model misspecification. We should use a better test.&nbsp;</p><p>But sometimes the test based on $D$ is the most powerful test or not far from it. Since we know what $\hat\beta_S$ and $\hat\beta_L$ look like as functionals of the distribution, we could try to maliciously arrange for the model misspecification to be in the direction that maximised $\hat\beta_S-\hat\beta_L$, and $D$ would then be the Neyman-Pearson most powerful test -- that’s what UMP tests look like for Gaussian shift alternatives. We can’t quite do that, but in large enough sample sizes we can come as close as we need.</p>]]></content:encoded>
    <wp:post_name>137194475311</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/136572298846</link>
    <pubDate>Mon, 04 Jan 2016 14:27:29 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/136572298846</guid>
    <!--<wp:post_id>136572298846</wp:post_id>-->
    <wp:post_date>2016-01-03 17:27:29</wp:post_date>
    <wp:post_date_gmt>2016-01-04 01:27:29</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Uncertainty and variability</title>
    <description></description>
    <content:encoded><![CDATA[<p>There’s a new (beta) tool called <a href="http://getguesstimate.com/">Guestimate</a>, designed to do sensitivity analysis for computations with uncertain data. &nbsp;This isn’t paranoid-engineer worst-case interval arithmetic, it’s simulation to compute probability distributions of a calculated parameter from probability distributions of the inputs. That’s trivial in, say, R, but Guesstimate lets you do it in a pointy-clicky way.</p><p>Unfortunately, it’s no good at modelling random variability. &nbsp;That may sound silly, since the whole basis of Bayesian statistics is claiming there isn’t a difference between uncertainty and variability, but it’s true. The problem is the type of sampling it will do.</p><p>The demonstration video on the site estimates the total number of hours that will be spent watching that very video. &nbsp;The input parameters are the length of the video, modelled as $N(2, 0.61^2)$ minutes and the number of viewers, modelled as $N(775, 442^2)$. The unround numbers there are because Guesstimate quite sensibly specifies Normal distributions by their 5th and 95th percentiles rather than mean and variance.</p><p>Here’s the resulting histogram:</p><figure class="tmblr-full" data-orig-height="256" data-orig-width="538"><img src="https://40.media.tumblr.com/af3bd3a6ceba4e480625a4b997611e09/tumblr_inline_o0elowTXFG1s1hdxy_540.png" data-orig-height="256" data-orig-width="538"></figure><p>One of the problems with the early state of the tool is obvious: the spike at zero. A Normal distribution whose 95th percentile is 30 times its 5th percentile has quite a lot of negative values. &nbsp;The histogram sets them to zero, which is probably ok here, but a warning would be nice, as would a better option for skewed non-negative distributions.&nbsp;</p><p>At a more basic level, though,this is a silly question. It is very easy to find out how long the video is (1:58, as it happens), and you should just <b>do</b> that rather than guessing.&nbsp;</p><p>A more interesting question along the same lines would be to incorporate the variability in how much of the video each person watches. &nbsp;I spent a while doing a little example of this using a distribution based on one of my YouTube videos, intending to write about how Normal distributions just weren’t good enough. But the whole computation was wrong.&nbsp;</p><p>Suppose we have an uncertain number of people, modelled as, say, logNormal with the 5th and 95th percentiles from the demonstration. They watch some variable length of a video modelled as, say, a mixture of&nbsp;‘stayers’ who watch the whole thing, and `quitters’ who drop off at a constant rate</p><figure class="tmblr-full" data-orig-height="229" data-orig-width="885"><img src="https://40.media.tumblr.com/877d0c23eb0c1d975c8c9769b150ce5b/tumblr_inline_o0em46V19O1s1hdxy_540.png" data-orig-height="229" data-orig-width="885"></figure><p>The total time spent watching is not the product of a single lognormal number of viewers and a single video length. It has the same mean as that distribution, but smaller variance. Everyone gets their own independent viewing length, and because of the law of large numbers a lot of the variability in the total goes away.</p><p>So, thinking about the demonstration video is actually an exercise in distinguishing two types of uncertainty. &nbsp;In some areas of hazard engineering there are terms&nbsp;“aleatory” and&nbsp;“epistemic” uncertainty, from the Latin word for dice and and the Greek word for knowledge, respectively. &nbsp;You can model both types with probability distributions, but the way to reduce epistemic uncertainty is to get more knowledge and the way to reduce aleatory uncertainty is to roll more dice.&nbsp;</p>]]></content:encoded>
    <wp:post_name>136572298846</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/136315807636</link>
    <pubDate>Thu, 31 Dec 2015 21:25:36 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/136315807636</guid>
    <!--<wp:post_id>136315807636</wp:post_id>-->
    <wp:post_date>2015-12-31 0:25:36</wp:post_date>
    <wp:post_date_gmt>2015-12-31 08:25:36</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Circumspice</title>
    <description></description>
    <content:encoded><![CDATA[<p>Norman Breslow died early this month. &nbsp;If you’ve had any involvement with medical statistics you have used his work. There isn’t really any need to expound on his contributions. &nbsp;I have a few Norm memories.</p><p>In my first quarter at the University of Washington, I took BIOST 570 (generalised linear models) from Norm. One day, about halfway through the quarter, he appeared with a copy of&nbsp;‘Science’ and asked me why I hadn’t been a co-author on a paper from the Sydney Blood Bank. This was research about long-term HIV non-progressors, and I’d done a bit of consulting that led to an acknowledgement buried in the references.&nbsp;</p><p>Later, when I was looking for a PhD supervisor, I had some ideas of a topic related to correlated data with correlation structures more complicated than just clustering. I went to Norm, based on his paper with David Clayton on generalised linear mixed models. He said he wasn’t up to date with that literature and suggested Patrick Heagerty instead. Patrick claimed to be too new and inexperienced, and sent me back to Norm. &nbsp;Some people might have been put off by this. &nbsp;But, as Norm was always at pains to point out when I later told this story, he was right. Patrick was an ideal supervisor, and Norm (who did end up on my committee) was working on different topics by then.&nbsp;<br></p><p>The scary thing about talking with Norm in the audience was his interested and slightly puzzled expression. It looked the same whether he was learning something new or surprised that you’d say something that silly.&nbsp;</p><p>And finally, one of the most endearing things about Norm was his pride in the success of his students: for example, two winners of the COPSS Presidents’ Award, four winners of the Spiegelman Award, and other honours. One of the last times I saw him, he showed me his copy of the COPSS book&nbsp;“Past, Present and Future of Statistics” and pointed out his students among the fifty people on the cover.&nbsp;</p>]]></content:encoded>
    <wp:post_name>136315807636</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/136248780311</link>
    <pubDate>Wed, 30 Dec 2015 22:57:55 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/136248780311</guid>
    <!--<wp:post_id>136248780311</wp:post_id>-->
    <wp:post_date>2015-12-30 1:57:55</wp:post_date>
    <wp:post_date_gmt>2015-12-30 09:57:55</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Superfood sourcing</title>
    <description></description>
    <content:encoded><![CDATA[<p>Because reasons, I ended up looking at a website for a new superfood, &nbsp;<a href="https://www.konared.com/the-hawaiian-coffeeberry/">“The Hawaiian Coffeeberry ®”</a>. &nbsp;““The Hawaiian Coffeeberry ®”, of course, isn’t a Hawaiian plant -- it’s coffee, orginally from Ethiopia -- but at least the marketing is Hawaiian. Well, the parts that aren’t unsourced copying from various internet sites.</p><p>Here’s a description of what they think the dominant nutrients are:</p><figure data-orig-width="777" data-orig-height="352" class="tmblr-full"><img src="https://41.media.tumblr.com/4d226d6dd84a0be6d193ba6ded910d3b/tumblr_inline_o0605isYzc1s1hdxy_540.png" data-orig-width="777" data-orig-height="352"></figure><p>The chlorogenic acid bit is ok, though they don’t mention it’s also found in, eg, peaches, and eggplant, and potatoes.&nbsp;</p><p>For quinic acid, the first sentence is repeated verbatim at <a href="http://www.coffeechemistry.com/chemistry/acids">coffeechemistry.com</a>. More importantly, the second sentence is a warped version of Wikipedia</p><blockquote><p>This acid is a versatile chiral starting material for the synthesis of new pharmaceuticals. A medication for the treatment of <a href="https://en.wikipedia.org/wiki/Influenza" title="Influenza">influenza A</a> and <a href="https://en.wikipedia.org/wiki/Influenza" title="Influenza">B</a>&nbsp;strains called <a href="https://en.wikipedia.org/wiki/Tamiflu" title="Tamiflu">Tamiflu</a> has been successfully developed and launched into the market.<br></p></blockquote><p>Here it is pretty clear which is the original: the Wikipedia claim about Tamiflu is true (quinic acid was a starting material) and the&nbsp;“The Hawaiian Coffeeberry ®” claim is false (quinic acid is <b>not</b>&nbsp;marketed as Tamiflu).</p><p>For the third `nutrient’, the second spelling&nbsp;“ferulic acid” is the correct one. The second sentence is a <a href="https://en.wikipedia.org/wiki/Ferulic_acid#In_food">very close match</a> to Wikipedia. What’s really interesting is the first sentence, with the numbers that look like references but aren’t. Teachers will recognise free-floating reference markers as a sign of plagiarism. Teachers will also know the Google reveals all (or at least&nbsp;‘most’).&nbsp;</p><p>There are a <a href="http://www.digood.com/product/c/aHR0cDovL3d3dy5tYWRlLWluLWNoaW5hLmNvbS9zaG93cm9vbS9sZWFkZXJjaGVtZ3JvdXAvcHJvZHVjdC1kZXRhaWxpZVdKdkt0eG5acHcvQ2hpbmEtRmVydWxpYy1BY2lkLUNBUy1Oby0xMTM1LTI0LTYtRmFjdG9yeS5odG1s.html">few</a> places where the phrase&nbsp;"contributes to the antibacterial [1] and antioxidant [2] properties" appears. In those places, the full sentence is</p><blockquote><p>&nbsp;“It is one of the phenolic compounds found in the invasive plant species carpobrotus edulis and contributes to the antibacterial[1] and antioxidant[2] properties of the plant.”</p></blockquote><p>Searching on that sentence gives a Wikipedia entry, but for <a href="https://en.wikipedia.org/wiki/Rutin#Occurrences">rutin</a>, not for ferulic acid.&nbsp;</p><p>Fail.&nbsp;</p>]]></content:encoded>
    <wp:post_name>136248780311</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/135901026306</link>
    <pubDate>Fri, 25 Dec 2015 22:27:39 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[photo]]></category>
		<category domain="category" nicename="photo"><![CDATA[photo]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/135901026306</guid>
    <!--<wp:post_id>135901026306</wp:post_id>-->
    <wp:post_date>2015-12-25 1:27:39</wp:post_date>
    <wp:post_date_gmt>2015-12-25 09:27:39</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title></title>
    <description></description>
    <content:encoded><![CDATA[<div class="figure"><figure>
  <img src="http://78.media.tumblr.com/45a051221edc58937460cf52fe06fb8c/tumblr_nzwqa3P9DX1sueztxo1_1280.jpg" alt="">
</figure></div>

    <p>The new Onehunga beach, Christmas Day</p>]]></content:encoded>
    <wp:post_name>135901026306</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/135824020146</link>
    <pubDate>Thu, 24 Dec 2015 17:34:17 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/135824020146</guid>
    <!--<wp:post_id>135824020146</wp:post_id>-->
    <wp:post_date>2015-12-23 20:34:17</wp:post_date>
    <wp:post_date_gmt>2015-12-24 04:34:17</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>The Muntab Question Strikes Back</title>
    <description></description>
    <content:encoded><![CDATA[<blockquote><p>Public Policy Polling, which is known for adding questions in surveys to exploit Republicans who are less informed, recently found that 30% of Republican voters would support bombing Agrabah, a fictional country in the Disney film Aladdin. On December 20, 2015, <a href="http://www.wparesearch.com/uncategorized/44-percent-of-democrats-support-taking-refugees-from-a-fictional-country/">WPA Research fielded </a>a national survey of 1,132 registered voters that found 44% of Democrats would support .....<br></p></blockquote><p>As you know, I think the Agrabah bombing question was misleading and unhelpful to public political discourse. It’s only slightly interesting that a <a href="https://en.wikipedia.org/wiki/Wilson_Perkins_Allen_Opinion_Research">Republican polling firm</a>&nbsp;wants to play the same trick.&nbsp;</p><p>The `bombing Agrabah’ question made a good story because on both moral and international-law grounds you shouldn’t bomb other countries unless you have a very good reason. You need to trick people into saying they want to bomb somewhere they haven’t heard of.&nbsp;</p><p>So, what can you trick lefties into saying about Agrabah? Something about taxes or gay marriage or abortion? Well, no.</p><p>Article 3 of the &nbsp;UN Convention Relating to the Status of Refugees says</p><blockquote><p>The Contracting States shall apply the provisions of this Convention to refugees
without discrimination as to race, religion or country of origin.<br></p></blockquote><p>And, according to Wilson Perkins Allen Research&nbsp;the scandalous opinion you can &nbsp;trick 44% of Democratic-leaning into stating: they would&nbsp;<b>support</b> accepting refugees from somewhere they’ve never heard of.&nbsp;</p><p>I think someone’s got that backwards.</p>]]></content:encoded>
    <wp:post_name>135824020146</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/135700254541</link>
    <pubDate>Tue, 22 Dec 2015 22:16:36 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/135700254541</guid>
    <!--<wp:post_id>135700254541</wp:post_id>-->
    <wp:post_date>2015-12-22 1:16:36</wp:post_date>
    <wp:post_date_gmt>2015-12-22 09:16:36</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Potential energy and kinetic energy</title>
    <description></description>
    <content:encoded><![CDATA[<p>Q: So, I hear you have a new bike?</p><p>A: Yes, it’s electric.</p><p>Q: One you don’t need to pedal?</p><p>A: No, you do still need to pedal, but the motor helps</p><p>Q: Doesn’t that defeat the purpose of cycling?</p><p>A: Well, that rather depends on what you think the purpose of cycling is.&nbsp;</p><p>Q: Do you want to expound?</p><p>A: Why, yes, thank you! Cycling is a great way to travel moderate distances on flat ground. As my learned colleague Richard Easther <a href="http://excursionset.com/blog/?offset=1438198000662">notes</a>, &nbsp;you can go about three times as fast as walking with roughly the same effort, if the road is flat and the air is still, but in Auckland, it isn’t. &nbsp;Here’s my commute home:</p><figure class="tmblr-full" data-orig-height="149" data-orig-width="431"><img src="https://41.media.tumblr.com/d6a4fd687c99d4d419cffeca17e28d3d/tumblr_inline_nzhst6xpEe1s1hdxy_540.png" data-orig-height="149" data-orig-width="431"></figure><p>Power-assisted bikes make cycling in Auckland like cycling in Melbourne.&nbsp;</p><p>Q: It still means less exercise, surely.</p><p>A: Power-assisted cycling up Symonds St burns lot fewer calories than cycling unassisted, yes. On the other hand, it burns a lot more than taking the bus, which is the real alternative.&nbsp;</p><p>Q: Ok, how often does it need to be charged?</p><p>A: About every 40km, so not every night. That’s with the smallest battery pack; there’s one that lasts about twice as long.&nbsp;</p><p>Q: How fast is it?</p><p>A: Depends on how fast you pedal, just like an ordinary bike. The top gear on mine isn’t very high, so not dangerously fast unless I’m going downhill.&nbsp;</p><p>Q: How expensive is all of this?</p><p>A: There’s a range of options, and it depends on the exchange rate. &nbsp;I’ve got the boring version, with three gears and a small battery, at about NZ\$2100. You can get a nice city bike with derailleur and front suspension, and a larger battery for about NZ\$2800. Or if you’re a <a href="https://www.bikeauckland.org.nz/flykly-like-the-wind-a-magic-electric-wheel-reviewed/">Dutch millennial </a>who needs to look cool, you can get a bike wheel with no gears but with regenerative breaking for US\$1000 and make an electric fixie.</p><p>Q: What happens if all the expensive technology is horribly unreliable?</p><p>A: Then I have a sad. However, the place I bought it is close enough to a train station that I can take a sick bike in for treatment without too much problem. And they use the bikes as rentals on Waiheke Island, so they can’t be too fragile.</p>]]></content:encoded>
    <wp:post_name>135700254541</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/135613650581</link>
    <pubDate>Mon, 21 Dec 2015 16:07:51 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/135613650581</guid>
    <!--<wp:post_id>135613650581</wp:post_id>-->
    <wp:post_date>2015-12-20 19:07:51</wp:post_date>
    <wp:post_date_gmt>2015-12-21 03:07:51</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Case-control estimation is more complicated than you think</title>
    <description></description>
    <content:encoded><![CDATA[<p>Well, obviously I don’t know how complicated you think it is, but it’s more complicated than <b>I</b> thought, and more complicated than my colleagues thought.</p><p>In a case-control design you sample all the cases ($Y=1$) and a fraction $\pi_0$ of the controls ($Y=0$) from a cohort. &nbsp;You could fit a logistic regression with sampling weights ($1$ for cases, $1/\pi_0$ for controls). Or, you can fit an unweighted logistic regression, which should be biased except that all the bias ends up in the intercept term and doesn’t affect the regression coefficients.&nbsp;</p><p>The unweighted analysis is maximum likelihood, so you’d expect it to be fully efficient (and it is). The weighted analysis is typically not fully efficient. Lots of people will tell you that the loss of efficiency is due to the variation in the weights, which makes sense but is not actually true.</p><p>When you have a regression model with weighted and unweighted estimators both consistent for the same parameters, it’s true that the loss of efficiency from weighting basically just depends on the coefficient of variation of the weights. &nbsp;There are two ways we can tell this isn’t what’s happening with case-control logistic regression. The first is that the unweighted estimator is fully efficient in some situations: eg, in a saturated model, or when all the regression parameters are zero. The second is that the weighted and unweighted estimators aren’t actually consistent for the same thing, it’s just that the bias is in the intercept term. The inefficiency of weighted estimation for case-control logistic regression is real, but it isn’t simple.&nbsp;</p><p>Another aspect of efficiency being more complicated than you would probably think is multiple imputation. Suppose you have a single continuous covariate $X$, with Normal distribution in controls and a (different) Normal distribution in cases. Suppose you impute $X$ for non-sampled controls using $X$ for sampled controls multiple times to give multiple full cohorts, and do the Rubin analysis. Does this give the same efficiency as weighted logistic regression on the sample or as unweighted logistic regression on the sample?&nbsp;</p><p>Both.&nbsp;</p><figure data-orig-width="504" data-orig-height="504" class="tmblr-full"><img src="https://41.media.tumblr.com/b85a99589a9a0b7f8c89dad16d476f02/tumblr_inline_nzotb39lrH1s1hdxy_540.png" alt="image" data-orig-width="504" data-orig-height="504"></figure><p>In the boxplots above, &nbsp;based on 1000 simulations with 100 cases and a cohort of 1000, “ML” is the unweighted logistic regression coefficient,&nbsp;“WL” is the weighted logistic regression coefficient, and the other two are both multiple imputation. &nbsp;The&nbsp;“Normal” box imputes $X$ for non-sampled controls from $N(m, s^2)$ with $m$ and $s^2$ the mean and variance of $X$ in sampled controls. &nbsp;The&nbsp;“Resampled” box imputes non-sampled controls by resampling $X$ from sampled controls (aka hot-deck imputation).&nbsp;</p><p>Imputing with the Normal distribution gives essentially full efficiency; hot-deck imputation gives essentially the same efficiency as weighted estimation. If you could see this coming, you get a gold star.</p><figure data-orig-width="378" data-orig-height="378" class="tmblr-full"><img src="https://40.media.tumblr.com/dd0578718976ddf64d6584666afa6092/tumblr_inline_nzotirmSg71s1hdxy_540.png" alt="image" data-orig-width="378" data-orig-height="378"></figure><p>Once I saw this, I could work out why it happened. It isn’t quite the issue of models for imputation and analysis being `compatible’ or `congenial’ -- the distribution of the imputed $X$ by hot-deck imputation converges uniformly to the true distribution of $X|Y=0$ -- but it’s a sort of second-order version of the same thing.</p><p>The extra information used by the maximum likelihood estimator that isn’t used by the weighted estimator comes from the relationship between the case and control densities of $X$, especially in the tails. Resampling from control $X$ loses the case information, but since the control distribution actually is Normal, the parametric imputation gets this information back. The Right Thing is to impute from a mixture of the control distribution and an exponentially tilted version of the case distribution, but to do this you need to know the true parameter value so it’s an iterative process and involves different iterative sets of imputations for each model you fit.&nbsp;</p>]]></content:encoded>
    <wp:post_name>135613650581</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/135494203646</link>
    <pubDate>Sat, 19 Dec 2015 21:09:21 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/135494203646</guid>
    <!--<wp:post_id>135494203646</wp:post_id>-->
    <wp:post_date>2015-12-19 0:09:21</wp:post_date>
    <wp:post_date_gmt>2015-12-19 08:09:21</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>The Muntab Question</title>
    <description></description>
    <content:encoded><![CDATA[<p>In a survey by Public Policy Polling, 30% of Republican-leaning and 20% of Democratic-leaning people said they supported bombing the fictional country of Agrabah. I’ve<a href="http://www.statschat.org.nz/2015/12/19/punkd/"> written on StatsChat</a> why I think these are deliberately misleading percentages and asking the question amounts to &nbsp;pissing in the swimming pool of public discourse. What I want to say here is that the&nbsp;“Don’t Know”s aren’t getting nearly enough stick.</p><p>Now, some of the&nbsp;“Don’t Knows” will have been successfully trolled by PPP and will think Agrabah is actually the name of somewhere in Syria or Iraq where bombing is a genuine question, and a subset of these will legitimately not have given enough attention to the question to be sure. That’s ok. But not-bombing should be the default. If you recognise that you don’t know where Agrabah is, the right answer to&nbsp;“Do you support bombing them?” isn’t “Don’t Know”, it’s &nbsp;“HELL, NO!”.</p>]]></content:encoded>
    <wp:post_name>135494203646</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/135212117851</link>
    <pubDate>Tue, 15 Dec 2015 11:49:57 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/135212117851</guid>
    <!--<wp:post_id>135212117851</wp:post_id>-->
    <wp:post_date>2015-12-14 14:49:57</wp:post_date>
    <wp:post_date_gmt>2015-12-14 22:49:57</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>A simple probability problem</title>
    <description></description>
    <content:encoded><![CDATA[<p>Amy Hogan, a stats and maths teacher who blogs at <a href="http://alittlestats.blogspot.co.nz/">A Little Stats</a>, posted the following quiz on twitter:</p><blockquote><p>(Assuming fair dice) which has the highest probability:</p><p>1 six from 6 dice</p><p>2 sixes from 12 dice</p><p>3 sixes from 18 dice</p></blockquote><p>The calculations aren’t too hard even by hand, and we have pbinom() available (if we remember to check $&lt;$ vs $\le$ conditions). In that sense the question is easy, but I was looking for an intuitive argument.&nbsp;</p><p>Obviously, the probability of exactly $n$ sixes from $6n$ dice is decreasing in $n$, because the distribution is becoming less discrete. On the other hand, the probability of more than $n$ sixes is increasing towards 1/2, since the distribution is becoming more symmetric. It isn’t obvious to me which one wins.&nbsp;</p><p>Although I’d never encountered this before, it turns out to be a real classic. Isaac Newton answered it for Samuel Pepys, and got the brute-force calculations right, but then came up with an incorrect heuristic argument. Stephen Stigler has a <a href="http://arxiv.org/pdf/math/0701089.pdf">paper</a>, Joe Blitzstein pointed me to it before I wasted too much time.&nbsp;</p><p>The neatest relevant fact is that the difference between the median and mean of a Binomial distribution is strictly less than 1, and so when the mean is an integer the two are equal. That implies the sequence $P[\mathrm{Bin}(nk,1/k)\ge n]$ will tend to decrease with increasing $n$ for any $k$, but even that doesn’t quite prove the sequence is strictly monotone: we only know the probability is between $0.5$ and $0.5+P[\mathrm{Bin}(nk,1/k)=n]$. Also, there’s apparently no simple intuition behind the bound on the difference between mean and median.&nbsp;</p><p>In the end, it turns out to be true that $P[\mathrm{Bin}(nk,1/k)\ge n]$ is decreasing in $n$ for any integer $k$, &nbsp;but (pretty obviously)  $\mathrm{Bin}(nk,p)$&nbsp; doesn’t have to be decreasing with $n$ for general $p$. Any valid intuition has to take advantage of $p=1/k$. Stigler seems to think that’s an important barrier; I’m not convinced. Perhaps more off-putting, &nbsp;any valid intuitive argument would probably have to make it obvious that the mean and median were equal when the mean is an integer.&nbsp;</p>]]></content:encoded>
    <wp:post_name>135212117851</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/134689230456</link>
    <pubDate>Mon, 07 Dec 2015 13:17:59 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/134689230456</guid>
    <!--<wp:post_id>134689230456</wp:post_id>-->
    <wp:post_date>2015-12-06 16:17:59</wp:post_date>
    <wp:post_date_gmt>2015-12-07 00:17:59</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>QR codes and postal votes</title>
    <description></description>
    <content:encoded><![CDATA[<p>The voting papers for the NZ flag referendum have a QR code that uniquely identifies the voter.&nbsp;</p><figure class="tmblr-full" data-orig-height="768" data-orig-width="1476"><img src="https://40.media.tumblr.com/f7a306b7b5ccdb4b4edfb2e9945c8ee0/tumblr_inline_nyyn16PVwm1s1hdxy_540.jpg" data-orig-height="768" data-orig-width="1476"></figure><p>None of this is secret: the QR code is obvious, and is described on the Electoral Commission <a href="http://www.elections.org.nz/events/referendums-new-zealand-flag-0/voting-first-referendum">web page</a> for the referendum -- that’s where I got the picture above. &nbsp;In thinking about whether the QR code is bad, we need a&nbsp;‘threat model’. Who are we worried about, and what does the code let them do?</p><p>The Electoral Commission will have the codes and could (in principle) link them to votes. But that’s unavoidable with postal voting. There needs to be some way to determine whether something is an authentic ballot, and without widespread crypto use by voters[1] or currency-scale anti-counterfeiting measures this will have to be an identifier that is on the ballot or the return envelope. &nbsp;The Electoral Commission will have had access to the identifiers and postal addresses in order to send out the votes; we already have to trust them.&nbsp;</p><p>A printed number on the ballot would also work, but a printed number would be less machine-readable and more human-readable. Checking off votes would be slower, and there would be very slightly more potential for the vote-counters to remember a few of the numbers, and for these to somehow get looked up against names.&nbsp;</p><p>The QR code gives the Electoral Commission information they would already have access to and be able to misuse if they were Evil, and doesn’t make it any harder for them to have procedures that prevent misuse. For electoral workers to disclose any information is a&nbsp;‘<a href="http://www.legislation.govt.nz/act/public/2015/0066/latest/DLM6405411.html">corrupt practice</a>’, with prison time as a possible penalty.&nbsp;</p><p>Replacing in-person secret ballots with postal voting introduces insecurity both from tracking votes and from making vote-buying and extortion much easier. In some countries, the risk of vote-tracking would be real. Even if you think New Zealand is one of them, having the tracking mechanism visible on the ballots wouldn’t make things any worse.&nbsp;</p><p><br></p><p><br></p><p><i>[1] "Whoever thinks his problem can be solved using cryptography, doesn't understand his problem and doesn't understand cryptography." -- Roger Needham/Butler Lampson</i></p>]]></content:encoded>
    <wp:post_name>134689230456</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/134048614041</link>
    <pubDate>Fri, 27 Nov 2015 21:25:41 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/134048614041</guid>
    <!--<wp:post_id>134048614041</wp:post_id>-->
    <wp:post_date>2015-11-27 0:25:41</wp:post_date>
    <wp:post_date_gmt>2015-11-27 08:25:41</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Serious tongue-twister</title>
    <description></description>
    <content:encoded><![CDATA[<p>This is a flow-chart I put together, that may go in the documentation for the Health Research Council data monitoring committee. Obviously the questions in boxes are simplified and would need expansion in the text.</p><figure class="tmblr-full" data-orig-height="540" data-orig-width="720"><img src="https://40.media.tumblr.com/f3c1a3e68cc5718331b914c215bbfa23/tumblr_inline_nygrwyHyLE1s1hdxy_540.png" data-orig-height="540" data-orig-width="720"></figure><p>Most of the work of the data monitoring committee, and the work that the study does on our behalf, is concentrated at the twice-yearly meetings. Some things, though, can’t safely be left to accumulate for six months, so there’s urgent reporting of sufficiently noteworthy clinical events to a clinician on the data monitoring committee.</p><p>SUSAR stands for Suspected Unexpected Serious Adverse Reaction; you can see why we use the acronym. A SUSAR means: a <b>Seriously Adverse</b> thing happened to someone; there was some reason to attribute it to the treatment, so it’s a <b>Suspected</b>&nbsp;<b>Reaction</b>; and it was <b>Unexpected</b>, so participants weren’t warned about the possibility before giving consent.</p><p>It’s pretty standard around the world that SUSARs are reported urgently. The problem with just doing this with SUSARs is that the Suspected and Unexpected aspects are a bit subjective. For example, it might be that the severity is Unexpected, even though qualitatively the Adverse Reaction is Expected. Or it might be that the trial investigators don’t Suspect a connection to treatment, but a more paranoid person might. &nbsp;For that reason, it’s reasonably common for all Serious Adverse Events to be reported urgently, even if they aren’t Unexpected or aren’t Suspected Reactions. &nbsp;</p>]]></content:encoded>
    <wp:post_name>134048614041</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/133232938191</link>
    <pubDate>Sun, 15 Nov 2015 14:15:32 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[video]]></category>
		<category domain="category" nicename="video"><![CDATA[video]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/133232938191</guid>
    <!--<wp:post_id>133232938191</wp:post_id>-->
    <wp:post_date>2015-11-14 17:15:32</wp:post_date>
    <wp:post_date_gmt>2015-11-15 01:15:32</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title></title>
    <description></description>
    <content:encoded><![CDATA[
              <iframe width="400" height="225"  id="youtube_iframe" src="https://www.youtube.com/embed/bQ895ccz2rc?feature=oembed&amp;enablejsapi=1&amp;origin=http://safe.txmblr.com&amp;wmode=opaque" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>              <p>So, I was trying to write something serious last night about communicating probabilities for my talk to journalists next weekend, but it was a depressing day, so instead I did something frivolous that was vaguely related.</p><p>Wisława Szymborska won the 1996 Nobel Prize for Literature. I first encountered this poem when mathematician Evelyn Lamb linked to it at JoAnne Growney’s blog <a href="http://poetrywithmathematics.blogspot.co.nz/2012/02/szymborska-1921-2012-on-statistics.html">Poetry with Mathematics</a>.</p>      ]]></content:encoded>
    <wp:post_name>133232938191</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/132988806706</link>
    <pubDate>Wed, 11 Nov 2015 20:03:13 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/132988806706</guid>
    <!--<wp:post_id>132988806706</wp:post_id>-->
    <wp:post_date>2015-11-10 23:03:13</wp:post_date>
    <wp:post_date_gmt>2015-11-11 07:03:13</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Should SPRINT have stopped?</title>
    <description></description>
    <content:encoded><![CDATA[<p>The SPRINT trial comparing standard blood pressure treatment (to 140mmHg) with intensive treatment (to 120mmHg) stopped early in September and just <a href="http://www.nejm.org/doi/full/10.1056/NEJMoa1511939?query=featured_home">published this week</a>. &nbsp;Hilda Bastian <a href="http://blogs.plos.org/absolutely-maybe/2015/09/30/post-sprint-trial-headaches/">wrote about</a> the problems of early stopping back in September.</p><p>Now the results are out, we can see a lot more detail on what was going on. I think the right decision was made, but it’s not completely straightforward. Also, I’m only a simple country statistician, so I may be missing some issues.</p><p>Rather to my surprise, the trial does provide pretty convincing evidence that more-intensive treatment is better. Not only did the results for the primary outcome (serious cardiovascular events) go well across the stopping boundary, the trial would have been close to stopping even if the decision had been based on all-cause mortality. The treatment effect estimates will be biased by early stopping, but that’s fixable: there’s not just theory but <a href="http://rctdesign.org/Welcome.html">even software</a> that will undo the bias.&nbsp;</p><p>It’s not clear how useful the results will be in practice -- less than two-thirds of diagnosed hypertensives <a href="http://www.ncbi.nlm.nih.gov/pubmed/24171916">in the US</a> have blood pressure controlled even to the 140mmHg level -- but we won’t learn anything more about that in SPRINT, and at least the expected minor side-effects didn’t show up.&nbsp;</p><p>There are two main reasons why one might not want to stop the trial. The first is that there’s very little data so far on the <b>long-term</b> effects of intensive treatment. The second is that the intensive-treatment group had a higher rate of acute kidney injury measured by drop in eGFR. &nbsp;In a sense, these two reasons are connected.&nbsp;<br></p><p>In the Women’s Health Initiative hormone trials, an obvious difficulty right from the planning stage was that the heart disease results would be available faster than the breast cancer results. If, as expected, estrogen decreased cardiovascular risk but increased cancer risk, it would be necessary to keep running the trial to look for cancer effects while watching extra deaths from heart disease accumulate in the control group. If the investigators weren’t willing to do that, there wouldn’t be any value in the trial. In the end, as we know, estrogen increased cardiovascular risk and the decision was easy, but they had to plan for it being hard. In SPRINT, my sense is that there wasn’t the same planned tradeoff between short-term benefit and longer-term harm, and that the trial was designed to run longer in order to have power to detect smaller differences.</p><p>However, there was that increase in kidney injury, and something similar was seen <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4123215/">in the ACCORD tria</a>l of the same intervention in diabetics. &nbsp;There’s clearly an interest in finding out whether this is likely to lead to end-stage renal disease or even to chronic kidney disease. We’d also like to know whether the rate is going to go up or stay the same, or whether basically all the susceptible people have already had it.</p><p>There are two reasons I don’t think the kidney injury is sufficient reason to continue the trial, but I could be wrong about either of them. &nbsp;The first is time: I don’t think even the planned trial length would be enough to see if intensive treatment is causing end-stage renal disease. &nbsp;The second is ethical: I don’t think assessing fairly definite short-term CVD benefits vs possible long-term renal harm is the trial that patients and investigators signed up for.&nbsp;</p>]]></content:encoded>
    <wp:post_name>132988806706</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/132977678461</link>
    <pubDate>Wed, 11 Nov 2015 16:21:58 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/132977678461</guid>
    <!--<wp:post_id>132977678461</wp:post_id>-->
    <wp:post_date>2015-11-10 19:21:58</wp:post_date>
    <wp:post_date_gmt>2015-11-11 03:21:58</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Come work with us!</title>
    <description></description>
    <content:encoded><![CDATA[<p>The University of Auckland Stats department has three jobs available now, and we’re likely to have further positions next year and the following year: we’ve got several people close to retirement age.</p><figure class="tmblr-full" data-orig-height="800" data-orig-width="1200"><img src="https://41.media.tumblr.com/129977edd91fab2deda3b0cd388d4503/tumblr_inline_nxmrlo72Q91s1hdxy_540.jpg" data-orig-height="800" data-orig-width="1200"></figure><p><i>[photo from Wikimedia commons, by user partyzane CC-BY]</i></p><p>There are two Lecturer in Statistics positions; the US translation would be hard-money tenure-track Assistant Professor</p><blockquote><p>The Department of Statistics invites applications for three positions as Lecturer, following the retirement of senior academic staff. The Department is the largest in Australasia and is involved in undergraduate and postgraduate teaching, and in research across a broad range of theoretical and applied fields.</p><p>Applicants should have or be about to complete a PhD in Statistics or a closely related discipline. The department welcomes applicants with research in any area of Statistics or Probability, but is especially interested in forming productive collaborations inside the department or with other departments at the University. Areas of particular interest include statistics education, applied probability, forensic statistics, mathematical statistics, data science, ecological statistics, financial mathematics and economics, and social statistics.</p><p>Successful applicants will be expected to teach undergraduate and postgraduate courses, supervise student research projects and PhDs, develop their own independent research program, and participate in the organisation and running of the department.</p></blockquote><figure class="tmblr-full" data-orig-height="2301" data-orig-width="1823"><img src="https://41.media.tumblr.com/a60efab2e408fb16ea97edacb70a99a7/tumblr_inline_nxmrp1ONjo1s1hdxy_540.jpg" data-orig-height="2301" data-orig-width="1823"></figure><p><i>[Auckland Lantern Festival, next to campus. photo from Wikimedia commons by user Avenue CC-BY-SA]</i></p><p>There is also a Professional Teaching Fellow position. This is a full-time, permanent academic staff position specialising in teaching -- entirely unlike the North American&nbsp;‘adjunct’ approach to teaching. &nbsp;We have twelve current PTFs, this position is to replace someone who is taking well-earned retirement.</p><p>Our first-year statistics course is one of the largest courses at the University, and is both very highly rated by students and pretty modern. Most (though not all) of the lecturing is done by the PTFs; other academic staff are also involved with curriculum design. &nbsp;Some of the Professional Teaching Fellows also teach at higher levels, including postgraduate courses, and others are involved in the Tuākana programme for Māori and Pacific students</p><blockquote><p>The Department of Statistics at the University of Auckland is the largest Statistics Department in Australasia and one of the largest departments in the Faculty of Science.  <br> <br>The Statistics Department is seeking to appoint a highly organised, energetic and collegial person for this role. The Professional Teaching Fellow will join the team responsible for the organisation and delivery of first year courses in statistics. Duties may also include teaching other courses in the department. The department has a reputation for and commitment to teaching excellence. You will be encouraged to investigate novel strategies and technologies to enhance student learning and engagement, to support diverse styles of teaching and learning and delivery of high quality course materials.</p><p>You will have qualifications in statistics. You will also have a proven record of excellence in teaching and have demonstrated successful innovation or leadership in teaching. Formal teaching qualifications would be an advantage.</p></blockquote><p>The University careers site is <a href="https://www.opportunities.auckland.ac.nz/psp/ps/EMPLOYEE/HRMS/c/HRS_HRAM.HRS_CE.GBL?languageCd=ENG">here</a>; the magic code numbers are 17657 for the lecturers and 17663 for the Professional Teaching Fellow</p><figure class="tmblr-full" data-orig-height="667" data-orig-width="1000"><img src="https://40.media.tumblr.com/7a9cae8cecafc67f90db9e34ad2dc6b7/tumblr_inline_nxmru4WczC1s1hdxy_540.jpg" data-orig-height="667" data-orig-width="1000"></figure><p><i>[Vulcan Lane, Auckland: photo&nbsp;by Urban+Explorer, CC-BY-SA, at <a href="http://flickr.com/photos/44448999@N03/7850111118">http://flickr.com/photos/44448999@N03/7850111118</a>]</i></p>]]></content:encoded>
    <wp:post_name>132977678461</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/132525358386</link>
    <pubDate>Wed, 04 Nov 2015 20:22:29 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/132525358386</guid>
    <!--<wp:post_id>132525358386</wp:post_id>-->
    <wp:post_date>2015-11-03 23:22:29</wp:post_date>
    <wp:post_date_gmt>2015-11-04 07:22:29</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Communication not quite in your field</title>
    <description></description>
    <content:encoded><![CDATA[<p>Statisticians and journalists both have the problem of writing about a field from outside. I’ve picked up a pretty good background in the areas of biology and medicine relevant to primary prevention of cardiovascular disease, but I’m not a physician.<br></p><p>In statistics this is most important when trying to find examples either for teaching or for writing about statistical methods. Good examples, where the medical question really is covered by the statistical question, make it easier to understand; bad examples make it harder. My Seattle colleague Scott Emerson, in his course outlines, specifically asks students who think there’s something wrong with the substantive background of his examples to tell him (and he does have a medical degree)</p><p>In journalism it probably doesn’t matter so much, but bad examples can perhaps be misleading. They can certainly be jarring when they are in a field you do know something about. The same is presumably true of blogging.</p><p>This is all prompted by<a href="http://www.statnews.com/2015/10/29/cholesterol-lowering-drugs-may-undermine-effectiveness-of-flu-vaccine-studies-find/"> a story in STAT</a>, the new medical news offering from the owners of the <a href="https://www.bostonglobe.com/business/2015/11/03/globe-owner-launches-stat-national-health-and-life-sciences-news-site/jxObvL2S78o9qNWcvFq7lN/story.html">Boston Globe</a>. &nbsp;The story is about the possibility that statin drugs could reduce the effectiveness of the flu vaccine. There were two studies, one of <a href="http://jid.oxfordjournals.org/content/early/2015/10/15/infdis.jiv456.full">antibodies</a> in people taking or not taking statins, and one looking at <a href="http://jid.oxfordjournals.org/content/early/2015/10/15/infdis.jiv457.full">acute respiratory infection</a>.&nbsp;</p><p>The story was very good in most ways, but I spent a lot of time checking up on details because of the second paragraph</p><blockquote><p>Two research groups reported Thursday that people who take statins appear to benefit far less from influenza vaccinations than those who do not. The effect is particularly marked among people taking synthetic statins — fluvastatin (Lescol) and cerivastatin (Baycol) are examples — rather than naturally produced varieties of the drugs.</p></blockquote><p>Basically no-one in these studies would have been taking fluvastatin, which has a tiny market share. <b>Literally</b> no-one is taking cerivastatin: it was withdrawn from the market in 2001 for safety reason. I know about this because my collaborators in Seattle have been interested in cerivastatin for a long time, both as an example in drug safety regulation, and as a window into understanding the biology of rhabdomyolysis, the safety problem that got the drug pulled.</p><p>Looking at the actual papers, cerivastatin isn’t mentioned. Fluvastatin is, and it’s even first in the list of synthetic statins, but the main one by volume will be atorvastatin, the world’s most successful blockbuster drug. It looks as though fluvastatin is first because it’s the oldest of the three drugs mentioned.</p>]]></content:encoded>
    <wp:post_name>132525358386</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/131478660126</link>
    <pubDate>Mon, 19 Oct 2015 21:20:06 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/131478660126</guid>
    <!--<wp:post_id>131478660126</wp:post_id>-->
    <wp:post_date>2015-10-19 1:20:06</wp:post_date>
    <wp:post_date_gmt>2015-10-19 08:20:06</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Prefiltering very large numbers of tests</title>
    <description></description>
    <content:encoded><![CDATA[<p>Genome-wide association studies involve lots of analyses. Nearly always they involve lots of tests. Also, in contrast to gene expression studies or to state-specific estimates of political attitudes or small-area disease rate estimates, a lot of the null hypotheses are effectively true. That is, most single-nucleotide polymorphisms are so close to not having any effect on anything that we might as well call it zero. Most people express this in terms of the need for stringent Type I error control; Bayesians like Matthew Stephens might talk in terms of the very low prior probability of a non-negligible effect.&nbsp;</p><p>An obvious strategy is to prefilter the SNPs to cut down on the number of tests. You might pick SNPs in or near genes. Among those in genes you might pick non-synonymous variants. Among those, you might pick mutations in sites that are strongly evolutionarily conserved or ones that should have a big effect on protein structure. Or you might get really specific and just look at mutations that are predicted to cause <a href="http://www.nature.com/ng/journal/v47/n6/full/ng.3270.html">complete loss of function</a>. These all make sense, but they don’t work as well as you might think, for a simple mathematical reason: $\int_t^\infty e^{-x^2}\;dx$ gets very, very small very fast with increasing $t$.</p><p>This graph shows the effect size detectable after Bonferroni correction to p-values, relative to 90% power and a single test.&nbsp;</p><figure class="tmblr-full" data-orig-height="820" data-orig-width="1367"><img src="https://40.media.tumblr.com/469983daebe20e46cf5afe9578952962/tumblr_inline_nwgk40a2dJ1s1hdxy_540.png" data-orig-height="820" data-orig-width="1367"></figure><p>A million tests at 90% power takes you to an effect size a bit above 4. The halfway point, an effect size of about 2, happens before 100 tests, and an effect size of 3 is at a few thousand tests. Prefiltering by 99.9% means that you need about half the sample size you’d need with no prefiltering. This is just one of the examples where intuition based on the shoulders of the Normal distribution breaks down in the tails.&nbsp;</p><p>If the Normal tail probabilities didn’t fall off this fast, GWAS wouldn’t be feasible at all: you couldn’t take five cohorts with good power for one test and combine them to get a consortium with moderate power for a million tests. The penalty for doing a million tests is surprisingly small; correspondingly, the benefit for not dong a million tests is also surprisingly small.</p>]]></content:encoded>
    <wp:post_name>131478660126</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/131406539601</link>
    <pubDate>Sun, 18 Oct 2015 22:04:24 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/131406539601</guid>
    <!--<wp:post_id>131406539601</wp:post_id>-->
    <wp:post_date>2015-10-18 2:04:24</wp:post_date>
    <wp:post_date_gmt>2015-10-18 09:04:24</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Double robustness</title>
    <description></description>
    <content:encoded><![CDATA[<p>“Double robust” estimation in a regression problem uses a model for the outcome $Y$ given available data $Z$ and a model for the exposure $X$ given available data $Z$. The estimates are consistent if either model is correct and efficient if they both are correct<sup>1</sup>.<br></p><p>Described that way, double robustness doesn’t sound very useful.&nbsp;“All models are wrong; many models are useless”, as we can deduce from Box’s familiar aphorism, so the chance of one of two models being correct is no more than twice the chance of one model being correct: two times $4/5$ of $5/8$ of fuck all<sup>2</sup>.&nbsp;</p><p>There’s more to double robustness than that, in two ways.&nbsp;</p><p>First, the bias terms of the two models are multiplied in computing the bias for the parameter of interest. &nbsp;Asymptotically, a `correct’ parametric model would have bias $o_p(n^{-1/2})$, but a bias of $o_p(1)$ in one model is enough for the multiplication to be helpful, and a bias of $o_p(n^{-1/4})$ in both models is enough for the bias to be asymptotically negligible. Neither of those is a completely useless possibility. [In practice we care about actual bias rather than asymptotic bias, but the same principles more or less hold.]</p><p>Secondly, the real benefit of two models is that we can have completely different kinds of substantive knowledge about them. We could have useful knowledge about what affects $Y$, or we could have useful knowledge about what affects $X$, and these are different substantive possibilities.&nbsp;</p><p><br></p><p><br></p><p><i>1. When the two models are inconsistent, some people still call that&nbsp;“doubly robust”, but Robins et al say&nbsp;“generalised doubly robust”</i></p><p><i>
2. The notorious limerick about `A mathematician named Hall'
</i></p>]]></content:encoded>
    <wp:post_name>131406539601</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/131260408501</link>
    <pubDate>Fri, 16 Oct 2015 15:58:46 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/131260408501</guid>
    <!--<wp:post_id>131260408501</wp:post_id>-->
    <wp:post_date>2015-10-15 19:58:46</wp:post_date>
    <wp:post_date_gmt>2015-10-16 02:58:46</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Modelling assumptions</title>
    <description></description>
    <content:encoded><![CDATA[<p>I often complain about the word `assumption’ being overused, but here I’m genuinely talking about assumptions about the structure of the data made and not checked by statistical modelling code.</p><p>I’m teaching part of a longitudinal data analysis course at the moment, and so I looked at the newish <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4289620/">geeM</a> package, by Lee McDaniel and co-workers. This is based on a very nice idea: rather than implementing GEE with a (C-based) loop over clusters and lots of little covariance matrices, use a single sparse covariance matrix and pure R code (and analytic inverses for some matrix structures). If you have enough memory, the sparse-matrix approach turns out to be much faster, and since the code is all in R it’s much easier to extend and modify.</p><p>However, while preparing a lecture example, I read the following on the help page for geem(), in the documentation of the id argument</p><blockquote><p>a vector identifying the clusters. For instance, this could be subject id. Data are assumed to be sorted such that observations in a cluster are in consecutive rows. Further, higher numbered rows in a cluster are assumed to be later, and this determines the correlation structure. If NULL, then each observation is assigned its own cluster.</p></blockquote><p>My data didn’t have this structure; &nbsp;the answers I was getting were wrong.</p><p>I first met this issue back in 1996, when I first used Vince Carey’s&nbsp;“gee” package for S-Plus, which had the same assumption. I have been told by Patrick Heagerty, that requiring the user to sort the data once had noticeable speed advantages in the early 1990s over sorting the data in every model fit. Given the informant and the programmer I have to assume this really was true back then, even though it’s a bit surprising.&nbsp;</p><p>The original GEE macro for SAS by Rezaul Karim (1988) made the same assumption</p><blockquote><p>An &nbsp;ID &nbsp;variable &nbsp;should &nbsp;identify &nbsp;observations &nbsp;corresponding &nbsp;to each cluster. &nbsp;If there is a &nbsp;time sequence for observations within a cluster it should be preserved.</p></blockquote><p>In 1996 I implemented GEE i<a href="http://www.jstatsoft.org/article/view/v001i03">n XLISP-Stat</a>, and did not make assumptions about the data ordering: data on the same individual are not assumed contiguous, and when the ordering within an individual matters for the computations it must be specified. &nbsp;Stata approaches the problem by requiring the user to declare the structure of the data in advance and then using this information for subsequent analyses. SAS PROC GEE does not assume observations are contiguous within individual (though it does default to assuming they are in time order unless otherwise specified with the WITHIN= argument).<br></p><p>In one sense the problem with assuming the data are sorted is obvious: they may not be, and the user can get the wrong answer with no warning. Nowadays there’s more than that, though. &nbsp;Whether you think about it in terms of 3rd Normal Form or of Tidy Data, the order of rows in a data frame <b>should not matter.</b>&nbsp; For example,<a href="http://www.jstatsoft.org/article/view/v059i10"> to quote Hadley</a></p><blockquote><p>While the order of variables and observations does not affect analysis, a good ordering makes
it easier to scan the raw values.</p></blockquote><p>I’d always treated data frames that way, and that’s part of why I never liked the sorting assumption.</p><p><br></p><p>If we assume that sorting takes meaningful time/memory there are basically three strategies</p><ul><li>assume the data are sorted and just give wrong answer if they aren’t</li><li>check if the data are sorted and fail if they aren’t</li><li>don’t require or assume anything <i>(either check and sort if necessary, or don’t take advantage of ordering)</i></li></ul><p>The second strategy is used by some older Stata commands. Since the computations already must touch every row of the dataset there can’t be any important computational cost if the data are sorted. If they aren’t, it will often be obvious from the first few rows; in any case, since the answers will be wrong there is no particular virtue in shaving a little time from their computation.</p><p>As far as I can see, the second strategy dominates the first one, regardless of the actual computational costs.&nbsp;</p><p>Nowadays, the speed advantages are not important for GEE -- reordering the 20,000 record data set I’m using in this example takes 0.005 seconds -- so I would argue strongly that the third strategy is the best. I’m surprised that geeM uses the first strategy, and even more surprised that it’s not one of items listed in the&nbsp;‘Limitations’ section of the paper.&nbsp;</p>]]></content:encoded>
    <wp:post_name>131260408501</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/130538426111</link>
    <pubDate>Mon, 05 Oct 2015 21:08:08 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/130538426111</guid>
    <!--<wp:post_id>130538426111</wp:post_id>-->
    <wp:post_date>2015-10-05 1:08:08</wp:post_date>
    <wp:post_date_gmt>2015-10-05 08:08:08</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Convergent evolution and NZ Bird of the Year</title>
    <description></description>
    <content:encoded><![CDATA[<p>Forest &amp; Bird is the New Zealand equivalent of the UK’s Royal Society for the Protection of Birds, or the USA’s Audubon Society. Each year, they hold a&nbsp;“Bird of the Year” competition, to get more publicity for NZ birds.</p><p>The <a href="http://www.birdoftheyear.org.nz/">competition</a>&nbsp;is made possible by the relatively small number of bird species in New Zealand, partly because it’s an isolated set of islands, and partly because a depressing number of the birds are ex-species. The number of NZ species is small enough that there’s room for a few ‘courtesy natives’ that arrived with or after humans.&nbsp;</p><p>Anyone with an email address can vote -- for the beautiful but dumb takahē, the elegant kōkako, the rotund and orotund kereru, the raucous tui, or the Kiwi emblem of the kiwi.&nbsp;</p><p>Since the whole point of the exercise is to come up with superficially plausible rationales for supporting your preferred candidate, I’m going to argue for the <a href="http://www.nzscienceteacher.co.nz/curriculum-literacy/the-living-world/a-new-zealand-evolutionary-story-the-wrens/#.VhItu2SeDGd">New Zealand wrens</a> -- the rock wren and the rifleman -- as examples of convergent evolution.</p><p>Several NZ birds, extant and extinct, can be seen as filling mammalian ecological niches (moa, takahē, kiwi). The wrens, on the other hand, look and behave very much like the northern hemisphere wrens of the family&nbsp;<i>Troglodytidae,</i>&nbsp;and like the Australia fairywrens, emu-wrens, and grasswrens of the family <i>Maluridae</i>.&nbsp;</p><p>Despite the clear similarities, the three groups are as unrelated as possible -- their most recent common ancestor would be a common ancestor of most or all modern songbirds. We only know of the lack of relationship from molecular studies: first, analysis of eggwhite proteins, and then <a href="https://en.wikipedia.org/wiki/Sibley%E2%80%93Ahlquist_taxonomy_of_birds">DNA similarity</a>&nbsp;measured by the binding strength of one species’ DNA to another’s.&nbsp;</p><p>So, in the spirit of the Bird of the Year competition; <a href="https://shar.es/17VtLX">VOTE ROCK WREN FOR CONVERGENT EVOLUTION.</a></p>]]></content:encoded>
    <wp:post_name>130538426111</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/129692767316</link>
    <pubDate>Wed, 23 Sep 2015 17:16:22 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
		<category domain="tag"><![CDATA[edgelering]]></category>
		<category domain="tag" nicename="edgelering"><![CDATA[edgelering]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/129692767316</guid>
    <!--<wp:post_id>129692767316</wp:post_id>-->
    <wp:post_date>2015-09-22 22:16:22</wp:post_date>
    <wp:post_date_gmt>2015-09-23 05:16:22</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>NZ Flag Referendum pseudorandom numbers</title>
    <description></description>
    <content:encoded><![CDATA[<p>The counting process for the NZ Flag Referendum needs some way to break ties. The Act defines a way to generate pseudo-random numbers <a href="http://www.legislation.govt.nz/act/public/2015/0066/16.0/DLM6405468.html">(Schedule 4, clauses 14 to 22</a>). &nbsp;Anyone in computational statistics who reads this will recognise some of the magic numbers; for the rest of you, here’s what’s going on.</p><p>The Act almost defines the <a href="https://en.wikipedia.org/wiki/Wichmann-Hill">Wichmann-Hill</a>&nbsp;PRNG, a respectable, if old-fashioned algorithm that was the original RNG in R. We’ve changed the generator in R because Wichmann-Hill isn’t up to modern research use. Its period is only $6.95\times 10^{12}$, and you ideally don’t run a PRNG for longer than the square root of its period. For a research statistician, a limit of 2.5 million numbers in a stream isn’t enough, but the flag referendum can’t possibly need more than 5+4+3+2=14 random numbers.</p><p>I said the Act almost defines the&nbsp;Wichmann-Hill PRNG. The process to update the random state $(x,\,y,\,z)$ at each iteration is the same, &nbsp;but the output of Wichmann-Hill, &nbsp;is $$(x/30269 + y/30307 + z/30323) \bmod 1$$ and the output of the Flag PRNG is $$(10000x/30269 + 10000y/30307 + 10000z/30323).$$&nbsp;</p><p>If Clause 17 were changed to use</p><blockquote><p>rc = {[(10 000x) div 30 269] + [(10 000y) div 30 307] + [(10 000z) div 30 323]} rem 10 000</p></blockquote><p>the Flag PRNG would give Wichmann-Hill scaled to $[0,\,9999]$ with a uniform distribution over its range. Without the final “rem 10000″ it actually gives a non-uniform distribution on $[0,\,29997]$. In this application that’s not a big deal, but it’s a small change for the worse.</p><p>In the Amendment proposed by NZ Greens to add the&nbsp;‘Red Peak’ flag to the referendum, the seed for the pseudo-random generator is changed. Initially I couldn’t see why, but it was clearly harmless. The entropy in the PRNG seed comes from the $z$ component, which is based on the number of votes, and that didn’t change.</p><p>Chuan-Zheng Lee points out that the PRNG is the one in <a href="http://www.legislation.govt.nz/regulation/public/2001/0145/latest/DLM57125.html">Part 5 of Schedule 1A of the Local Electoral Regulations 2001</a>, and this defines the $x$ part of the seed to be the number of candidates plus 5. A possible reason is to ensure that elections for different positions (which might have the same number of votes, especially with a postal ballot) have different PRNG seeds.</p><p>There does seem to be a potential bug in the Local Electoral Regulations version though. Clause 48 says</p><blockquote><p>For the second and subsequent steps, replace the pseudo-random number for each candidate with the candidate's PRN at the previous step subtracted from 10 000.</p></blockquote><p>That could be negative. In a sense that’s ok: they’re still just as pseudorandom. It does suggest that the omission of the&nbsp;“rem 10000″ from the output operation was a mistake.</p><p><br></p><p><b>Update</b>: The description of counting Single Transferable Vote at the <a href="http://www.dia.govt.nz/DIAWebsite.nsf/0/38f781eae0881449cc256c0f00152ea3?OpenDocument">Department of Internal Affairs</a>&nbsp;refers to a paper by Hill, Wichmann, and Woodall (1987) that gives a program in Pascal. The program (unsurprisingly) uses the Wichmann-Hill PRNG initialised using number of seats, number of candidates, and number of votes, but gets it right. &nbsp;The seed setup isn’t exactly the same as in the regulations, but it’s similar.&nbsp;</p>]]></content:encoded>
    <wp:post_name>129692767316</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/129557070666</link>
    <pubDate>Mon, 21 Sep 2015 20:16:08 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/129557070666</guid>
    <!--<wp:post_id>129557070666</wp:post_id>-->
    <wp:post_date>2015-09-21 1:16:08</wp:post_date>
    <wp:post_date_gmt>2015-09-21 08:16:08</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Oranges and lemons</title>
    <description></description>
    <content:encoded><![CDATA[<p>One of the basic principles of applied statistics is that the data don’t tell you what the question is. &nbsp;For example, the distribution of a variable doesn’t tell you what summary statistic you are interested in.</p><p>For mean vs median, a good example is binary variables. If a variable (like the indicator variable for dying in a car crash) is 0 for most people and 1 for a few people, the variable is very highly skewed but the mean (probability) is a much more useful summary statistic than the median (zero). For minimum or maximum it’s a bit harder.&nbsp;</p><p>A few weeks ago I was in Melbourne, and in the discussions of weather that inevitably arise, I claimed Auckland had warmer winters. After returning to Auckland I started to doubt this. It’s kinda chilly here, still. <i>[Y’all in Michigan can stop laughing now]</i></p><p>My reason for believing the winters were warmer in Melbourne was horticultural. There are plants you can easily grow in Auckland that can’t cope with Melbourne weather. In particular, lemons grow in any backyard, but oranges and grapefruit require well-chosen varieties and especially sunny locations to produce good fruit in Melbourne. In Auckland, you just stick them anywhere convenient.&nbsp;</p><p>It turns out that the average minimum temperatures in Melbourne are lower than in Auckland, and the record minima are distinctly lower: there are four months where Melbourne’s monthly record is lower than Auckland’s all-time record.&nbsp;</p><p>Melbourne is colder than Auckland in the sense that the minimum temperature (in a typical period of a few years) is lower, and the minimum is an important summary statistic for garden design.&nbsp;</p>]]></content:encoded>
    <wp:post_name>129557070666</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/129400298906</link>
    <pubDate>Sat, 19 Sep 2015 17:51:26 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/129400298906</guid>
    <!--<wp:post_id>129400298906</wp:post_id>-->
    <wp:post_date>2015-09-18 22:51:26</wp:post_date>
    <wp:post_date_gmt>2015-09-19 05:51:26</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>398ppm and All That</title>
    <description></description>
    <content:encoded><![CDATA[<p><br>The theory that global warming is a Chinese hoax looks silly at first. Why would Chinese scientists and intellectuals, at the nadir of their influence during the Cultural Revolution, have been working on a Cunning Plan to give Chinese industry a minor competitive advantage that would begin fifty years in the future just as it stopped needing one? And where would they have got the resources to plant this idea with the right Western scientists?</p><p>However, when you realise that The Donald’s knowledge might be incomplete, things begin to fall into place. One of the signs that a scientific hypothesis is going somewhere is that it explains facts that weren’t used to create it. One of the most implausible facts about the “official” history of air pollution is that the same person, Thomas Midgely, is supposed to have invented both leaded gasoline and chlorofluorocarbon refrigerants. One person, responsible for both the twentieth century’s “invisible pollution” scares, allegedly requiring international anti-industry regulation to fix. What are the odds?</p><p>Midgely, we can now see, must have been an agent of the Chinese global warming conspiracy. Starting with lead, which is at least something with known toxicity, the conspiracy worked up through steadily more implausible ideas. &nbsp;Uncovering the earlier history of the conspiracy also gives us a more likely candidate than Mao for the instigator: the great Chinese revolutionary Sūn Zhōngshān, known as Sun Yat-sen in the West. Here is someone with the grand vision and the appreciation of the importance of science and technology to plan a century-long conspiracy to make China great. </p>]]></content:encoded>
    <wp:post_name>129400298906</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/129106239236</link>
    <pubDate>Tue, 15 Sep 2015 11:28:02 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/129106239236</guid>
    <!--<wp:post_id>129106239236</wp:post_id>-->
    <wp:post_date>2015-09-14 16:28:02</wp:post_date>
    <wp:post_date_gmt>2015-09-14 23:28:02</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Good reasons for assuming a spherical cow</title>
    <description></description>
    <content:encoded><![CDATA[<p>Talks and papers in statistics often have what purports to be an application but with assumptions that look implausible. That can be fine, but you need to know, and tell us, why you’re making those assumptions.</p><p>If I ask&nbsp;“Why are you assuming a spherical cow?”, here are some possible good answers:</p><ul><li>Honest theory: “It’s not really about cows. Greeble’s Conjecture is the leading open question in heterotrophic morphon theory. Cows are the traditional example because Greeble grew up on a dairy farm”</li><li>Toy problem, theoretical: “Realistic cows are much too hard, but we hope that if we can understand the case of spherical cows we will at least get an idea of where to start”<br></li><li>Toy problem, computational: “Computations for spherical cows are a million times faster, but when we have the parameters optimised we’ll do it for realistic cows”<br></li><li>Toy problem, back-compatibility: “Of course we’re using realistic cows for the actual data analysis, but the literature has focused on spherical cows so we need that for method comparisons”<br></li><li>Modelling, universality: “There’s a theoretical result that says the shape of the cow doesn’t affect the gravitational attraction”<br></li><li>Modelling, approximation: “Cows don’t lose much heat through their legs, so the spherical approximation is pretty good”<br></li><li>Modelling, budget: “We didn’t have funding to build a 3-d cow model, and the client was happy with spherical cows”<br></li><li>Realism, surprising: “I asked my collaborators about that and these cows actually are spherical.”<br></li></ul><p>There’s absolutely nothing wrong with abstract theory or toy problems in their place. If that’s what you’re doing, make sure we know that you know it.&nbsp;</p>]]></content:encoded>
    <wp:post_name>129106239236</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/129047372011</link>
    <pubDate>Mon, 14 Sep 2015 15:26:16 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/129047372011</guid>
    <!--<wp:post_id>129047372011</wp:post_id>-->
    <wp:post_date>2015-09-13 20:26:16</wp:post_date>
    <wp:post_date_gmt>2015-09-14 03:26:16</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>(high-dimensional) Space is Big.</title>
    <description></description>
    <content:encoded><![CDATA[<blockquote><p>Space is big. Really big. You just won't believe how vastly, hugely, mind-bogglingly big it is. I mean, you may think it's a long way down the road to the chemist, but that's just peanuts to space. <i>Hitchhikers Guide to the Galaxy</i></p></blockquote><p>There’s a <a href="https://gist.github.com/tslumley/373e3759a766af106a4e">simple simulation</a> that I used in stat computing class last year and for Rob Hyndman’s working group last week. Simulate data uniformly on a $p$-dimensional hypercube $[0,\,1]^p$ and compute nearest-neighbour distances.&nbsp;</p><p>Obviously these distances increase with $p$, since the squared Euclidean distance is a sum of non-negative terms. The mean distance between two random points is $\sqrt{p}/3$. &nbsp;How does this compare to the nearest-neighbour distance?</p><blockquote><p>&gt; x1&lt;-runif(5e4)<br>&gt; x2&lt;-matrix(runif(5e4*2),ncol=2)<br>&gt; x10&lt;-matrix(runif(5e4*10),ncol=10)<br>&gt; x100&lt;-matrix(runif(5e4*100),ncol=100)<br>&gt; <br>&gt; system.time(d1&lt;-knn.dist(x1,k=1))<br> &nbsp; user &nbsp;system elapsed <br> &nbsp;0.044 &nbsp; 0.002 &nbsp; 0.045 <br>&gt; system.time(d2&lt;-knn.dist(x2,k=1))<br> &nbsp; user &nbsp;system elapsed <br> &nbsp;0.066 &nbsp; 0.002 &nbsp; 0.088 <br>&gt; system.time(d10&lt;-knn.dist(x10,k=1))<br> &nbsp; user &nbsp;system elapsed <br> &nbsp;4.005 &nbsp; 0.010 &nbsp; 3.993 <br>&gt; system.time(d100&lt;-knn.dist(x100,k=1))<br> &nbsp; user &nbsp;system elapsed <br>705.281 &nbsp; 0.996 702.481<br>&gt; mean(d1/sqrt(1)/(1/3))<br>[1] 2.994883e-05<br>&gt; mean(d2/sqrt(2)/(1/3))<br>[1] 0.004764815<br>&gt; mean(d10/sqrt(10)/(1/3))<br>[1] 0.306989<br>&gt; mean(d100/sqrt(100)/(1/3))<br>[1] 0.9269603</p></blockquote><p>The typical nearest-neighbour distance for 50,000 uniform points in $[0,\,1]$ is 0.00003 times the mean interpoint distance, which is sort of what you’d expect. In $[0,\,1]^2$ it’s 0.005 times the mean distance, which is not too surprising.&nbsp;</p><p>In $[0,\,1]^{100}$, though, the nearest-neighbour distance is over 90% of the mean distance. If you haven’t seen this sort of computation before, that seems outrageous. Roughly speaking, in 100-dimensional space, if you have 50,000 points in a unit cube they are all about equidistant from each other!</p><p>If you think of the interpoint distance as a sum, that makes sense in terms of the Law of Large Numbers and Central Limit Theorem. The squared Euclidean distance is a sum of $p$ approximately independent things, so it should be close to its expected value; differing from it by a relative amount $O_p(1/\sqrt{p})$. If we have $n$ independent random variables like that, and they don’t have heavier tails than a Gaussian (which they &nbsp;don’t), the worst departure from the expected value should scale like $$O_p\left(\sqrt{\frac{\log n}{p}}\right).$$ In other words, nearest-neighbours can’t be very much closer than the average interpoint distance.</p><p>Many&nbsp;‘black-box’ prediction methods are based on using outcome values from nearby points to predict outcome values for new points. This can’t work in high-dimensional space; there are no nearby points.&nbsp;</p><p>In a sense, everyone knows this. Statisticians call it the&nbsp;‘curse of dimensionality’, and modern sparse regression methods are aimed at bypassing it by not working in high-dimensional spaces. It’s still more dramatic that most people realise, and it’s hard for our intuition to cope, to think of a high-dimensional unit cube or sphere and ‘believe how vastly, hugely, mind-bogglingly big it is’.</p>]]></content:encoded>
    <wp:post_name>129047372011</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/127851439401</link>
    <pubDate>Sat, 29 Aug 2015 20:45:30 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/127851439401</guid>
    <!--<wp:post_id>127851439401</wp:post_id>-->
    <wp:post_date>2015-08-29 1:45:30</wp:post_date>
    <wp:post_date_gmt>2015-08-29 08:45:30</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Net Reclassification Index: surprisingly weird.</title>
    <description></description>
    <content:encoded><![CDATA[
<p><i>Attention Conservation Notice: Long. Really long. No, longer than that. Here: read the <a href="http://biostats.bepress.com/uwbiostat/paper392/">original</a> instead</i>.</p><p>The Net Reclassification Index (NRI) is a summary of improvement in prediction when new information is added, and an intuitively plausible one. Suppose that we’re trying to predict $Y=1$ vs $Y=0$, and that for person $i$ we have an old predicted probability $\hat p_{\textrm{old}}(i)$ and a new predicted probability $\hat p_{\textrm{new}}(i)$. &nbsp;We’d hope that the probabilities for cases ($Y=1$) go up and the probabilities for controls ($Y=0$) go down when more information is used. </p><p>Suppose the test set has $N_1$ cases and $N_0$ controls. The NRI is defined by<br>$$\frac{1}{2}\textrm{NRI}=\frac{1}{N_1}\sum_{Y_i=1} I\{\hat p_{\textrm{new}}(i)&gt;\hat p_{\textrm{old}}(i)\} -&nbsp;\frac{1}{N_0}\sum_{Y_i=0} I\{\hat p_{\textrm{new}}(i)&gt;\hat p_{\textrm{old}}(i)\}$$<br>The definition avoids evaluating tradeoffs about how much the probabilities go up, and is standardised to be (at least apparently) comparable between data sets with different case:control ratios.&nbsp;</p><p>As any schoolchild knows, evaluating the NRI on the same data used to estimate the probabilities will make it biased upwards: you’re basically asking&nbsp;“Do the probabilities in these data change in the same ways as the probabilities in the data where the estimation was done?” This isn’t hard. They do.&nbsp;</p><p>Basically everyone except Margaret Pepe assumed that using an independent test dataset would make this bias go away, as it does for other measures of predictiveness, good or bad. That’s not what happens. [The <a href="http://link.springer.com/article/10.1007%2Fs12561-014-9118-0">paper by Pepe and co-workers</a> is behind a paywall, but their&nbsp;<a href="http://biostats.bepress.com/uwbiostat/paper392/">working paper</a> is available.] &nbsp;After hearing talks about the bias I still didn’t understand why it happened. This post is an attempt to explain. My conclusion for what’s actually going on is a bit different from theirs, but the implications are similar.&nbsp;</p><p>First, looking at a silly example shows that NRI can behave badly. Suppose $X$ is also binary and is predictive of $Y$, and that $$\hat p_{\textrm{old}}=P[Y=1|X=x_i].$$ The prediction rule divides people into&nbsp;‘high risk’ and&nbsp;‘low risk’. Now define $\hat p_{\textrm{new}}$ to be larger than $\hat p_{\textrm{old}}$ for&nbsp;‘high-risk’ people and to be smaller than&nbsp;$\hat p_{\textrm{old}}$ for ‘low-risk’ people. You can do this any way you like.&nbsp;</p><p>Since high-risk people are more likely to be cases than low-risk people, a greater proportion of cases than controls will have their probabilities go up. Conversely, a greater proportion of controls than cases will have their probabilities go down. The NRI will be positive, even though the old prediction rule is the best possible one based on $X$ and the new rule is strictly worse.&nbsp;</p><p>Since this is a silly example, it doesn’t necessarily mean there is a problem with NRI, but it isn’t encouraging. Under the same definitions of $X$ and $Y$, if we defined<br>$$\hat p_{\textrm{new}}(i)=\hat p_{\textrm{old}}(i)+\epsilon_i$$<br>with $\epsilon_i$ having zero mean, independent of $X$ and $Y$, NRI would be zero. That’s still not ideal, since the predictions are worse rather than the same, but it’s certainly better than NRI being positive.&nbsp;</p><p>Can we get NRI to be positive (on average) without doing something silly? Yes, in fact. Pepe and co-workers looked at a very simple continuous case, where Normal $X$ &nbsp;predicts (binary) $Y$, and (Normal) $Z$ is independent of $X$ and $Y$. &nbsp;If $\hat p_{\textrm{old}}$ is based on logistic regression with $X$, and&nbsp;$\hat p_{\textrm{new}}$ on logistic regression with $X$ and $Z$, their simulations showed the NRI will be positive (on average) even though the predictions are slightly worse using $Z$. I’ve put an <a href="https://gist.github.com/tslumley/77d3653b5a43fc129d15">example up as a GitHub gist.</a></p><p>The simulation shows that NRI is weird, but it still doesn’t explain why. &nbsp;When confusing things happen with logistic regression, a useful trick is to try the same problem with linear regression. Either the same confusing things will happen, but will be easier to analyse, or they won’t happen, meaning that the non-linearity is important.&nbsp;</p><p>In a <a href="https://gist.github.com/tslumley/6d3aeb0a8148c0f03028">linear version</a> of the simple problem with Normal predictors, the NRI averages very close to zero. That’s still probably not right -- it should be negative -- but it is different from logistic regression. Non-linearity is important.</p><p>Because logistic regression is an exponential-family model, the maximum likelihood estimators are moment estimators. We have $E[\hat p_{\textrm{old}}-\hat p_{\textrm{new}}]\approx 0$ both overall and conditional on $X$. Since $\textrm{logit}&nbsp;\hat p_{\textrm{new}}-\textrm{logit} \hat p_{\textrm{old}}$ has a symmetric distribution, $\hat p_{\textrm{new}}-\hat p_{\textrm{old}}$ will have an asymmetric, skewed distribution. Specifically, it will be positively skewed when $p$ is small, symmetric when $p\approx 0.5$, and negatively skewed when $p$ is large; that’s the only way to force it into $[0,\,1]$.&nbsp;</p><p>A positively-skewed, mean-zero distribution (typically) has a negative median, and a negatively-skewed mean-zero distribution (typically) has a positive median; the&nbsp;‘typical’ behaviour holds for these logit-Normal distributions. The change in $\hat p$ will be positively skewed and have negative median when $\hat p$ is small; it will be negatively skewed and have positive median when $\hat p$ is large. Since $X$ is predictive, $\hat p$ is larger for cases than for controls, so&nbsp;$P[\hat p_{\textrm{new}}&gt;\hat p_{\textrm{old}}]$ is greater than 1/2 for cases and less than 1/2 for controls and the NRI will be positive on average.</p><p>The silly example shows that NRI can behave very badly for arbitrary prediction rules. For rules that are well calibrated in the sense of means, the non-linearity of the data-to-probability transformation and the use of ordering rather than differences in NRI makes it tend positive when useless variables are added. Even with a linear model, though, the NRI doesn’t pick up the degradation of performance from irrelevant variables.</p><p><br></p><p>[<b>tl;dr:</b> &nbsp;NRI? Just say&nbsp;“No, thank you.”]</p>]]></content:encoded>
    <wp:post_name>127851439401</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/127145536376</link>
    <pubDate>Thu, 20 Aug 2015 20:58:34 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/127145536376</guid>
    <!--<wp:post_id>127145536376</wp:post_id>-->
    <wp:post_date>2015-08-20 1:58:34</wp:post_date>
    <wp:post_date_gmt>2015-08-20 08:58:34</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>A conservation tragedy</title>
    <description></description>
    <content:encoded><![CDATA[
<p>The <a href="http://www.nzherald.co.nz/nz/news/article.cfm?c_id=1&amp;objectid=11500322">NZ Herald is reporting</a> that hunters taking part in a pūkeko cull on one of the islands near Auckland killed four takahē.&nbsp;</p><p>Pūkeko (Porphyrio porphyrio) are the closest relatives of takahē (Porphyrio&nbsp;hochstetteri), but they aren’t all that close. Takahē are New Zealand natives, which were forced out of their wetland habitat to alpine grasslands by the Māori, and then nearly wiped out by the stoats and red deer introduced by Europeans. Pūkeko are a relatively recent introduction -- they arrived in New Zealand either with or after humans.</p><p>Pūkeko are common in New Zealand and around the world; takahē are critically endangered. The four birds killed in the cull represent 20% of the population on that island, and more than 1% of the total population of the species. It’s like losing 25 pandas or 70 black rhinos in one day.</p><figure data-orig-width="620" data-orig-height="310" class="tmblr-full"><img src="https://40.media.tumblr.com/c2273f6e57ac37ad3849fda140a21776/tumblr_inline_ntdi8uuCgE1s1hdxy_540.png" alt="image" data-orig-width="620" data-orig-height="310"></figure><p>The two species are moderately similar. On Tiritiri island I’ve seen the well-fed local pūkeko and wondered if they were takahē, but when I saw the real thing the differences were obvious. Pūkeko are black and dark blue, smooth and sleek, and all legs and knees. Takahē are the defensive forwards of the rail family, stolid and broad, with thick legs, big beaks, and gauzy green back feathers.&nbsp;<br></p><p>I can understand people confusing the two, but anyone who could make that mistake shouldn’t have been shooting large blue rails in a Hauraki Gulf bird sanctuary.</p>]]></content:encoded>
    <wp:post_name>127145536376</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/127141150256</link>
    <pubDate>Thu, 20 Aug 2015 19:00:52 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/127141150256</guid>
    <!--<wp:post_id>127141150256</wp:post_id>-->
    <wp:post_date>2015-08-20 0:00:52</wp:post_date>
    <wp:post_date_gmt>2015-08-20 07:00:52</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Color names from XKCD in R</title>
    <description></description>
    <content:encoded><![CDATA[<figure data-orig-width="363" data-orig-height="455" class="tmblr-full"><img src="https://40.media.tumblr.com/1641bb3eeceb333b0996186d8ce60d2a/tumblr_inline_ntdcccuEhl1s1hdxy_540.png" data-orig-width="363" data-orig-height="455"></figure><p>Randall Munroe at XKCD did a color names survey a few years ago, and <a href="http://xkcd.com/color/rgb.txt">published a list</a> of about a thousand colour names whose RGB values (averaged across his readers’ monitors) could be fairly reliably estimated.</p><p>I have finally got around to turning them into <a href="https://github.com/tslumley/xkcdcolors">an R package</a>. It’s only on GitHub so far. The functions are</p><ul><li>xcolors(max_rank=-1): List the top (most commonly given) max_rank color names; analogous to colors()<br></li><li>name2color(name, exact = TRUE, hex_only = TRUE, n = -1): Find colours exactly or partly matching a given name, reporting just the hex color string or the full data, optionally only using the top n colours.</li><li>nearest_named(color, hex_only = FALSE, max_rank = -1, Lab = TRUE): given a matrix of RGB or a vector of color hex strings, report the nearest colours with names, optionally restricting to the top n colors, and using either Lab or R’s RGB colour space to define&nbsp;‘nearest’.&nbsp;</li></ul><p>For example, the nearest named colors to the standard palette are&nbsp;</p><blockquote><p>[1] "black" &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "fire engine red" "vibrant green" &nbsp; "primary blue" &nbsp; <br>[5] "cyan" &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"bright magenta" &nbsp;"bright yellow" &nbsp; "silver" &nbsp; &nbsp; &nbsp; &nbsp; <br></p></blockquote><p>Restricted to the top 100 colours they are</p><blockquote><p>[1] "black" &nbsp; &nbsp; &nbsp; &nbsp; "red" &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "green" &nbsp; &nbsp; &nbsp; &nbsp; "violet" &nbsp; &nbsp; &nbsp; &nbsp;"cyan" &nbsp; &nbsp; &nbsp; &nbsp; <br>[6] "bright purple" "yellow" &nbsp; &nbsp; &nbsp; &nbsp;"grey" &nbsp; &nbsp; &nbsp; &nbsp; <br></p></blockquote><p>The picture at the top of this post shows the ColorBrewer&nbsp;“Dark2″ palette, with the nearest named colors in both RGB and Lab space.&nbsp;</p>]]></content:encoded>
    <wp:post_name>127141150256</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/126056960856</link>
    <pubDate>Fri, 07 Aug 2015 13:33:28 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/126056960856</guid>
    <!--<wp:post_id>126056960856</wp:post_id>-->
    <wp:post_date>2015-08-06 18:33:28</wp:post_date>
    <wp:post_date_gmt>2015-08-07 01:33:28</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Fox fails statistics; does NYT?</title>
    <description></description>
    <content:encoded><![CDATA[
<p>From <a href="http://www.foxnews.com/politics/2015/08/04/how-fox-news-determined-who-qualified-for-prime-time-gop-debate/">Fox</a></p><blockquote><p>Each poll has a different margin of error, and averaging requires a distinct test of statistical significance. Given the over 2,400 interviews contained within the five polls, from a purely statistical perspective it is at least 90% likely that the tenth place Kasich is ahead the eleventh place Perry.</p></blockquote><p>The <a href="http://www.nytimes.com/2015/08/07/upshot/why-fox-failed-statistics-in-explaining-its-gop-debate-decision.html?_r=0">Upshot blog</a> at the New York Times correctly points out that if this is a p-value, that’s not the way to interpret it. </p><blockquote><p>As anyone who has ever taken an introductory statistics course can attest, this is not what a test of statistical significance reveals. Significance tests answer a somewhat different question: If Mr. Perry and Mr. Kasich actually have an equal number of supporters, then how unlikely is it that the polls would reveal a difference that is this large?</p></blockquote><p>They go on to say</p><blockquote><p>Mathematically, there does exist a prior that allows statistical significance tests to be interpreted as probabilities. It’s the prior that anything can happen, and everything is equally likely.<br></p></blockquote><p>For the p-value to exactly match the posterior probability that’s true, but for it to approximately match it’s enough that the prior is locally flat -- that it’s flat over the range of values reasonably consistent with the data. In this case, that’s saying that our prior knowledge about the relative popularity of Kasich and Perry is much weaker than this set of polls.&nbsp;</p><p>More importantly, though, what is this 90%?</p><p>We have a difference between 3.2% and 1.8% based on an unweighted average of five polls with sample sizes <a href="http://www.quinnipiac.edu/images/polling/us/us07302015_U645de.pdf">710</a>, <a href="http://www.foxnews.com/politics/interactive/2015/08/03/poll-new-high-for-trump-new-low-for-clinton/">475</a>, <a href="http://images.businessweek.com/cms/2015-08-04/8302475320_tue.pdf">500</a>, <a href="http://www.monmouth.edu/assets/0/32212254770/32212254991/32212254992/32212254994/32212254995/30064771087/67f674c8-fd4a-4a93-afbc-8b246a83da56.pdf">423</a>, and <a href="http://www.scribd.com/doc/273490309/CBS-News-GOP-presidential-candidates-poll">353</a>. &nbsp;The standard error of the estimated difference will be about 0.45%. The difference is about three standard errors. Where does 90% come from?</p><p>Fox isn’t clear about how they did the computations, but the p-value for a two-candidate comparison is about 0.01. A Bonferroni correction for doing 10 tests comparing adjacent candidates, or 10 tests comparing Perry to the candidates in the debate would basically explain it. &nbsp;But if you’re interested in posterior probabilities for Kasich vs Perry (or for whoever is just above the line vs whoever is just below the line), you don’t need the factor of ten. Well, you never need the factor of ten, but in other circumstances you might need <i>something</i>.</p><p>We can do the Bayesian calculation right(ish). At these sample sizes the likelihood can be treated as Normal and at these low support ratings we can ignore the negative correlation between candidates. I used a Normal prior and Normal likelihood for the difference of the logit of the support percentage, to allow for diffuse priors that still stay non-negative. &nbsp;</p><p>Here’s the posterior probability as a function of the prior mean and standard deviation. The red area is a posterior probability of more than 90% than Kasich is more popular (in the sense measured by opinion polls) than Perry.</p><figure data-orig-width="504" data-orig-height="504" class="tmblr-full"><img src="https://41.media.tumblr.com/d2ab49d79afdfe819afcf233e7827289/tumblr_inline_nsounyLNtN1s1hdxy_540.png" alt="image" data-orig-width="504" data-orig-height="504"></figure><p>In order to not have a posterior probability greater than 90%, you need either an incredibly strong prior belief that the candidates have the same popularity (sd less than about 0.1 on logit scale) or a strong prior belief that Perry was more popular. &nbsp;</p><p>The real weaknesses in the Fox criteria were using opinion poll scores this early in the campaign as a measure of whether someone was a serious candidate and having a ten-person debate to begin with. &nbsp;</p>]]></content:encoded>
    <wp:post_name>126056960856</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/125916551041</link>
    <pubDate>Wed, 05 Aug 2015 20:54:41 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/125916551041</guid>
    <!--<wp:post_id>125916551041</wp:post_id>-->
    <wp:post_date>2015-08-05 1:54:41</wp:post_date>
    <wp:post_date_gmt>2015-08-05 08:54:41</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>JSM2015: notes on Seattle from an ex-resident</title>
    <description></description>
    <content:encoded><![CDATA[
<p><b>Getting from the airport:</b>&nbsp;Public transport. No question. Well, unless you have significant mobility problems, in which case why are you looking at travel advice from random strangers on the internet? Take the light rail to the end of the line (Westlake), then catch any bus from the same platform one stop to Convention Place. The alternatives are much more expensive, and have a fair chance of being slower.</p><p><b>Trolls</b>: The Fremont Troll is under the highway bridge in Fremont. Worth a visit. Take the 26 or 28 bus from whichever downtown ave they’re running on this month. 3rd, maybe?</p><p><b>Last minute printing</b>: There’s FedEx in the Convention Center.&nbsp;</p><p><b>Psychoactive substances:&nbsp;</b></p><ol><li>Coffee. Everywhere. If you want something that doesn’t say “Starbucks”, head uphill to Victrola on Pike or Stumptown on Pine. If you don’t mind the mermaid, head uphill to the totally over-the-top Starbucks Reserve, on Pike.</li><li>Tea. Starbucks still do not have a clue. There’s a Peets at 904 7th Ave, out the back of the Convention Center and about four blocks logical south.</li><li>Beer. Any nearby good places are too small to be worth recommending, except Pike Brewery, at the bottom of Pike St. On the other hand, most places that serve beer have something that even a hipster would reluctantly admit was drinkable. Anything you don’t recognise has a good chance of being ok. &nbsp;If you head to Fremont (qv) try Brouwer’s, 35th &amp; Phinney, for Belgian.&nbsp;</li><li>Chocolate: Fremont (qv) has a gourmet chocolate factory, Theo. No, not a confectionery shop; a factory. They make excellent dark chocolate, and also a big range of flavoured things.&nbsp;</li><li>Ok, I’m on record as being in favour of cannabis legalisation, so I should mention the one you’re thinking of. <a href="http://www.highaboveseattle.com/seattle-marijuana-stores/">High Above Seattle</a> has a list, and it appears there’s at least one in Fremont (qv)</li><li>Books: All the way up the hill , between Pine and Pike St, one block past Broadway is Elliott Bay Books. The University Bookstore is also excellent (especially for SF/F fans), but not remotely walking distance. The main bookstore in Fremont (qv) has closed, but there is a good second-hand store, Opheilia’s, on Fremont Ave just above the main intersection (the cats are not for sale).&nbsp;</li></ol><p><b>Former Soviet Dictators</b>: Fremont (qv) also has a statue of Lenin, rescued from Slovakia. It’s historically unusual in showing him as a revolutionary, with guns and flames.&nbsp;</p><p><b>Weird architecture</b>:&nbsp;</p><ol><li>Half-melted, brightly coloured metal (Gehry). The EMP museum at the Seattle Center is a museum focused around rock-and-roll and science fiction. It’s half way to Fremont (qv)<br></li><li>Glass and steel origami (Koolhaas). The Central Library actually functions quite well as a library, as well as being an architectural exhibit. The (nonfiction) books are in a long six-storey spiral. The blocky red chairs are more comfortable than they look. Sadly, the view from the top is disappointing.&nbsp;</li><li>Smith Tower: what skyscrapers used to look like, in 1914</li><li>The Space Needle. Look, it was 1962, ok? On a completely clear day (which is possible in August) the view is, actually, spectacular. &nbsp;If you do go, it’s cheaper to book a specific time in advance on the web.&nbsp;</li></ol><p><b>Food</b>: &nbsp;Most good places are too small to be worth recommending on a blog. &nbsp;If you have a group of statisticians of a certain age, including some with moderately conservative tastes and/or poor hearing, and want to find somewhere everyone can enjoy, Wild Ginger (downtown) is absolutely reliable, fairly quiet, and not outrageously expensive.&nbsp;</p><p><b>Weird sculpture</b>: In addition to the Troll and Lenin in Fremont (qv), there are dinosaurs by the canal (near 34th &amp; Phinney).&nbsp;</p><p>For High Art Weird Sculpture fans there is a sculpture park near the waterfront north of downtown (or west of the Seattle Center). The most dramatic piece is Calder’s <i>Eagle</i>, a 40-foot abstract llama in bright orange steel.&nbsp;</p>]]></content:encoded>
    <wp:post_name>125916551041</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/125624807926</link>
    <pubDate>Sun, 02 Aug 2015 10:18:17 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/125624807926</guid>
    <!--<wp:post_id>125624807926</wp:post_id>-->
    <wp:post_date>2015-08-01 15:18:17</wp:post_date>
    <wp:post_date_gmt>2015-08-01 22:18:17</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Pianos, heaps, and ethics of randomisation</title>
    <description></description>
    <content:encoded><![CDATA[<p><br>Suppose you could make the following observations:</p><ol><li>&nbsp;Ling (零) was a pianist</li><li>Every pianist has a favourite student</li><li>&nbsp;Different pianists have different favourite students</li><li>Ling was not the favourite student of any pianist*&nbsp;</li><li>Anything that Ling knew and that every pianist teaches to his favourite student ends up known by everyone in the Ling School of Piano (it’s like martial arts)</li></ol><p>If the first three observations continue to be true, the Ling School of Piano will obviously go on forever. From all five, &nbsp;you can deduce a lot of mathematical results about it, starting with most of arithmetic . Piano arithmetic turns out to be just the same as ordinary arithmetic†.</p><p>On the other hand, if you make these observations (per Dennett)</p><ol><li>I am a mammal<br></li><li>The mother of every mammal is a mammal#<br></li></ol><p>you can deduce that there have been infinitely many mammals. Which turns out not to be the case. </p><p>The problem was identified by the Greeks, if not earlier, as the logical paradox of the heap (σωρίτης, “sorites”). Suppose:</p><ol><li>I have a heap of sand<br></li><li>If I remove a single grain of sand from a heap, I still have a heap of sand<br></li></ol><p>From this, we can deduce that even when I’ve removed every grain of sand I will still have a heap of sand. Which seems silly. Most philosophers‡ who have considered the question say that the argument is wrong somehow.</p><p>There is a randomised-trial version of the sorites: “Should we randomise the last patient?”</p><p>Note that:</p><ol><li>&nbsp;It is unethical to choose a treatment at random when there is reliable evidence as to the best treatment<br></li><li>Therefore, it is unethical to conduct a randomised trial that is larger than needed to produce reliable evidence<br></li><li>Stopping the trial one person earlier won’t affect whether it produces reliable evidence<br></li></ol><p>From these principles we could deduce that randomised trials of non-zero size are never ethical. </p><p>While this is a coherent viewpoint, it’s an extreme one. The output of the argument seems much stronger than the inputs. The paradox of the heap is reassuring: not that it implies randomising the last patient is actually ethical, but that it supports thinking about the question in real settings rather than as an exercise in piano arithmetic. </p><p><br></p><p><br></p><p><i>*&nbsp;he was a brilliant pianist, but he liked thrash metal and dyed his hair</i></p><p><i>† at least, as applied to the ‘standard’ members of the Ling School — there could be nonstandard members who aren’t the favourite student of any standard member<br></i></p><p><i># that’s kind of the point of mammals</i></p><p><i>‡ but not all; these are philosophers we’re talking about</i></p>]]></content:encoded>
    <wp:post_name>125624807926</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/125376093496</link>
    <pubDate>Thu, 30 Jul 2015 10:04:59 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/125376093496</guid>
    <!--<wp:post_id>125376093496</wp:post_id>-->
    <wp:post_date>2015-07-29 15:04:59</wp:post_date>
    <wp:post_date_gmt>2015-07-29 22:04:59</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>The finite-but-very-large improbability drive</title>
    <description></description>
    <content:encoded><![CDATA[<p>From the <a href="http://www.nzherald.co.nz/technology/news/article.cfm?c_id=5&amp;objectid=11488924">Herald</a>, from the Telegraph</p><blockquote><p>Interplanetary travel could be a step closer after scientists confirmed that an electromagnetic propulsion drive, which is fast enough to get to the Moon in four hours, actually works.<br></p><p>The EM Drive was developed by British inventor Roger Shawyer nearly 15 years ago but was ridiculed at the time as scientifically impossible.</p></blockquote><p>I like a good scientific underdog story as much as the <a href="http://excursionset.com/blog/2015/7/30/secret-nasa-bs-detector">next guy</a>, but sometimes the scientific establishment is right. Roger Shawyer proposed a space drive that worked without propellant, and so violates the principle of conservation of momentum. His justification for the drive was based only on classical physics, and so could not possibly be right: the equations of classical physics imply conservation of momentum. There is no loophole. This is just maths.&nbsp;</p><p>Now, we know classical physics doesn’t completely describe the universe, so it is conceivable that the drive, by an amazing coincidence, happens to do exactly what Shawyer thought it would, but for some completely unrelated &nbsp;reason involving poorly-understood or still-undiscovered physics.&nbsp;</p><p>If you’re going to argue for that, though, you need good evidence. The most recent experiments used 700 watts of power to produce enough thrust to nearly lift a US penny or NZ 10c coin. More importantly, the device kept working just as well when it was pointed the wrong way, at right-angles to the supposed thrust effect. Clearly, experimental errors have not been reduced far enough to say anything positive.&nbsp;</p><p>Overturning the conservation of momentum is not just contrary to previous experiments. It implies that the laws of physics have a preferred direction: that light doesn’t work in the same way going north as going south. We’d need a lot more evidence for this to be worth taking seriously, let alone worth headlines about space travel.</p><p>While knowing some physics helps you to be skeptical about this sort of claim, it’s not necessary. You could look at <a href="https://en.wikipedia.org/wiki/RF_resonant_cavity_thruster">Wikipedia</a>. &nbsp;You could even look further down in the Herald story, at the quote from the guy who did the experiments:</p><blockquote><p><i>"Our test campaign cannot confirm or refute the claims of the EM Drive but intends to independently assess possible side-effects in the measurements methods used so far,"</i><br></p></blockquote><p>If you believe him about all the incredibly-unlikely stuff, why not consider that he might be right about this, too.&nbsp;</p>]]></content:encoded>
    <wp:post_name>125376093496</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/125156072946</link>
    <pubDate>Mon, 27 Jul 2015 20:14:09 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
		<category domain="tag"><![CDATA[silly tereo conservation]]></category>
		<category domain="tag" nicename="silly-tereo-conservation"><![CDATA[silly tereo conservation]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/125156072946</guid>
    <!--<wp:post_id>125156072946</wp:post_id>-->
    <wp:post_date>2015-07-27 1:14:09</wp:post_date>
    <wp:post_date_gmt>2015-07-27 08:14:09</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Te Wiki o Te Reo Māori</title>
    <description></description>
    <content:encoded><![CDATA[<p><br>Scene: A wetland by high-country stream, Te Wai Pounamu. Rangers from Te Papa Atawhai pick up a bedraggled hunter.</p><p>“How did you get up here? Where’s your boat”</p><p>“Not a boat. Over there.” He gestures</p><p>“Yeah? So why are you out here in the swamp?”</p><p>“Can’t go back.”</p><p>“What’s the matter, bro?” asks the bigger ranger, who’s obviously been chosen as the good cop.</p><p>“They got out of the net,” the hunter said, showing his bleeding hand is missing a finger</p><p>“You mean..”</p><p>“Kī tōnu taku waka topaki i te tuna”</p><p>The smaller ranger looks blank. His colleague fills him in on the full strength.</p><p>“F-ing poachers get more careless every year. His hovercraft is full of eels.” </p>]]></content:encoded>
    <wp:post_name>125156072946</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/124987394001</link>
    <pubDate>Sat, 25 Jul 2015 19:57:03 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
		<category domain="tag"><![CDATA[dfa]]></category>
		<category domain="tag" nicename="dfa"><![CDATA[dfa]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/124987394001</guid>
    <!--<wp:post_id>124987394001</wp:post_id>-->
    <wp:post_date>2015-07-25 0:57:03</wp:post_date>
    <wp:post_date_gmt>2015-07-25 07:57:03</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>stringsAsFactors = &lt;sigh&gt;</title>
    <description></description>
    <content:encoded><![CDATA[<p>Problems with R can be divided into several groups:</p><ul><li>R has the defects of its virtues: pass-by-value and deep copying make the language easy to learn, but waste a lot of memory.&nbsp;<br></li><li>R is old: it’s not written in C++ and doesn’t have a 64-bit integer type because those weren’t things in 1992<br></li><li>Base R (and S before it) was developed for interactive use and then got extended into computational infrastructure. For example, the&nbsp;‘magical’ handling of extra arguments to model.frame() probably seemed like a good idea at the time.</li><li>If you want Genstat, you know where to find it: base R isn’t going to address everyone’s often-inconsistent needs</li><li>R-core screws up just like everyone else (examples would be invidious)</li></ul><p>The stringsAsFactors option to read.table() and data.frame() is an example of the first, second, and fourth groups (and, some people would say, the fifth).&nbsp;</p><p>Backing up, why do we have strings and factors? There are (at least) three incompatible use cases for things that look superficially the same:</p><ul><li>Character strings: sequences of characters, accessible by position, by regular expression search, and by more complicated search algorithms like suffix trees</li><li>Enumerations: a type with a fixed, known-in-advance set of values</li><li>Value labels: a mostly-numeric type, but with some values representing non-numeric values such as&nbsp;“don’t know”,&nbsp;“refused”,&nbsp;“could not contact”,&nbsp;“impossible value removed in editing”,&nbsp;“below limit of detection”</li></ul><p>Statistical systems tend to have value labels, plus half-assed support for strings. Programming languages tend to have enumerations and strings (or, in older languages like Fortran, half-assed support for strings)</p><p>R had strings (a three-quarter-assed implementation that, like classic C, is just enough to build something useful on top of). R (and S) also had&nbsp;“factors”.</p><p>In S, the implementation of factors is as small integers with a&nbsp;‘levels’ attribute. They could be used (well) as an enumerated type, or (badly) for labels on integer variables. They also had an important secondary use as a data-compression hack for strings with repeated values: each additional copy of&nbsp;“Massachusetts” as a string took a pointer plus 13 bytes plus a terminating NUL for each copy, but as a factor took just the 4 bytes for the integer. Even better, comparison of factor levels was simple integer equality, done in a single clock cycle, but comparison of strings required walking the string byte by byte. Back in the day, this mattered. &nbsp;</p><p>Originally in R, factors were a native type and were unambiguously for enumerations. For compatibility, this was changed R 0.62. As the NEWS entry says:</p><pre> o	All internal mechanisms to support factors and
    data.frames have been removed. These are now 
    entirely supported by interprete code! 
   `is.unordered' has been eliminated. Thanks
    to John Chambers for allowing the distribution of
    his StatLib code.</pre><p>So, for lo these many years we struggled on with read.table() automagically coercing strings to factors, and users painfully coercing them back again, but saving memory. The Right and Proper use of factors was as enumerated types, but not everyone agreed.</p><p>The next change happened in R 2.6.0</p><pre> o	There is now a global CHARSXP cache, R_StringHash.
  CHARSXPs are no longer duplicated and must not be modified in
  place. Developers should strive to only use mkChar (and
  mkString) for creating new CHARSXPs and avoid use of
  allocString. A new macro, CallocCharBuf, can be used to
  obtain a temporary char buffer for manipulating character
  data. This patch was written by Seth Falcon.</pre><p>The Bioconductor project needed to store and manipulate really big strings. &nbsp;It’s a bit inefficient to store and compare multiple copies of&nbsp;“Massachusetts”, but it’s a really bad idea to store and compare multiple copies of an entire chromosome.&nbsp;</p><p>The new format stored each string once, and then used pointers for copies: memory use was about the same as factors, speed of comparison was about the same for duplicated strings but much faster for unique strings. Bioconductor also introduced a bunch of string tools in packages, in particular so that large numbers of short segments of a ridiculously long string could be handled efficiently.&nbsp;</p><p>As a result of all this development, R now has strings, and it has an enumerated type. It still doesn’t have value labels (there’s some support in packages, but nothing low-level). There’s now no reason to confuse strings and factors, and no reason to automatically assume that non-numeric variables are factors.&nbsp;</p><p>Or rather, that would be true if we wiped out all R users and code and started from scratch. Otherwise, as an early opponent of backwards compatibility noted:</p><blockquote><p>``'Tis the Last judgment's fire must cure this place,<br>``Calcine its clods and set its prisoners free.''</p></blockquote>]]></content:encoded>
    <wp:post_name>124987394001</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/124556697816</link>
    <pubDate>Mon, 20 Jul 2015 18:53:42 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/124556697816</guid>
    <!--<wp:post_id>124556697816</wp:post_id>-->
    <wp:post_date>2015-07-19 23:53:42</wp:post_date>
    <wp:post_date_gmt>2015-07-20 06:53:42</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Talks in the near future</title>
    <description></description>
    <content:encoded><![CDATA[<ul><li>On calibration of weights, efficiency, and model robustness, at the <a href="https://www.amstat.org/meetings/jsm/2015/onlineprogram/AbstractDetails.cfm?abstractid=315734">Joint Statistical Meetings</a> (8:30 on the Tuesday morning)<br></li><li>On StatsChat, for the data journalism program at AUT, on August 25</li><li>A repeat of my Lancaster Lecture:&nbsp;“Data Science: will computing and informatics eat our lunch?” for the Victorian branch of SSAI, the evening of Tuesday Sept 1</li><li>On the non-existence of ordinal tests, for <a href="http://www.vicbiostat.org.au/why-theres-no-such-thing-ordinal-test">VicBiostat</a>, Sept 3</li><li>On mixed models under complex sampling, at Monash University, Department of Econometrics and Business Statistics, Sept 11, 2pm</li><li>[Probably] On estimating carryover effects in randomised trials, NZSA meeting, Christchurch, &nbsp;Nov 24-26</li><li>On mixed models under complex sampling, for the Australasian Region of the Biometric Society, in <a href="http://www.biometrics.org.au/conferences/Hobart2015/index.html">Hobart</a>, Nov 30-Dec 3</li></ul>]]></content:encoded>
    <wp:post_name>124556697816</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/123054582462</link>
    <pubDate>Fri, 03 Jul 2015 08:02:11 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/123054582462</guid>
    <!--<wp:post_id>123054582462</wp:post_id>-->
    <wp:post_date>2015-07-02 13:02:11</wp:post_date>
    <wp:post_date_gmt>2015-07-02 20:02:11</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Base-free π day</title>
    <description></description>
    <content:encoded><![CDATA[<p>Pi day is celebrated on March 14 in all the countries that use the MM/DD/YYYY date format (ie, the USA). Pi Approximation day is celebrated in the rest of the world on July 22.&nbsp;</p><p>I’m proposing today for another one: π continued fraction day. Like the 22/7 festival it doesn’t depend on using base 10, and like American Pi day it is extensible when the stars align correctly.&nbsp;</p><p>The continued fraction expansion of&nbsp;π is<br>$$\pi=3+\frac{1}{7+\frac{1}{15+\frac{1}{292+\frac{1}{1+\cdots}}}}$$<br>Taking just a initial segment of this gives the best possible rational expansions: 3, 22/7, 355/113,...</p><p>In most years, π continued fraction day will be 3:7, giving the approximation 22/7, slightly better than 3.14. This year, however, it’s 3:7:15, the continued fraction giving $\pi\approx 355/113$, with an error of about $2.7\times 10^{-7}$. That’s two orders of magnitude smaller than the error in 3.1415, and it’s even smaller than the error when the US celebrates Correctly-Rounded Pi Day next year, 3/14/16</p>]]></content:encoded>
    <wp:post_name>123054582462</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/121985241006</link>
    <pubDate>Sat, 20 Jun 2015 19:47:56 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/121985241006</guid>
    <!--<wp:post_id>121985241006</wp:post_id>-->
    <wp:post_date>2015-06-20 0:47:56</wp:post_date>
    <wp:post_date_gmt>2015-06-20 07:47:56</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>A much-needed gap</title>
    <description></description>
    <content:encoded><![CDATA[<p>There are a surprisingly large number of research papers that use the Shapiro-Wilk normality test on data from NHANES or the British Household Panel Survey, two large multi-stage surveys. &nbsp;</p><p>This is a bad idea for multiple reasons</p><ul><li>Testing for normality is typically a bad idea. It’s unusual for Normal/non-Normal to be an interesting question. That’s in contrast to testing for a power law in skewed data, where apparently many people are interested in the question, though fewer of them in<a href="http://bactra.org/weblog/491.html"> how to answer it</a>.&nbsp;</li><li>The power of the test depends a lot on the sample size: in small samples it will reject nothing; in large samples it will reject almost anything. These national multistage samples tend to be large</li><li>The usual rationalisation for Normality tests is to do with outliers leading to either incorrect standard errors or to undue sensitivity to individual observations. When you’re doing an analysis with sampling weights it doesn’t make any sense to look at influence without looking at the sampling weights</li><li>What even is the null hypothesis? Since the test doesn’t take any account of the sampling, it can’t be a hypothesis about the population or the data-generating process.&nbsp;</li></ul><p>You might think there is an opportunity here for methodological innovation, adapting the Shapiro-Wilk test so its null hypothesis is a super-population Normal distribution. That would handle the fourth point above, but it would still count as filling a much-needed gap.&nbsp;</p>]]></content:encoded>
    <wp:post_name>121985241006</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/120598746581</link>
    <pubDate>Wed, 03 Jun 2015 20:40:21 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/120598746581</guid>
    <!--<wp:post_id>120598746581</wp:post_id>-->
    <wp:post_date>2015-06-03 1:40:21</wp:post_date>
    <wp:post_date_gmt>2015-06-03 08:40:21</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Countermatching</title>
    <description></description>
    <content:encoded><![CDATA[<p>Countermatching is a simple case-control sampling mechanism that makes people uncomfortable when they first encounter it. Get ready.</p><p>Suppose you want to study the effect of a relatively rare exposure (sufficiently high dose radiation to the heart) on a relatively rare outcome (heart failure in breast cancer survivors). If you just took a random sample of the population there would be very few breast cancer survivors, so you work with a cohort of breast-cancer survivors. But even if you took a random sample of them, not that many would have heart failure. &nbsp;We’ve known how to handle this for generations: you take a sample of cases, and then a sample of about as many&nbsp;‘controls’, people without the disease.</p><p>Now the problem will likely be that the cases are older than the controls, since heart failure gets more common as you get older. If we’re not interested in age we can match the cases and controls. For each case, find a breast cancer survivor of the same age but without heart failure as a control, and treat the two as a pair. If one of the pair is exposed and the other is unexposed, the pair gives information about the association between exposure and disease.</p><p>Even with matching on age, there’s a problem with the rare exposure. In many, perhaps most pairs, neither the case nor the control will be exposed. These pairs are uninformative about the association. &nbsp;Wouldn’t it be nice if we could ensure that every pair was informative?</p><p>The only way to make every pair informative is to match an unexposed control with every exposed case, and an exposed control with every unexposed case. There are two obvious problems. First, we’d need to know the exposure for everyone in the population to do this, so why would we even be sampling? Second, that’s just <b>weird</b> and will mess up the association between exposure and disease.</p><p>To address the first problem, while we can’t do this exactly, we might have a reasonable guess as to exposure for everyone in the population. In the breast cancer example (which I heard in a talk by Bryan Langholz) they knew which side the tumour had been on, and whether the woman had received radiotherapy. There would be no exposure without radiotherapy or for right-sided tumours; there would probably be some exposure for radiotherapy in left-sided tumours. The researchers could choose a definitely-unexposed control for each possible-exposed case, and a possibly-exposed control for every definitely-unexposed case. Once they had the sample they could look up the exact direction and intensity used for each radiation beam and work out the exact dose to the heart, but it wasn’t feasible to do this for all the women in their study population.&nbsp;</p><p>So, we can <b>countermatch</b> on a <b>surrogate exposure</b>&nbsp;to get a more informative matched case-control sample, but apparently at the cost of completely stuffing up the association we’re trying to measure. &nbsp;That’s why the idea initially makes people uncomfortable, especially epidemiologists, who have been trained in all the terrible things that can happen in a case-control study when the sampling is biased. Fortunately, we know <b>exactly</b> how the sampling is biased, because we did it. We can correct the bias by reweighting the data.&nbsp;</p><p>The usual way to reweight the data works with each countermatched pair separately. The weight depends on the ratio of the number of potential controls in the (surrogate) exposed and unexposed groups, and the resulting weighted likelihood is a <a href="http://folk.uio.no/borgan/cv/Langholz-Borgan-Biometrika-1995.pdf">genuine Cox-style partial likelihood</a>.&nbsp;</p><p>Another way to reweight the data is to break the matches, so a case is compared to all the sampled people, both cases and controls, at the same age. <a href="http://biomet.oxfordjournals.org/content/84/2/379.abstract">Sven Ove Samuelsen</a> came up with this idea for matched case-control studies. (I thought of it independently, but more than ten years too late.) Claudia Rivera and I have recently extended it to countermatched designs.</p><p>In this analysis the weight for a case is just 1, the weight for a control is the reciprocal of the sampling probability of that control -- not just in the matched pair where she was sampled, but cumulatively across all matched pairs. For this to work, you need to be able to work out exposure for each sampled person at a range of ages, not just at a single age. In the breast-cancer example that’s fine.</p><p>This weighted likelihood isn’t a partial likelihood, it’s one of the second-rate survey pseudolikelihoods. Typically, the pseudollikelihood analysis is less efficient, despite taking more work and requiring more data, so why would anyone use it? Unlike the partial likelihood, the survey pseudolikelihood allows&nbsp;<a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2768499/">calibration of weights</a>&nbsp;to&nbsp;bring in information on any relevant variables from all the unsampled people in your study cohort, and that can more than pay for the effort.&nbsp;</p>]]></content:encoded>
    <wp:post_name>120598746581</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/120095349481</link>
    <pubDate>Thu, 28 May 2015 22:26:08 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/120095349481</guid>
    <!--<wp:post_id>120095349481</wp:post_id>-->
    <wp:post_date>2015-05-28 3:26:08</wp:post_date>
    <wp:post_date_gmt>2015-05-28 10:26:08</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Was the fake chocolate study unethical?</title>
    <description></description>
    <content:encoded><![CDATA[<p>John Bohannon <a href="http://io9.com/i-fooled-millions-into-thinking-chocolate-helps-weight-1707251800">writes</a></p><blockquote><p>Other than those fibs, the study was 100 percent authentic. My colleagues and I recruited actual human subjects in Germany. We ran an actual clinical trial, with subjects randomly assigned to different diet regimes. And the statistically significant benefits of chocolate that we reported are based on the actual data. It was, in fact, a fairly typical study for the field of diet research. Which is to say: It was terrible science. The results are meaningless, and the health claims that the media blasted out to millions of people around the world are utterly unfounded.<br></p></blockquote><p>This is what he says about how his collaborators recruited participants</p><blockquote><p>They used Facebook to recruit subjects around Frankfurt, offering 150 Euros to anyone willing to go on a diet for 3 weeks. They made it clear that this was part of a documentary film about dieting, but they didn’t give more detail.</p></blockquote><p>So, was this unethical, given that the participants got dietary advice and blood tests?</p><p><b>1. Did the participants give informed consent?</b></p><p>More or less, yes. They weren’t told the true purpose of the study, but I would argue they weren’t importantly misled -- as they would have been if told the aim was to assess chocolate as a weight-loss intervention.</p><p><b>2. Was this an intervention that it would be unreasonable to ask someone to consent to?</b></p><p>No. Short-term dietary changes and blood samples are low risk. Unless there’s some reason why consent might not be freely given, there’s no problem with asking people to give blood samples, fill in questionnaires, and change their diets for a short period.&nbsp;</p><p><b>3. Was there independent ethical review, in particular of whether participants could be misled as to the purpose?</b></p><p>Apparently not. That’s the problem.</p><p><br></p><p>&nbsp;In this case, the study risks were sufficiently small that I have no trouble saying participants could be asked to consent to them. I’m just about ok with the extent to which participants were misled about the true purpose of the study.&nbsp;</p><p>That doesn’t mean it was right for this group to make the ethical evaluation on their own. Independent ethical review for human experimentation is a compliance check-box activity, but that’s not <b>all</b> it is. There is depressing historical support for the principle that researchers can’t be trusted to do their own ethics evaluations, and that independent input is important.&nbsp;</p><p>There’s a good case for handling violation of these norms severely, and the fact that you intend your research to be useless probably shouldn’t be a defence.</p>]]></content:encoded>
    <wp:post_name>120095349481</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/119901462361</link>
    <pubDate>Tue, 26 May 2015 14:06:24 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/119901462361</guid>
    <!--<wp:post_id>119901462361</wp:post_id>-->
    <wp:post_date>2015-05-25 19:06:24</wp:post_date>
    <wp:post_date_gmt>2015-05-26 02:06:24</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Zero-inflated Poisson from complex samples</title>
    <description></description>
    <content:encoded><![CDATA[<p><i> A very long post about how to add models to the survey package; specifically, the zero-inflated Poisson. 
</i>
</p><p><br>The Zero-Inflated Poisson model is a model for count data with excess zeros. The response distribution is a mixture of a point mass at zero and a Poisson distribution: if $Z$ is Bernoulli with probability $1-p_0$ and $P$ is Poisson with mean $\lambda$ then<br>$$Y=Z+(1-Z)P$$<br>is zero-inflated Poisson. The ZIP is a latent-class model; we can have $Y=0$ either because $Z=0$ or because $P=0$. That's natural in some ecological examples: if you didn't see any moose it could be because the area is moose-free (it's downtown Montréal) or because you just randomly didn't see any. </p><p>There isn't existing software in R for design-based inference in zero-inflated Poisson models, so it's a good example for showing how to add models. There's another example in Appendix E of my book, but that's owned by Wiley. The `pscl` package (Zeileis et al) fits zero-inflated models, and they use an example counting medical visits, taken from the Journal of Applied Econometrics (Deb &amp; Trivedi, 1997). &nbsp;The data in that paper were actually from a complex survey, but the survey design was ignored in the analysis. &nbsp;</p><p>It's a bit tricky to get the full data they used, so I'll do an example with data on number of sexual partners, from NHANES 2003-2004. We will look at questions SXQ200 and SXQ100: number of male sexual partners. &nbsp;Combining these gives a 'real' zero-inflated variable: based on sexual orientation the zeroes would divide into "never" and "not yet".<br></p><pre>library(foreign)
library(survey)
setwd("nhanes")
demo = read.xport("demo_c.xpt")
sxq = read.xport("sxq_c.xpt")
merged = merge(demo, sxq, by='SEQN')
merged$total = with(merged, ifelse(RIAGENDR==2, SXQ100+SXQ130, SXQ170+SXQ200))
merged$total[merged$SXQ020==2] = 0
merged$total[merged$total&gt;2000] = NA
merged$age = merged$RIDAGEYR/25
merged$malepartners=with(merged, ifelse(RIAGENDR==2,SXQ100,SXQ200))
merged$malepartners[merged$malepartners&gt;200]=NA
merged$scaledwt=with(merged,WTINT2YR/mean(WTINT2YR))
des = svydesign(id=~SDMVPSU,strat=~SDMVSTRA,weights=~WTINT2YR, nest=TRUE, data=merged)
</pre><p>
First, look at an unweighted analysis, and at a weighted analysis using frequency weights (scaled to have mean 1 for numerical stability). There are warnings about non-integer numbers of successes; these are due to the weights; ignore them.</p><pre>library(pscl)
## Loading required package: MASS
## Loading required package: lattice
## Classes and Methods for R developed in the
## 
## Political Science Computational Laboratory
## 
## Department of Political Science
## 
## Stanford University
## 
## Simon Jackman
## 
## hurdle and zeroinfl functions by Achim Zeileis
unwt = zeroinfl(malepartners~RIDAGEYR+factor(RIDRETH1)+DMDEDUC|RIDAGEYR+factor(RIDRETH1)+DMDEDUC, data=merged)
summary(unwt)
## 
## Call:
## zeroinfl(formula = malepartners ~ RIDAGEYR + factor(RIDRETH1) + 
##     DMDEDUC | RIDAGEYR + factor(RIDRETH1) + DMDEDUC, data = merged)
## 
## Pearson residuals:
##    Min     1Q Median     3Q    Max 
## -1.020 -0.943 -0.787  0.150 29.257 
## 
## Count model coefficients (poisson with log link):
##                    Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)        1.666622   0.050666   32.89  &lt; 2e-16 ***
## RIDAGEYR          -0.005510   0.000897   -6.14  8.1e-10 ***
## factor(RIDRETH1)2 -0.039402   0.077948   -0.51     0.61    
## factor(RIDRETH1)3  0.650882   0.034573   18.83  &lt; 2e-16 ***
## factor(RIDRETH1)4  0.667532   0.036596   18.24  &lt; 2e-16 ***
## factor(RIDRETH1)5  0.564296   0.059493    9.49  &lt; 2e-16 ***
## DMDEDUC            0.009426   0.013518    0.70     0.49    
## 
## Zero-inflation model coefficients (binomial with logit link):
##                   Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept)        0.18813    0.18708    1.01   0.3146   
## RIDAGEYR          -0.00294    0.00363   -0.81   0.4182   
## factor(RIDRETH1)2 -0.07964    0.24231   -0.33   0.7424   
## factor(RIDRETH1)3  0.11837    0.11612    1.02   0.3080   
## factor(RIDRETH1)4  0.14330    0.12776    1.12   0.2620   
## factor(RIDRETH1)5  0.25952    0.22303    1.16   0.2446   
## DMDEDUC           -0.14888    0.05334   -2.79   0.0052 **
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
## 
## Number of iterations in BFGS optimization: 21 
## Log-likelihood: -9.52e+03 on 14 Df
wt= zeroinfl(malepartners~RIDAGEYR+factor(RIDRETH1)+DMDEDUC|RIDAGEYR+factor(RIDRETH1)+DMDEDUC, data=merged, weights=scaledwt)
summary(wt)
## 
## Call:
## zeroinfl(formula = malepartners ~ RIDAGEYR + factor(RIDRETH1) + 
##     DMDEDUC | RIDAGEYR + factor(RIDRETH1) + DMDEDUC, data = merged, 
##     weights = scaledwt)
## 
## Pearson residuals:
##    Min     1Q Median     3Q    Max 
## -1.586 -0.842 -0.543  0.132 31.911 
## 
## Count model coefficients (poisson with log link):
##                    Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)        1.834067   0.061499   29.82  &lt; 2e-16 ***
## RIDAGEYR          -0.007388   0.000906   -8.16  3.5e-16 ***
## factor(RIDRETH1)2 -0.107324   0.085353   -1.26    0.209    
## factor(RIDRETH1)3  0.655138   0.048168   13.60  &lt; 2e-16 ***
## factor(RIDRETH1)4  0.635817   0.052917   12.02  &lt; 2e-16 ***
## factor(RIDRETH1)5  0.477415   0.066661    7.16  8.0e-13 ***
## DMDEDUC           -0.023739   0.014307   -1.66    0.097 .  
## 
## Zero-inflation model coefficients (binomial with logit link):
##                   Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)        0.66051    0.21446    3.08  0.00207 ** 
## RIDAGEYR          -0.00783    0.00367   -2.13  0.03296 *  
## factor(RIDRETH1)2 -0.11680    0.25245   -0.46  0.64361    
## factor(RIDRETH1)3  0.10197    0.15153    0.67  0.50100    
## factor(RIDRETH1)4 -0.16081    0.18143   -0.89  0.37543    
## factor(RIDRETH1)5  0.10678    0.23034    0.46  0.64296    
## DMDEDUC           -0.20240    0.05762   -3.51  0.00044 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
## 
## Number of iterations in BFGS optimization: 20 
## Log-likelihood: -9.77e+03 on 14 Df
</pre><p>

The zeroinfl function takes weights, so it can be fed to withReplicates if we have replicate weights. Typically you’d just use the default options to as.svrepdesign(), but in this example the model fitting is a bit sensitive. We go for Fay’s method, which is only available for 2 PSU/stratum designs but has the virtue of having non-zero weight for all units in all replicates.

</p><pre>repdes = as.svrepdesign(des,type="Fay",fay.rho=0.2)
rep1 = withReplicates(repdes, quote( 
    coef(zeroinfl(malepartners~RIDAGEYR+factor(RIDRETH1)+DMDEDUC|RIDAGEYR+factor(RIDRETH1)+DMDEDUC, weights=.weights))
    ))
rep1
##                            theta   SE
## count_(Intercept)        1.83407 0.14
## count_RIDAGEYR          -0.00739 0.00
## count_factor(RIDRETH1)2 -0.10733 0.25
## count_factor(RIDRETH1)3  0.65514 0.19
## count_factor(RIDRETH1)4  0.63581 0.14
## count_factor(RIDRETH1)5  0.47741 0.25
## count_DMDEDUC           -0.02374 0.08
## zero_(Intercept)         0.66050 0.26
## zero_RIDAGEYR           -0.00783 0.00
## zero_factor(RIDRETH1)2  -0.11679 0.29
## zero_factor(RIDRETH1)3   0.10197 0.10
## zero_factor(RIDRETH1)4  -0.16080 0.09
## zero_factor(RIDRETH1)5   0.10678 0.21
## zero_DMDEDUC            -0.20240 0.06
</pre><p>

If what we care about is the mean we can just model the mean directly.

</p><pre>summary(svyglm(malepartners~RIDAGEYR+factor(RIDRETH1)+DMDEDUC,design=des,family=quasipoisson))
## 
## Call:
## svyglm(formula = malepartners ~ RIDAGEYR + factor(RIDRETH1) + 
##     DMDEDUC, design = des, family = quasipoisson)
## 
## Survey design:
## svydesign(id = ~SDMVPSU, strat = ~SDMVSTRA, weights = ~WTINT2YR, 
##     nest = TRUE, data = merged)
## 
## Coefficients:
##                   Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)        0.83012    0.19464    4.26  0.00210 ** 
## RIDAGEYR          -0.00370    0.00232   -1.60  0.14472    
## factor(RIDRETH1)2 -0.06209    0.21242   -0.29  0.77667    
## factor(RIDRETH1)3  0.61196    0.17245    3.55  0.00623 ** 
## factor(RIDRETH1)4  0.71514    0.12358    5.79  0.00026 ***
## factor(RIDRETH1)5  0.42544    0.22984    1.85  0.09720 .  
## DMDEDUC            0.06919    0.09980    0.69  0.50563    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for quasipoisson family taken to be 18.25)
## 
## Number of Fisher Scoring iterations: 6
</pre><p>

Or we can use observed zeroes rather than the latent class to split up the data

</p><pre>summary(svyglm(malepartners~RIDAGEYR+factor(RIDRETH1)+DMDEDUC,design=subset(des,malepartners&gt;0),family=quasipoisson))
## 
## Call:
## svyglm(formula = malepartners ~ RIDAGEYR + factor(RIDRETH1) + 
##     DMDEDUC, design = subset(des, malepartners &gt; 0), family = quasipoisson)
## 
## Survey design:
## subset(des, malepartners &gt; 0)
## 
## Coefficients:
##                   Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)        1.84314    0.13812   13.34  3.1e-07 ***
## RIDAGEYR          -0.00735    0.00275   -2.68  0.02538 *  
## factor(RIDRETH1)2 -0.09993    0.21956   -0.46  0.65980    
## factor(RIDRETH1)3  0.64483    0.17210    3.75  0.00458 ** 
## factor(RIDRETH1)4  0.62560    0.12488    5.01  0.00073 ***
## factor(RIDRETH1)5  0.46776    0.22388    2.09  0.06625 .  
## DMDEDUC           -0.02375    0.07981   -0.30  0.77276    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for quasipoisson family taken to be 14.2)
## 
## Number of Fisher Scoring iterations: 5
summary(svyglm(I(malepartners==0)~RIDAGEYR+factor(RIDRETH1)+DMDEDUC,design=des,family=quasibinomial))
## 
## Call:
## svyglm(formula = I(malepartners == 0) ~ RIDAGEYR + factor(RIDRETH1) + 
##     DMDEDUC, design = des, family = quasibinomial)
## 
## Survey design:
## svydesign(id = ~SDMVPSU, strat = ~SDMVSTRA, weights = ~WTINT2YR, 
##     nest = TRUE, data = merged)
## 
## Coefficients:
##                   Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept)        0.67494    0.24722    2.73    0.023 * 
## RIDAGEYR          -0.00769    0.00392   -1.96    0.081 . 
## factor(RIDRETH1)2 -0.09831    0.26159   -0.38    0.716   
## factor(RIDRETH1)3  0.07980    0.09234    0.86    0.410   
## factor(RIDRETH1)4 -0.18227    0.08226   -2.22    0.054 . 
## factor(RIDRETH1)5  0.08620    0.20444    0.42    0.683   
## DMDEDUC           -0.20146    0.05631   -3.58    0.006 **
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for quasibinomial family taken to be 1.015)
## 
## Number of Fisher Scoring iterations: 4
</pre><p>

Or, finally, we can maximise the pseudolikelihood (population likelihood, weighted likelihood) directly, using svymle()


The likelihood is $$f(y;p,\mu)=p\{y=0\}+(1-p)\frac{e^{-\mu}\mu^y}{y!}.$$
We're going to work with models for $\mathrm{logit} p$ and $\eta=\log mu$

</p><pre>loglike = function(y,eta,logitp){
	mu = exp(eta)
	p = exp(logitp)/(1+exp(logitp))
	log(p*(y==0)+(1-p)*dpois(y,mu))
	}
</pre><p>

We also need the derivatives with respect to logitp and eta

</p><pre>dlogitp = function(y,eta,logitp){
	mu = exp(eta)
	p = exp(logitp)/(1+exp(logitp))
	dexpit = p/(1+p)^2
	num = dexpit*(y==0)-dexpit*dpois(y,mu)
	denom = p*(y==0)+(1-p)*dpois(y,mu)
	num/denom
	}	
	
deta = function(y,eta,logitp){
	mu = exp(eta)
	p = exp(logitp)/(1+exp(logitp))
	dmutoy = 0*y
	dmutoy[y&gt;0] = exp(-mu[y&gt;0])*mu[y&gt;0]^(y[y&gt;0]-1)/factorial(y[y&gt;0]-1)
	num = (1-p)*(-dpois(y,mu)+dmutoy)
	denom = p*(y==0)+(1-p)*dpois(y,mu)
	num/denom
	}	

score = function(y,eta,logitp) cbind(deta(y,eta,logitp), dlogitp(y,eta,logitp))
</pre><p>


Now, fit using svymle.  We give a list of model formulas, with one for each parameter of the loglikelihood. If you just want to fit a scalar parameter used ~1 as the formula.  The outcome variable malepartners should be supplied as the lefthand-side of one of the formulas.</p><p>

We also need a set of starting values; a good one is the weighted fit from zeroinfl.

</p><pre>nlmfit = svymle(loglike=loglike, grad=score, design=des, 
        formulas=list(eta=malepartners~RIDAGEYR + factor(RIDRETH1) + DMDEDUC, 
        logitp=~RIDAGEYR + factor(RIDRETH1) + DMDEDUC),
      start=coef(wt), na.action="na.omit")

summary(nlmfit)

## Survey-sampled mle: 
## svymle(loglike = loglike, gradient = score, design = des, formulas = list(eta = malepartners ~ 
##     RIDAGEYR + factor(RIDRETH1) + DMDEDUC, logitp = ~RIDAGEYR + 
##     factor(RIDRETH1) + DMDEDUC), start = coef(wt), na.action = "na.omit")
##                               Coef        SE p.value
## eta.(Intercept)           1.834067 0.0221688 &lt; 0.001
## eta.RIDAGEYR             -0.007388 0.0003664 &lt; 0.001
## eta.factor(RIDRETH1)2    -0.107324 0.0583560 0.06590
## eta.factor(RIDRETH1)3     0.655138 0.0312890 &lt; 0.001
## eta.factor(RIDRETH1)4     0.635817 0.0251859 &lt; 0.001
## eta.factor(RIDRETH1)5     0.477415 0.0351678 &lt; 0.001
## eta.DMDEDUC              -0.023739 0.0100524 0.01820
## logitp.(Intercept)        0.660511 0.2231099 0.00307
## logitp.RIDAGEYR          -0.007833 0.0034823 0.02448
## logitp.factor(RIDRETH1)2 -0.116798 0.2403620 0.62702
## logitp.factor(RIDRETH1)3  0.101968 0.0844237 0.22712
## logitp.factor(RIDRETH1)4 -0.160809 0.0741717 0.03015
## logitp.factor(RIDRETH1)5  0.106776 0.1834985 0.56064
## logitp.DMDEDUC           -0.202397 0.0505458 &lt; 0.001
## Stratified 1 - level Cluster Sampling design (with replacement)
## With (30) clusters.
## svydesign(id = ~SDMVPSU, strat = ~SDMVSTRA, weights = ~WTINT2YR,
##     nest = TRUE, data = merged)
</pre><p>

The replicate-weight and svymle approaches agree well, and they give the same point estimates as the frequency-weighted analysis, but different standard errors -- especially for the zero-inflation part of the model.</p><p>

Ignoring the weights does change the point estimates. In this particular example, nearly all the zeroes are 'inflated' zeroes, not Poisson zeroes, so a two-part model with two svyglm fits is also pretty comparable. </p><p>

<b>References</b></p><p>


Partha Deb and Pravin K. Trivedi, “Demand for Medical Care by the Elderly: A Finite Mixture Approach”, Journal of Applied Econometrics, Vol. 12, No. 3, 1997, pp. 313-336.&nbsp;</p><p>&nbsp;Zeileis A, Kleiber C, Jackman S. “Regression Models for Count Data in R”. <a href="http://cran.r-project.org/web/packages/pscl/index.html">Vignette for the pscl package</a>, version 1.4.9</p>]]></content:encoded>
    <wp:post_name>119901462361</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/119430684781</link>
    <pubDate>Wed, 20 May 2015 20:52:46 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/119430684781</guid>
    <!--<wp:post_id>119430684781</wp:post_id>-->
    <wp:post_date>2015-05-20 1:52:46</wp:post_date>
    <wp:post_date_gmt>2015-05-20 08:52:46</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Call me, Ishmael</title>
    <description></description>
    <content:encoded><![CDATA[<p>Making small changes in text to escape plagiarism-detection software is challenging but not really difficult. The relationship between your text and the urtext in the software is precise and syntactic; the software doesn’t know about ideas. Making small changes in a text that change or obscure the meaning is also easy, and is why copyeditors exist. Making small changes in a text that give an interesting new meaning is enormously harder, as in the opening of Peter de Vries’ “The Vale of Laughter”, quoted as the title of this post.</p><p>A similar distinction connects two recent science stories. &nbsp;A Berkeley-Montreal <a href="http://www.nature.com/news/drugs-regulate-home-brew-opiates-1.17563">biochemistry collaboration</a>&nbsp; has made large steps towards GMO yeast that produces morphine. A US-German collaboration discovered a <a href="http://sciblogs.co.nz/infectious-thoughts/2015/01/08/will-new-antibiotic-teixobactin-save-us-all-umm-not-quite/">new candidate antibiotic, teixobactin</a>, and claimed it was resistant to antibiotic resistance.</p><p>Engineering yeast to produce morphine sounds like it should be easy. After all, morphine is a hugely simpler molecule than insulin, and recombinant insulin is older than <a href="http://en.wikipedia.org/wiki/Y.M.C.A._(song)">“Y.M.C.A”</a>&nbsp;or Space Invaders. &nbsp;The distinction is that insulin is a protein, and morphine isn’t. </p><p>The Central Dogma of Molecular Biology is that DNA is transcribed to RNA and translated to protein, and that (for protein-coding parts of the genome) this is a precise one-to-one copying process. Like all dogmas, this isn’t quite true — splicing matters, as does all the bling attached after translation (formally, post-translational modification). It’s still a good approximation. Making a protein such as insulin is almost as simple as sticking the insulin gene into a cell and running the transcription/translation program on it. </p><p>Morphine isn’t a protein. Proteins (enzymes) are the machinery that manufacture morphine. Getting a yeast cell to produce morphine means getting it to produce a whole series of enzymes, but that’s not enough. In order to produce morphine, this set of machinery must get assembled to a properly connected production line that takes in the right raw materials and spits out morphine at the end. That’s not easy.</p><p>Most targets of antibiotics are protein or RNA. Penicillins, cephalosporins, and carbapenems target the penicillin-binding proteins that construct the bacterial cell wall. Macrolides, lincosamides, streptogramins, and tetracyclines attack the RNA/protein machinery of the ribosome. Fluoroquinolines such as Cipro block the protein DNA gyrase. Enzymes make good drug targets because the cell produces only small amounts; you don’t need much drug. &nbsp;Teixobactin is an exception, as are the current last-resort antibiotic vancomycin and teicoplanin. These block the construction of cell walls as penicillin does, but bind to non-protein construction materials rather than protein machinery. &nbsp;</p><p>To evolve resistance to antibiotics with protein targets, bacteria need to make lots of random changes to the proteins until they find a version that does pretty much the same job but doesn’t bind to the antibiotic. That was hard, but there are a lot of bacteria and they reproduce very fast, so we got MRSA. </p><p><a href="http://en.wikipedia.org/wiki/Vancomycin#Antibiotic_resistance">Resistance to vancomycin</a>&nbsp;took a lot longer to develop. A because a vancomycin-resistant germ has to find mutated versions of enzymes that do a <b>different job</b>. Vancomycin-resistant enterococci make a new enzyme that strips off one end of the target construction material and replaces it with a new piece that still works for cell wall construction but won’t bind to the antibiotic. </p><p>Proteins are incredibly complicated molecules. Morphine and vancomycin-resistance are much simpler, but they are new ideas for the cell rather than just edits to the same text.&nbsp;<br></p>]]></content:encoded>
    <wp:post_name>119430684781</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/118767311371</link>
    <pubDate>Tue, 12 May 2015 20:22:06 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/118767311371</guid>
    <!--<wp:post_id>118767311371</wp:post_id>-->
    <wp:post_date>2015-05-12 1:22:06</wp:post_date>
    <wp:post_date_gmt>2015-05-12 08:22:06</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Superefficiency</title>
    <description></description>
    <content:encoded><![CDATA[<p>If you have $X_1,\ldots,X_n$ independent from an $N(\mu,1)$ distribution you don’t have to think too hard to work out that $\bar X_n$, the sample mean, is the right estimator of $\mu$ (unless you have quite detailed prior knowledge). As people who have taken an advanced course in mathematical statistics will know, there is a famous estimator that appears to do better.&nbsp;</p><p>Hodges’ estimator is given by $H_n=\bar X_n$ if $|\bar X_n|&gt;n^{-1/4}$, and $H_n=0$ if&nbsp;$|\bar X_n|\leq n^{-1/4}$. If $\mu\neq 0$, $H_n=\bar X_n$ for all large enough $n$, so $$\sqrt{n}(H_n-\mu)\stackrel{d}{\to}N(0,1)$$ just as for $\bar X_n$. On the other hand, if $\mu=0$,&nbsp;$$\sqrt{n}(H_n-\mu)\stackrel{p}{\to}0.$$ $H_n$ is asymptotically better than $\bar X_n$ for $\mu=0$ and asymptotically as good for any other value of $\mu$. Of course there’s something wrong with it: it sucks for $n^{-1/2}\ll\mu&lt;n^{-1/4}$. Here’s its mean square error:</p><figure data-orig-width="2075" data-orig-height="1632" class="tmblr-full"><img src="https://36.media.tumblr.com/e69fbc5fff24de7c6220af6a7f961db4/tumblr_inline_no7zhmSJD41s1hdxy_540.png" alt="image" data-orig-width="2075" data-orig-height="1632"></figure><p>Even <a href="http://en.wikipedia.org/wiki/Hodges%27_estimator">Wikipedia</a> knows this much. What I recently got around to doing was extending this to an estimator that’s asymptotically superior to $\bar X_n$ on a dense set. This isn’t new -- Le Cam did it in his PhD thesis. It may even be the same as Le Cam’s construction <strike>(which isn’t online, as far as I can tell)</strike>. [Actually, Le Cam’s construction is a draft exercise in a draft <a href="http://www.stat.yale.edu/~pollard/Courses/618.fall2010/Handouts/Heuristics.pdf">chapter</a> for David Pollard’s long-awaited ‘Asymptopia’. And it <b>is</b> basically my one, so it’s quite likely that as a Pollard fan I got at least the idea from there.]</p><p>First, instead of just setting the estimate to zero when it’s close enough to zero, we can set it to the nearest integer when it’s close enough to an integer. &nbsp;Define $\tilde H_n=i$ if $|\bar X_n-i|&lt;0.5n^{-1/4}$, with $\tilde H_n=\bar X_n$ otherwise.</p><p>If $n$ is large enough, we can shrink to multiples of 1/2. For example, using the same threshold for closeness, if $n&gt;16$ there is at most one multiple of 1/2 within $0.5n^{-1/4}$. If $n&gt;256$ there is at most one multiple of 1/4 within that range.</p><p>Define $H_{n,k}=2^{-k}i$ if $|x-2^{-k}i|&lt; 0.5n^{-1/4}$ and $H_{n,k}=\bar X_n$ otherwise. This is well-defined if $n&gt;2^{4k}$. For any fixed $k$, $\tilde H_{n,k}$ satisfies $$\sqrt{n}(H_n-\mu)\stackrel{p}{\to}0$$ if $\mu$ is a multiple of $2^{-k}$ and&nbsp;$$\sqrt{n}(H_n-\mu)\stackrel{d}{\to}N(0,1)$$ otherwise.</p><p>The obvious thing to do now is to let $k$ increase slowly with $n$. This doesn’t work. Consider a value for $\mu$ whose binary expansion has infinitely many 1s, but with increasingly many zeroes between them. Whatever your rule for $k(n)$ there will be values of this type that are close enough to multiples of $2^{-k(n)}$ to get pulled to the wrong value infinitely often as $n$ increases. $H_{n,k(n)}$ will be asymptotically superior to $\bar X_n$ on a dense set, but it will be asymptotically inferior on another dense set, violating the rules of the game.&nbsp;</p><p>What we can do is pick $k$ at random. The efficiency gain isn’t 100% as it was for fixed $k$, but it’s still there.&nbsp;</p><p>Let $K$ be a random variable with probability mass function $p(k)$, independent of the $X$s. &nbsp;The distribution of $H_{n,K}$ conditional on $K=k$ is the distribution of $H_{n,k}$. If $p(k)&gt;0$ for all $k$, the probability &nbsp;of seeing $K=k$ infinitely often is 1, so we can look the limiting distribution of $\sqrt{n}(H_{n,K}-\mu)$ along subsequences with $K=k$. This limiting distribution is a point mass at zero &nbsp;if $2^k\mu$ is an integer, and $N(0,1)$ otherwise. So, $$\sqrt{n}(H_{n,K}-\mu)\stackrel{d}{\to}q_k\delta_0+(1-q_k)N(0,1)$$ where $$q_k=\sum_k p_k I(2^k\mu\textrm{ is an integer})$$</p><p>For a dense set of real numbers, and in particular for all numbers representable in binary floating point, $H_{n,K}$ has greater asymptotic efficiency than the efficient estimator $\bar X_n$. &nbsp;The disadvantage of this randomised construction is that working out the finite-sample MSE is just horrible to think about.</p><p>The other interesting thing to think about is why the&nbsp;‘overflow’ heuristic doesn’t work. Why doesn’t superefficiency for all fixed $k$ translate into superefficiency for sufficiently-slowly increasing $k(n)$? As a heuristic, this sort of thing has been around since the early days of analysis, but it’s more than that: the field of non-standard analysis is basically about making it rigorous.&nbsp;</p><p>My guess is that $H_{n,k}$ for infinite $n$ is close to the superefficient distribution on the dense set only for&nbsp;‘large enough’ infinite $k$, and close to $N(0,1)$ off the dense set only for&nbsp;‘small enough’ infinite $k$. The failure of the heuristic is similar to the failure in Cauchy’s invalid proof that a convergent sequence of continuous functinons has a continuous limit, the proof into which later analysis retconned the concepts of&nbsp;‘uniform convergence’ and&nbsp;‘equicontinuity’.</p>]]></content:encoded>
    <wp:post_name>118767311371</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/118102423391</link>
    <pubDate>Mon, 04 May 2015 21:09:02 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/118102423391</guid>
    <!--<wp:post_id>118102423391</wp:post_id>-->
    <wp:post_date>2015-05-04 2:09:02</wp:post_date>
    <wp:post_date_gmt>2015-05-04 09:09:02</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Precise answers, but not necessarily to the right question</title>
    <description></description>
    <content:encoded><![CDATA[<p>Nicholas Schork has a <a href="http://www.nature.com/news/personalized-medicine-time-for-one-person-trials-1.17411">commentary at Nature</a> about precision medicine, arguing in favour of n-of-1 trials. These are the extreme version of crossover trials: you randomise each individual to a long sequence of periods on each of two treatments and see which they do better on.&nbsp;</p><p>The idea makes sense: you get genuinely individual-specific results for people in the study, and the ability to aggregate them to generalise to people not in the study. &nbsp;In situations where the main cost of a trial is recruitment rather than follow-up, it might not even be unreasonably expensive.&nbsp;</p><p>However, some of the examples are seriously misleading. He talks about how drugs don’t help everyone, &nbsp;</p><blockquote>For some drugs, such as statins — routinely used to lower cholesterol — as few as 1 in 50 may benefit<br></blockquote><p>To get low numbers like those you need to define&nbsp;‘benefit’ to mean&nbsp;‘avoid a serious clinical event such as myocardial infarction or death over 5-10 years’. That’s not a definition that makes sense in an n-of-1 trial. You can’t randomise one person to five-year periods of statin or no-statin (with washout intervals in between) and tally up the number of deaths in each arm. Deaths are one to a customer.</p><p>What you can do with a statin in an n-of-1 trial is see if it reduces LDL cholesterol. The fall in cholesterol only takes a matter of weeks and is quickly reversible. If you define benefit that way, you will find that everyone benefits.</p><p>In the same way, if you define benefit from aspirin as prevention of a composite of heart attack, stroke, and death, it’s true that a minority of people benefit, but it’s not possible to do an n-of-1 trial. If you define benefit as reduced platelet aggregation you can do an n-of-1 trial, but you find that most everyone benefits.</p><p>There are too many settings where n-of-1 trials force you to use surrogate outcomes -- often ones where we know the surrogate is not up to the task of capturing between-medication differences, such as blood pressure or LDL cholesterol.&nbsp;</p><p>Where n-of-1 trials are really useful is looking at reversible symptoms: either because the drug is supposed to control them (as in asthma or heartburn) or because you want to avoid causing them (as with antidepressants). This design should have been used more even before genomics made the subgroup problem completely intractable, but it’s not a panacea.&nbsp;</p>]]></content:encoded>
    <wp:post_name>118102423391</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/118005229311</link>
    <pubDate>Sun, 03 May 2015 19:13:36 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/118005229311</guid>
    <!--<wp:post_id>118005229311</wp:post_id>-->
    <wp:post_date>2015-05-03 0:13:36</wp:post_date>
    <wp:post_date_gmt>2015-05-03 07:13:36</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>What’s the right proof of the Continuous Mapping Theorem?</title>
    <description></description>
    <content:encoded><![CDATA[<p>The Continuous Mapping Theorem says that if $X_n\stackrel{d}{\to}X$ and $f$ is continuous except at a set of points with zero probability under $X$, that $f(X_n)\stackrel{d}{\to}f(X)$. &nbsp;As David Pollard points out, it should be called the almost-everywhere-continuous mapping theorem, because the ability to have discontinuities is important in applications and is the only thing making the proof non-trivial.&nbsp;</p><p>There are three proofs that I’m aware of</p><ol><li>Mann and Wald used the&nbsp;‘pointwise convergence of cdfs’ definition of convergence in distribution, which gives a painful proof<br></li><li>The modern standard is to use the definition of convergence in distribution in terms of expectations of bounded continuous functions. You then rewrite the integral in terms of horizontal rather than vertical slices and use the Portmanteau Lemma<i>*</i> to show the discontinuities don’t matter.</li><li>The easy way: use almost-sure representations to construct a sequence $\tilde{X}_n \stackrel{a.s.}{\to} &nbsp;\tilde{X}$ with the same distributions.</li></ol><p>Mathematically, approach 3 looks like overkill. &nbsp;Certainly, if you had to prove the representation theorem (beyond the univariate real case) it would take longer than just using approach 2, even if you had to do it barehanded without the Portmanteau Lemma.&nbsp;<br></p><p>From the statistics point of view I’m still attracted to approach 3. A lot of the time I’m happy to treat advanced probability theory as a black box and just use it to call in air strikes on obstacles in the proof. For example, I do have some clue how the bracketing-entropy Donsker theorem works because I learnt it from Jon Wellner, but the proof isn’t something I’d ever want to teach. The result, on the other hand, is quite useful.&nbsp;</p><p>As another example, I recently cited a result on the non-existence of uncountable ascending chains in a Borel preorder. My understanding of the proof is at the level&nbsp;“well, Borel, so some kind of countable transfinite induction thing, right?”,&nbsp;but since lots of actual mathematicians have cited the paper, I trust someone’s checked the details.&nbsp;</p><p>On the other hand, if we aren’t making the students prove the Continuous Mapping Theorem from scratch, why are we making them prove it at all? Why not just prove the continuous case, which follows trivially from the definition of weak convergence, and assert that it also works for almost-surely-continuous functions?</p><p>To some extent, my justification for approach 3 is that I <b>want</b> to introduce almost-sure representations. The Skorohod univariate proof is very easy to understand in terms of computational random number generation: Skorohod’s construction is that you use the same uniform random numbers to generate each element of the sequence. That computational heuristic makes it plausible that you should be able to do the same thing more generally (and <a href="http://www-users.mat.uni.torun.pl/~adjakubo/SkorRepr.pdf">Jakubowski</a> notes that Skorohod’s construction still can be used to prove a slightly weaker result for separable metric spaces). I think proving the Skorohod representation theorem gives useful insight into how convergence in distribution is different from the other modes.</p><p>So, approach 2 is probably best in isolation, but in context I still like approach 3.</p><p><br></p><p><i>* which a surprisingly large number of internet posters seem to think takes an apostrophe-s attributing it to an eponymous presumably-French probabilist.&nbsp;</i></p>]]></content:encoded>
    <wp:post_name>118005229311</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/117278578404</link>
    <pubDate>Sat, 25 Apr 2015 09:01:12 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/117278578404</guid>
    <!--<wp:post_id>117278578404</wp:post_id>-->
    <wp:post_date>2015-04-24 14:01:12</wp:post_date>
    <wp:post_date_gmt>2015-04-24 21:01:12</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title></title>
    <description></description>
    <content:encoded><![CDATA[<figure data-orig-width="424" data-orig-height="640" class="tmblr-full"><img src="https://40.media.tumblr.com/679a3ae1c18e52cbb9f91aa41eaacfa0/tumblr_inline_nn75kgUPOr1s1hdxy_540.jpg" alt="image" data-orig-width="424" data-orig-height="640"></figure><p>Rosmarinus officinalis&nbsp;‘Gallipoli’ (<a href="https://www.flickr.com/photos/dramagirl/3471337599/in/photostream/">photo credit</a>)</p><p><br></p><p><br></p><blockquote>Because the Great War was a mad, brutal, awful struggle, distinguished more often than not by military and political incompetence; because the waste of human life was so terrible that some said victory was scarcely discernible from defeat; and because the war which was supposed to end all wars in fact sowed the seeds of a second, even more terrible, war -- &nbsp;we might think this Unknown Soldier died in vain.<br><br>But, in honouring our war dead, as we always have and as we do today, we declare that this is not true. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<a href="https://www.awm.gov.au/commemoration/keating.asp">Paul Keating, 1993</a></blockquote>]]></content:encoded>
    <wp:post_name>117278578404</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/117023886256</link>
    <pubDate>Wed, 22 Apr 2015 08:29:30 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/117023886256</guid>
    <!--<wp:post_id>117023886256</wp:post_id>-->
    <wp:post_date>2015-04-21 13:29:30</wp:post_date>
    <wp:post_date_gmt>2015-04-21 20:29:30</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>We’re recruiting</title>
    <description></description>
    <content:encoded><![CDATA[<p>The formal ad is <a href="https://www.opportunities.auckland.ac.nz/psp/ps/EMPLOYEE/HRMS/c/HRS_HRAM.HRS_CE.GBL?Page=HRS_CE_JOB_DTL&amp;Action=A&amp;JobOpeningId=17136&amp;SiteId=1&amp;PostingSeq=1">here</a></p><blockquote>The <a href="https://www.stat.auckland.ac.nz/en.html">Department of Statistics</a> invites applications for a position as Lecturer in Data Science. Applicants should have or be about to complete a PhD in Statistics, or other relevant discipline, pursuing research in inference or learning methods that involve computation on large-scale and complex data. Applicants from other branches of statistics with a strong interest in data science may also be considered. &nbsp;A record of successful consulting or collaboration with applied researchers would be advantageous.</blockquote><p>‘Lecturer’ is the entry-level academic position, so this is similar to a hard-money tenure-track Assistant Professor position in the US.&nbsp;</p><p>The department has been relatively computation-friendly for a long time: as every school child knows, R was initially developed at Auckland, and both Ross Ihaka and Paul Murrell have been promoted to the top standard academic grade (Associate Professor). Other academic staff with relevant research interests include James Curran (forensics), Yong Wang (optimisation), Thomas Yee (data mining, parametric modelling) and me.&nbsp;</p><p>We teach a computing course at 2nd and 3rd year undergraduate, and several related MSc/Honours courses, and there’s a Masters program in Data Science joint with Computer Science in addition to the MSc in Statistics and in Medical Statistics.&nbsp;</p><p>Other people in the department with blogs include <a href="https://plausibilitytheory.wordpress.com/">Brendon Brewer</a>, <a href="http://voices.nationalgeographic.com/author/jrussell/">James Russell</a>,&nbsp;&nbsp;and <a href="http://jamescurran.co.nz/">James Curran</a>.&nbsp;</p><p>Yes, there are women in the department, even our <a href="https://www.stat.auckland.ac.nz/people/izie001">Head of Department</a>, but we do mean it when we encourage applications from under-represented groups.&nbsp;</p>]]></content:encoded>
    <wp:post_name>117023886256</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/115285295826</link>
    <pubDate>Thu, 02 Apr 2015 22:38:30 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/115285295826</guid>
    <!--<wp:post_id>115285295826</wp:post_id>-->
    <wp:post_date>2015-04-02 2:38:30</wp:post_date>
    <wp:post_date_gmt>2015-04-02 09:38:30</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Eppur si muove</title>
    <description></description>
    <content:encoded><![CDATA[<blockquote>“The rules for winning the science competition focus on a small number of measures that incentivize poor practice” Hilda Bastian quoting&nbsp;Ottoline Leyser.</blockquote><p>It’s all true, and more and worse besides. Researchers are driven by the incentives for high-impact publication; p-value hacking makes results seem more convincing than they are; trials use surrogate outcomes; glamour journals publish insufficiently-checked linkbait; predatory online journals will do anything for money; change and decay in all around we see.</p><p>And yet it moves.&nbsp;</p><p>Science has always been an activity carried out by selfish, egotistical humans. In the 1990s, Gallo and Montagnier fought over credit for discovering the viral that causes AIDS. In the 1980s, thousands of patients died because the Class I antiarrhythmics were approved based on a surrogate outcome. In the 1950s, understanding of the structure of DNA was delayed by the competition between Pauling in the US and Crick and Watson in England (not to mention the treatment of Rosalind Franklin). &nbsp;In earlier generations every North America species of grass received multiple scientific names, and in the `<a href="http://en.wikipedia.org/wiki/Bone_Wars">Bone Wars</a>’ of the late 19th century</p><blockquote>The efforts of the two men led to over 142 new species of dinosaurs being discovered and described, though today only 32 are valid.&nbsp;<br></blockquote><p>Even Mendel probably tidied up his results in ways we should disapprove of.&nbsp;</p><p>And yet it moves.</p><p>We want to crack down on predatory journals and p-value inflation. We want publication based on study design and execution, not on results. We want reproducibility and &nbsp;pre-registration. We should demand adequate power for real clinical outcomes in randomised trials. We still need to remember that there was no Golden Age.&nbsp;</p><p>Compared to a Platonic ideal, scientific practice has always sucked. It’s important to recognise and try to fix the faults of modern-day science, but we can’t forget that the progress of past centuries happened in spite of similar faults.</p><p>Imperfect scientific practice discovered wonderful things in the past, and is discovering wonderful things today. &nbsp;Science is held back in many ways, and yet it moves.&nbsp;</p>]]></content:encoded>
    <wp:post_name>115285295826</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/115011616346</link>
    <pubDate>Mon, 30 Mar 2015 19:39:11 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/115011616346</guid>
    <!--<wp:post_id>115011616346</wp:post_id>-->
    <wp:post_date>2015-03-29 23:39:11</wp:post_date>
    <wp:post_date_gmt>2015-03-30 06:39:11</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Pharmacy ethics</title>
    <description></description>
    <content:encoded><![CDATA[<blockquote>‘You have heard that it was said to those of ancient times, “You shall not murder”; and “whoever murders shall be liable to judgement.” &nbsp;But I say to you that if you are angry with a brother or sister,&nbsp;you will be liable to judgement. &nbsp;Matthew 5:21-22<br></blockquote><p>In practice, we have to distinguish. Whoever murders is liable to judgement, but being angry isn’t enough. In the same way, formal codes of professional ethics come in two versions: the aspirational code that describes the way we want the profession to be, and the legalistic code that describes what will get you kicked out. &nbsp;In medical research, the <a href="http://www.hhs.gov/ohrp/archive/nurcode.html">Nuremberg Code</a> is a bare minimum, but the <a href="http://www.wma.net/en/30publications/10policies/b3/">Declaration of Helsinki</a> goes well beyond current practice (notably on registration and publication of trials and on use of untested treatments).&nbsp;</p><p>The NZ Pharmacy Council code of ethics is supposed to be of the legalistic kind, actually binding on members, but it <a href="http://www.pharmacycouncil.org.nz/cms_show_download.php?id=200">says</a></p><blockquote>6.9 Only purchase, supply or promote any medicine, complementary therapy, herbal remedy or other healthcare product where there is no reason to doubt its quality or safety and when there is
credible evidence of efficacy.</blockquote><p>Pharmacies very widely fail to follow this principle, and the Society for Science-Based Health has been annoying them by pointing this out, especially in relation to homeopathy. There have been <a href="http://www.pharmacytoday.co.nz/news/2015/march-2015/16/pharmacists-defend-sale-of-homeopathic-products.aspx">various defences</a>: they should be able to sell it because people want it, they should be able to sell it because cultural diversity, they should be able to sell it because quantum<sup>*</sup>. They may have a point<sup>#</sup>, but they also have a violation of what are supposed to be legally-enforceable professional standards. If pharmacists honestly believe these arguments, the solution is to change the code of ethics so that it matches reality. Be like the journalists: they will readily admit that horoscopes and reprinted sensationalist linkbait nutribollocks are, all things being equal, undesirable in newspapers. On the other hand, they say, people want it and it pays for the serious investigative journalism.&nbsp;</p><p>Perhaps that’s true of pharmacies as well. Maybe supplying the popular demand for woo is in the public interest, especially if it makes access to qualified pharmacists more widely available for people who want real medications. If so, they should admit it. &nbsp;At the moment, any pharmacist who sells homeopathy is violating the code of ethics, as is any pharmacist who knows of these breaches and doesn’t report them to the Pharmacy Council.</p><p><br></p><p><i>*&nbsp;I'm not making this up, you know.</i></p><p><i># except about the quantum</i></p>]]></content:encoded>
    <wp:post_name>115011616346</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/114906106186</link>
    <pubDate>Sun, 29 Mar 2015 17:40:37 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/114906106186</guid>
    <!--<wp:post_id>114906106186</wp:post_id>-->
    <wp:post_date>2015-03-28 21:40:37</wp:post_date>
    <wp:post_date_gmt>2015-03-29 04:40:37</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Paper helicopters at a science fair</title>
    <description></description>
    <content:encoded><![CDATA[<p>Today, we ran <a href="http://williamghunter.net/george-box-articles/teaching-engineers-experimental-design-with-a-paper-helicopter">Box’s paper helicopter experimental design example</a> at the <a href="http://www.motat.org.nz/experience/events/science-street-fair/">Science Street Fair </a>&nbsp;sponsored&nbsp;by the <a href="http://scientists.org.nz/posts/2015/01/motat-science-street-fair">NZ Association of Scientist</a>s and hosted by the Museum of Transport and Technology.</p><p>It went fairly well. In particular, the younger kids really liked dropping paper helicopters and comparing different designs and we got in a few useful discussions of experimental design with adults -- mostly school teachers.</p><p>Things to note:</p><ul><li>Use a photocopier and pre-printed design template, such as the one from the <a href="http://cran.r-project.org/web/packages/SixSigma/index.html">SixSigma</a> package for R</li><li>This lets you also produce a 2-up version, giving an extra interesting design factor. The little copters are easier for kids to drop.</li><li>Having five or six experimental factors and a $2^{5-1}$ or $2^{6-2}$ design makes it easier to motivate a good structured design</li><li>Let the kids drop the helicopters, and time the flight, but don’t use that data in modelling.&nbsp;</li><li>The kids like being given copters or paper templates to take home.</li><li>For the good data, it’s important that the drop height is standardised, and is as high as practicable. An adult standing on a chair is (barely) enough.</li><li>For timing, have the person dropping the copter count 3,2,1,drop and the person with the timer watching the floor where it will land.</li><li>It’s worth doing each drop twice and using the average as the measurement, because that’s easy.&nbsp;</li><li>You need at least two and probably three people to run things.</li><li>You can do most of the cutting ahead of time or in quiet periods</li><li>Enter the data as you go, so you can show boxplots for each factor.</li></ul><p>Our experimental factors were height, wing length, paper type, size (A4 vs A5), and paperclip or not. &nbsp;The paperclip and size are interesting because they have a big effect on how the flight looks, but not a big effect on the flight time.&nbsp;</p><figure data-orig-height="1011" data-orig-width="2008"><img src="https://40.media.tumblr.com/c29bbc622eab5842d3ba37a1cc968c69/tumblr_inline_nlyipa6ySv1s1hdxy_500.png" data-orig-height="1011" data-orig-width="2008"></figure><p>Things to avoid</p><ul><li>getting absolutely drenched by unexpected rain on the way to the event</li><li>doing it with a bad cold.&nbsp;</li></ul>]]></content:encoded>
    <wp:post_name>114906106186</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/113580307091</link>
    <pubDate>Sat, 14 Mar 2015 20:25:54 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/113580307091</guid>
    <!--<wp:post_id>113580307091</wp:post_id>-->
    <wp:post_date>2015-03-14 0:25:54</wp:post_date>
    <wp:post_date_gmt>2015-03-14 07:25:54</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>A completely trivial innovation</title>
    <description></description>
    <content:encoded><![CDATA[Hadley Wickham liked my use of

```
set.seed(2015-3-13)
```

in a bug report, occasioning some discussion on Twitter. 


Any reproducible example needs some way to set the seed. I like this one because

* It documents the date of the script
* It relies on the [One True Date Format](http://xkcd.com/1179/)
* It's obviously generic: I didn't have to try a bunch of different seeds before getting one that worked

Obviously I realise this doesn't give different seeds for every date. That's not relevant, let alone necessary, for the purpose it's being used for. People suggested various alternatives that *would* be unique. Many of these required loading other packages -- a Bad Idea in a bug report. 

My favorite (serious) alternative was `set.seed(as.Date("2015-3-13"))`, but even that I think is belabouring the point a bit. Perhaps more importantly, it relies on the implicit conversion of `Date` to integer, and thus on the implementation of the `Date` class. ]]></content:encoded>
    <wp:post_name>113580307091</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/113209424991</link>
    <pubDate>Tue, 10 Mar 2015 13:26:38 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/113209424991</guid>
    <!--<wp:post_id>113209424991</wp:post_id>-->
    <wp:post_date>2015-03-09 17:26:38</wp:post_date>
    <wp:post_date_gmt>2015-03-10 00:26:38</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>A little data and software carpentry</title>
    <description></description>
    <content:encoded><![CDATA[<p>I've got<a href="http://www.nzherald.co.nz/data-blog/news/article.cfm?c_id=1503710&amp;objectid=11414861"> a post up at the NZ Herald data blog</a>, breaking down electoral donations by electorate. This is how it was made.</p><p>For the benefit of people who aren't from NZ, we run a modified version of <a href="http://nielsenhayden.com/makinglight/archives/010837.html">Westminster</a> government as a virtual machine on top of a German-style electoral system. There are 71 electorates, each of which elects a member of Parliament (by first-past-the-post). There is also a nationwide party vote, and enough members without electorates are added to give each party proportional representation in &nbsp;Parliament (modulo various important edge cases).</p><p>In most cases, electorate MPs aren't terribly important to who wins power, but they are still your local voice: you want to choose someone good, or have undue influence over someone bad. Donations to them are still interesting.&nbsp;</p><p>Sadly, an <a href="http://www.nzherald.co.nz/matt-nippert/news/article.cfm?a_id=644&amp;objectid=11409374">infelicity in the disclosure laws</a> allows the parties to donate to candidates' campaigns without saying where the money was from originally. The National Party was by far the largest donor, responsible for more than a third of all donations, and over 80% of donations to its own candidates. Still, it's interesting to look at the patterns across the country.&nbsp;</p><p>The donation data was the easiest: the Herald has been running a <a href="http://www.nzherald.co.nz/nz/news/article.cfm?c_id=1&amp;objectid=11412326">data journalism project </a>on the donations and expenses and&nbsp;<a href="http://media.nzherald.co.nz/webcontent/infographics/515/electoral_donations_2014.csv">provides it</a>.&nbsp;</p><p>I needed a map, but not a straightforward NZ map. Electorates all have about the same number of people, but are very different sizes geographically. Just colouring in a map gets you the well-known "one cow, one vote" effect: rural electorates dominate.&nbsp;</p><p>In the US, '<a href="http://www.washingtonpost.com/wp-srv/special/business/states-most-threatened-by-trade/">statebins</a>' are a new solution, using a layout of equal-sized rectangles for the states. For New Zealand, I decided to use a &nbsp;grid of equal hexagons designed by Chris McDowall and modifed after the Census by David Friggens, &nbsp;Their code is in Javascript; I wanted it in R. &nbsp;It was easy to convert the data parts of the code either in to R directly or into JSON, and then use the jsonlite package for R to read it in.&nbsp;</p><p>I also wanted data on how close the voting was in each electorate, since if the motivation for donating was to influence the result you'd expect more donations in marginal electorates. The Electoral Commission provides the <a href="http://www.electionresults.govt.nz/electionresults_2014/electoratestatus.html">data</a>, but in a relatively complex HTML table. I cut-and-pasted the table into Excel and exported it as CSV. There was a hitch (isn't there always?). Excel converted non-ASCII characters into '_' for safety. There are a lot of non-ASCII characters in the file, for names such as Ōhāriu and&nbsp;Tāmaki. They aren't in the common Windows code pages, either. After a few tries it turned out that Excel can write what it calls Unicode, which is tab-separated UCS-2LE, and that worked. At this point I was relieved to find that all my data sources used the same spellings for all the electorate names, macrons and all.&nbsp;</p><p>Now I could merge the datasets and use the R hexbin functions to draw the maps. Only not. After some debugging I discovered -- actually, rediscovered -- that the hexbin structure is a lot less flexible than you'd think from the list of components. I restructured the code to get hexmaps. It was 7:15, so I left, missed a bus, and bought some <a href="http://giapo.com/">gelato</a>. I had to wait for them to make a new batch of toffee strawberries for the garnish.&nbsp;</p><p>What I wanted next was tooltip annotation so that the name of the electorate would pop up when you pointed at it with the mouse. Even without the distortions of the hexmap, I don't know from electorate names. Paul Murrell (and Simon Potter) have tools for this, the gridSVG package. Simon even had a detailed example of adding tooltips. It worked beautifully. Except that all the electorates were called <a href="http://pogostick.co.nz/why-visit-invercargill/">Invercargill</a>.</p><p>It turns out that you need to produce each hex with a separate plotting command in order for it to have its own identity and be given its own tooltip. That's ok. We have loops. I rewrote the hexbin plotting code to use loops and to produce SVG output. Then it worked! Well, after fiddling with margins and offsets so you could actually read the labels, it worked.</p><p>Next question was how to display it online. Mike Bostock's <a href="http://bl.ocks.org/">bl.ocks.org</a> site will display JavaScript and SVG from a Github gist. After a bit of confusion due to the caching in the Github API (it takes a few minutes to update). I had a <a href="http://bl.ocks.org/tslumley/raw/9d6265ad35cc9b0f125b/">web page</a> that displayed the maps in thumbnail form and let you click to get a version with electorate names. These all have area proportional to money, with a constant scale across all the maps, so you can compare parties as well as electorates.&nbsp;</p><p>This was not an especially difficult data carpentry and visualisation project, but it involved translation of data and code, a lot of web searching for how to do annotation of SVG, debugging, learning more about Github and bl.ocks.org, and finding exactly what colours the parties actually used in their logos.&nbsp;</p><p>If I'd done it before, it would have been faster. If I do it again, it will be faster. When I do something different, though, there will still be the task of learning new tools and making them fit together. Flexible tools such as R or d3.js or python make this sort of thing possible, but it still takes some effort on top of the effort of getting the right data into the right formats. The whole thing took about eight hours, though it might have been faster without interruptions (and, obviously would have been much faster without dead ends).&nbsp;</p><p>If you want to know what I found, <a href="http://www.nzherald.co.nz/data-blog/news/article.cfm?c_id=1503710&amp;objectid=11414861">read the post at the Herald</a></p>]]></content:encoded>
    <wp:post_name>113209424991</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/112957194026</link>
    <pubDate>Sat, 07 Mar 2015 22:06:03 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/112957194026</guid>
    <!--<wp:post_id>112957194026</wp:post_id>-->
    <wp:post_date>2015-03-07 1:06:03</wp:post_date>
    <wp:post_date_gmt>2015-03-07 09:06:03</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>What does measurability mean?</title>
    <description></description>
    <content:encoded><![CDATA[<p><i><a href="http://www.viridiandesign.org/notes/1-25/Note%2000002.txt">Attention conservation notice</a>: A long, meandering, &nbsp;and inconclusive attempt to explain why you perhaps shouldn't worry about a technical issue you almost certainly weren't worrying about already.&nbsp;</i></p><p>Mathematical proofs in statistics are, in some formal sense, useless. That is, they formally have conditions such as finite moments, boundedness, differentiability or stochastic equicontinuity that either apply to all things in the real world or to none. The proofs are also often formally about infinite sequences; these don't crop up all that often in data analysis.&nbsp;</p><p>Proofs are useful to the extent that the conditions say what you can ignore. That is, showing a remainder term $r_n(x)$ &nbsp;is $o_p(1)$ &nbsp;formally means that for every positive $\epsilon$<br>$$\lim_{n\to\infty} \Pr(|r_n(x)|&gt;\epsilon)=0$$<br>but practically means that in data sets of reasonable size there's a reasonable hope that ignoring $r_n(x)$ won't matter. Continuity assumptions mean that a result is likely to work for functions that don't jump around too much. Moment bounds mean a variable shouldn't have too many outliers, and boundedness assumptions mean a transformation shouldn't create too many outlliers.&nbsp;</p><p>Asymptotics works surprisingly well in statistics. The Central Limit Theorem appplies to infinite sequences, but means are usually pretty well Normal well before $n=100$. In principle, sharp bounds for the accuracy of the Normal approximation (such as those given by the Berry--Esseen Theorem) would &nbsp;be better. In practice, though, the Normal approximation to a mean works a whole lot better than the Berry-Esseen bound would suggest. &nbsp;</p><p>There's one mathematical condition that doesn't at first glance seem to have any approximate meaning: measurability. &nbsp;In most of mathematical statistics any set or function anyone could reasonably be interested in is measurable. In fact, there is a model of set theory (<a href="http://en.wikipedia.org/wiki/Solovay_model">Solovay's model</a>) in which all sets of real numbers are measurable. While it doesn't have the Axiom of Choice, it has a weaker version that's enough for many theoretical purposes. If you care about measurability, you have to care about the difference between the Axiom of Choice and the Axiom of Dependent Choice. If you're not a set theorist, this is hard. It can feel as if you're repeating the equivalent of "Four legs good; two legs bad."</p><p>In a sense, measurability problems are Pythagoras's fault. He (<a href="http://nrich.maths.org/2671">his followers</a>) found that there wasn't a number whose square was 2. Later researchers found other gaps between numbers. Even after you decide that all the powers of rational numbers and all the solutions of polynomial equations count as numbers, you still have more gaps than numbers. We've decided to just <a href="http://en.wikipedia.org/wiki/Dedekind_cut">decree</a> that the gaps are numbers too. &nbsp;'Constructions' (and I use that word in the weakest possible sense) of non-measurable sets rely on the way these gaps now constitute nearly all numbers. A non-measurable set or non-measurable function is one that treats gap numbers differently from numbers that have some definite reason for existence. It can't quite be as simple that, because the <a href="http://en.wikipedia.org/wiki/Axiom_of_constructibility">Axiom of Constructibility </a>implies non-measurable sets, but it seems like something in that direction. Because we have too many numbers, we have too many sets of numbers, and they can't all be measurable. Solovay's model solves this problem by saying that a lot of the sets (including all the non-measurable ones) aren't really sets, which emphasizes how difficult it's going to be to translate this into something helpful in statistics.&nbsp;</p><p>More evidence that measurability may not mean anything in most contexts comes from <a href="http://en.wikipedia.org/wiki/Littlewood%27s_three_principles_of_real_analysis">Littlewood's three principles</a> for heuristics in real analysis. The first principle is that a measurable set is 'almost' a finite union of intervals and the second principle is that an <strike>integrable</strike>&nbsp;measurable function is 'almost' continuous: that is, the practical approximations to measurability are basically the same as the practical approximations to continuity. Measurability is heuristically just a slightly weaker version of continuity. &nbsp;That's more or less what Robins and Ritov argue in their 1997 paper <i>"Towards a curse-of-dimensionality-appropriate asymptotics for semiparametric models".</i>&nbsp;Their point is that data in high dimensional spaces is always going to be sparse, so that continuity doesn't buy you anything at reasonable sample sizes, and measurability is the right model for what you can really assume. Even so, they are using 'measurable' to mean 'basically anything'.</p><p>The only examples I know of where measurability fails and you care are in really, really big spaces. &nbsp;Suppose you have a random number uniformly distributed on $[0,1]$ &nbsp;and consider the random empirical distribution function that results. &nbsp;There are good reasons why you might want to use the uniform metric on distribution functions -- taking advantage of the <a href="http://en.wikipedia.org/wiki/Glivenko%E2%80%93Cantelli_theorem">Glivenko--Cantelli theorem</a>&nbsp;-- but if you do, the map from random numbers to random distribution functions is not measurable. Because the uniform metric on distribution functions doesn't care which $x$s are close to each other, you can find &nbsp;an open set of distribution functions with jumps at any set of values. If the map were measurable, this would imply all subsets of $[0,1]$ were measurable, and that won't fly.&nbsp;</p><p>In this example measurability is a property that random distribution functions have under the Skorohod topology but not under the uniform topology. Potentially that gives us a lever to find a way &nbsp;to say something about what sort of property it is and why we should care. But, after all this, I'm still not sure whether measurability is sometimes a useful approximate property or whether it's a purely formal property. Math is hard. &nbsp;</p>]]></content:encoded>
    <wp:post_name>112957194026</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/112290446551</link>
    <pubDate>Sat, 28 Feb 2015 18:49:32 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/112290446551</guid>
    <!--<wp:post_id>112290446551</wp:post_id>-->
    <wp:post_date>2015-02-27 21:49:32</wp:post_date>
    <wp:post_date_gmt>2015-02-28 05:49:32</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>How hard did you look: equivalence and non-inferiority</title>
    <description></description>
    <content:encoded><![CDATA[<p>I usually don't read nutripharma articles outside the mainstream media, but someone tweeted <a href="http://www.donotlink.com/dw82">a link about saffron</a>, which apparently cures everything. The last straw was a line beginning&nbsp;</p><blockquote>"Saffron, a major component of the Mediterranean diet..."<br></blockquote><p>Saffron can't really be described as a major component of anything, even risotto milanese, and it's not unique to the Mediterranean region: it's a well-known spice in India, Pakistan, Iran. And it's not just the well-known places: England produced saffron before Italy produced tomatoes.&nbsp;</p><p>There's no shortage of things to criticise about the article, but there's one that is an important applied biostatistics point. &nbsp;The article says</p><blockquote>Loss of cognitive function is one of the most common, most feared consequences of aging. Studies show that saffron has promise in preventing or ameliorating some of those effects. A 16-week trial of saffron (30 mg per day) showed that the supplement was superior to placebo in patients with early, probable Alzheimer’s disease. The same dose, for 22 weeks, proved similar in effect to the prescription drugs memantine and donepezil in a comparable population.<sup>85,86</sup><br></blockquote><p>That is more or less what references <a href="http://www.ncbi.nlm.nih.gov/pubmed/19838862">85</a> and <a href="http://www.ncbi.nlm.nih.gov/pubmed/25163440">86</a> say (except they were in quite different populations to the first study: mild to moderate, and moderate to severe disease), but it isn't what they actually found. &nbsp;The two studies randomised a small number of patients to saffron or a fairly modestly effective drug, and didn't collect enough information to demonstrate a difference. Failing to find a difference isn't evidence that there isn't one there -- it may just mean you didn't look hard enough.</p><p>If you want to prove that a new treatment is not much worse than an existing treatment, you need a definition of "not much worse" (somewhat unfortunately called 'non-inferiority') &nbsp;and a test whose null hypothesis is 'not non-inferiority'. Alternatively, if you're a Bayesian, you need a prior that says saffron probably doesn't work (since almost everything doesn't) and you need to collect enough evidence to have high posterior probability that saffron is 'non-inferior' to the drug.</p><p>Reference 85 looks at cognitive function scores after 22 weeks and says<br></p><blockquote>The changes at the endpoint compared to baseline were −3.96 ± 3.50 (mean ± SD) and −3.77 ± 3.80 for saffron and donepezil, respectively.&nbsp;</blockquote><p>This is with 27 people in each group, so the standard error of the difference is 1 point. &nbsp;According to <a href="http://www.ncbi.nlm.nih.gov/pubmed/15254918">a systematic review</a>, the benefit of10mg donepezil over placebo at 24 weeks on this scale is 3.1 points with a standard error of 0.4.&nbsp;</p><p>Putting these together, the estimated benefit of saffron over placebo is&nbsp;3.1-(3.77-3.96)=3.29, with a standard error of 1.08. If we can completely rely on this study, it still allows for saffron to be only 1/3 as effective as donepezil. Given that it's controversial how worthwhile donepezil is, that's not really good enough. We can't say they've demonstrated a similar effect</p><p>Worse than that, though, is the whole idea of a non-inferiority trial. In a blinded, randomised trial, any imperfections in measurement or trial conduct will tend to reduce your ability to see differences. &nbsp;Usually people are trying to show a difference, so we don't need to worry too much about whether everything was done perfectly. In a non-inferiority trial, you are trying not to see a difference, so it's very important that the trial is planned and conducted to the highest possible standards. Starting off with too few participants and the wrong analysis isn't encouraging.</p><p>In any case, it's the wrong question. There's no reason to think saffron would worsen the side-effects of donepezil, so the right question would be whether donepezil plus saffron was better than donepezil alone, tested in an ordinary double-blind study.</p><p>While we're doing calculations, there are an <a href="http://www.ncbi.nlm.nih.gov/pubmed/23305823">estimated</a> 35 million people in the world with dementia. At the study dose of 30mg/day, that's 383 tonnes/year, or about 25% more than the current world production. &nbsp;If saffron worked, the next step would be identification and synthesis of the active components. It wouldn't be a natural herbal remedy for long. &nbsp;</p>]]></content:encoded>
    <wp:post_name>112290446551</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/112106337996</link>
    <pubDate>Thu, 26 Feb 2015 17:14:01 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/112106337996</guid>
    <!--<wp:post_id>112106337996</wp:post_id>-->
    <wp:post_date>2015-02-25 20:14:01</wp:post_date>
    <wp:post_date_gmt>2015-02-26 04:14:01</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Clinically proven ingredients</title>
    <description></description>
    <content:encoded><![CDATA[<p>NZ golf prodigy Lydia Ko has a sponsorship deal with a company that sells special jetlag-reducing water. She obviously knows how this sort of thing works, and what she <a href="http://www.nzherald.co.nz/lifestyle/news/article.cfm?c_id=6&amp;objectid=11408199">said to the Herald</a> was nicely crafted</p><blockquote>Ms Ko said she was excited to have the support of 1Above.<br>"I haven't really taken it for a long-haul flight before but I've seen some of the results and everything that comes with it and I have heard great things," she said.</blockquote><p>The reporter was also being careful about attribution</p><blockquote>1Above contains Pycnogenol. Its makers say the natural pine bark extract is a key active ingredient in reducing the severity and length of jetlag.</blockquote><p>I looked at the <a href="http://fly1above.com/benefits.html">company's web page</a>, which has references for research papers that are supposed to support its claims. You might be interested, too. If not, this would be a good place to stop.</p><p>It's not straightforward to find what dose is in 1Above (you'd think they would want you to know). However, a<a href="http://www.integratedsafety.com.au/blog/1above-product-review-a-magic-tonic-for-jet-lag"> web page</a> by someone who has used it and who was able to find some of the research papers says the 600ml bottle has 102mg of Pycnogenol.&nbsp;For 10 hours (the sort of flight length studied in the research) 1Above recommends 4 of their tablets, which makes two litres of drink, and so should have 340mg of Pycnogenol.</p><p><br></p><p>The first reference to research in people is (<a href="http://www.ncbi.nlm.nih.gov/pubmed/19597404">PubMed</a>)</p><p>Belcaro G et al (2008). Jet lag prevention with Pycnogenol. Preliminary report: evaluation in health individualised hypertensive patients. <i>Minerva Cardioangiol</i>: 56 (5 Suppl):3-9<br></p><p>The abstract doesn't say this was placebo-controlled. I'd ordinarily conclude it wasn't. On the other hand, one of the other references doesn't say in the abstract and was, so it might just be poor writing. On the other other hand, 1Above didn't claim it was placebo-controlled, so I think it's safe to conclude it wasn't. I can't tell more because the paper doesn't appear on the website of the journal. Together with the "Suppl" in the citation information it suggests conference papers, but that doesn't mean it wasn't peer-reviewed. The dose was 50mg of Pycnogenol three times per day for 7 days.</p><p>1Above report the results of the study, but don't mention that it used a week of treatment (which isn't what they recommend).</p><p><br></p><p>The second reference is given as</p><p>Belcaro G et al (2004). Pycnogenol® prevents thrombosis and thrombophlebitis in long-haul flights.<i> Clin Appl Thromb Hemost.</i> 10(4): 289-294</p><p><br>That's the right journal and page reference, <a href="http://www.ncbi.nlm.nih.gov/pubmed/15497024">but the wrong title</a>. The real title is a bit weaker: "Prevention of venous thrombosis and thrombophlebitis in long-haul flights with pycnogenol."<br></p><p>In this case the study was placebo-controlled (though still no specific mention of randomisation or blinding of the people doing measurements). The dose was 200mg before the flight, 200mg during the flight, and 100mg the next day -- a bit higher than 1Above, but not ridiculously different.</p><p>1Above says </p><blockquote>"It has been proven to reduce and possibly even eliminate the occurrence of deep vein thrombosis (DVT) during flight. In a double blind Placebo study on 198 passengers, those taking Pycnogenol® supplements showed no thrombosis events as opposed to 5 reported cases in the control group"</blockquote><p>Even if we take their word for it being double-blind, there's a fairly dodgy slip in definitions between the first and second sentence. There were 5 measured 'thrombotic events' in the control group, but only one was a DVT. Also "reported" means reported by the researchers, not by the traveller -- the DVT was detected by ultrasound and had not given rise to any symptoms. </p><p><br></p><p>The third reference is given as<br></p><p>Cesarone M R et al (2004). Pycnogenol® is effective against swelling of ankles during long flights based on the subjective and objective data in a double-blind, placebo controlled study. <i>Clin Appl Thromb Hemost</i> 11 (3): 289-294<br><br>Again, the right journal and page reference, but the real title is "Prevention of edema in long flights with Pycnogenol."<br><br>This is one is also placebo-controlled, but does not say so in the abstract or in the real title. In contrast to the website, the paper doesn't claim the study was double-blind, but they might have got that from the researchers. Swelling was lower by about half in the treated group. &nbsp;The dose, again, was 200mg before, 200mg during, and 100mg after.</p><p>1Above uses this to justify a claim that it can&nbsp;<i>reduce swelling by up to 69% of the lower limbs. </i>I don't know where they got that number. It's not one of the ones in the abstract, or in this results table from the paper.&nbsp;</p><figure><img src="https://31.media.tumblr.com/097ea750b723528858d406312648934a/tumblr_inline_nkcygjVBKV1s1hdxy.png" alt="image"></figure><p><br></p><p><br>The final reference on relevant human research is</p><p> Belcar G (2013). Pycnogenol supplementation speeds-up recovery from a common cold, and even more efficiently in combination with vitamin C and zinv. <i>Otorinolaringol</i> 63: 151-161, 2013<br></p><p>Again, the title is wrong. <a href="http://www.minervamedica.it/en/journals/otorinolaringologia/article.php?cod=R27Y2013N03A0151">The real title</a> is "The common cold winter study: effects of Pycnogenol® on signs, symptoms, complications and costs". Also, spelling. This one was harder to find, because the journal doesn't seem to be in PubMed or in our library. However, there's a <a href="http://www.now-university.com/Library/HealthConcerns/AllergyImmuneSystem/099634.htm">post</a> about it by a naturopath.&nbsp;</p><p>People who took Pycnogenol reported less symptoms. The dose was 100mg/day for the duration of the cold. It doesn't seem to have been blinded, and it's not clear it was randomised.</p><p>1Above cite this study to support <i>"reduce flu like symptoms by up to 2 days"</i>, and have a bar chart labelled "<i>Reduce flu longevity.</i>" The study didn't measure flu-like symptoms. It measured common cold symptoms, and while there's some overlap, these are different. More importantly, the study didn't look at taking Pycnogenol in advance to prevent flu-like illness (or a cold). It looked at taking it after symptoms started, to reduce the duration. So, it would only be relevant to flying if you already have a cold before you fly.</p><p><br></p><p>So, on the one hand this is not very good reporting of small-scale research. You couldn't get away with this for medications.&nbsp;On the other hand, it's better than the <a href="http://www.statschat.org.nz/2015/01/02/is-this-being-sold-to-people-who-care-if-it-works/">grape extract for weight-loss company</a>, or the one claiming to get f<a href="http://www.stuff.co.nz/technology/gadgets/66622959/With-one-DNA-sample-new-technology-can-recreate-your-face">acial reconstructions from DNA</a>, to name a few recent examples, and it's less likely that 1Above will do any harm that reaches beyond the wallet.</p><p>The most obvious potential problems with the research are quality of blinding (of people doing measurements) and of randomisation, which mostly weren't mentioned.&nbsp;</p><p>A less obvious but potentially important problem is publication bias. According to the <a href="http://www.ncbi.nlm.nih.gov/pubmed/?term=Belcaro+G%5BAuthor%5D+pycnogenol">PubMed database</a>, the first author of three of the references (and an author of the fourth), G. Belcaro, has 41 publications on Pycnogenol. All 41 abstracts report positive effects, across a wide range of conditions. Dr Belcaro has either been extremely lucky (in which case the results are biased by luck) or has not reported negative studies (in which case the published results are a biased sample), or has measured lots of things and put the positive results in the abstract (in which case they are again a biased sample).</p>]]></content:encoded>
    <wp:post_name>112106337996</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/111496162326</link>
    <pubDate>Fri, 20 Feb 2015 09:51:25 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/111496162326</guid>
    <!--<wp:post_id>111496162326</wp:post_id>-->
    <wp:post_date>2015-02-19 12:51:25</wp:post_date>
    <wp:post_date_gmt>2015-02-19 20:51:25</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Where my HOP card money goes</title>
    <description></description>
    <content:encoded><![CDATA[<p>Auckland Transport have a newish unified card to pay for public transport. After a lot of mess while it was being rolled out, this is a big improvement over the previous system.</p><p>I buy a Zone A monthly pass, which is excellent value since I live almost at the edge of Zone A. However, once or twice a month, the system charges me rather than using the pass. Since we're talking a couple of dollars each time, it's not actually worth making a special trip each time to the city-centre office to get a refund, so the balance on the card slowly gets lower. This morning, they did it again, and now my pass won't work until I put money on the card. <i>#grumpy</i>.</p><p>Transactions on the cards are recorded and available on line, with a couple of limitations. First,<i> today's</i> transactions aren't available, so I have to be more annoyed than usual to still want to chase down the problem the next day. Second, my online transactions only go back a month, so it's hard to get good data on what's wrong.</p><p>This is what my day usually looks like (ok, it's usually shorter)</p><figure><img src="https://31.media.tumblr.com/31acb48f9365e302c196b0f34ce2a5ae/tumblr_inline_nk1detKGfT1s1hdxy.png"></figure><p>This is last week, the previous charging error</p><figure><img src="https://31.media.tumblr.com/a8b0ff014acf5a984d4018380b3b1a52/tumblr_inline_nk1dg8XMkT1s1hdxy.png"></figure><p>The bus was <i>seriously</i> confused about its location: 730 Massey Rd is at the bottom of the map. I live in the middle of the map, in Onehunga, and the University is at the top of the map.</p><figure><img src="https://31.media.tumblr.com/cc81895fba1601de0e7391b986a7e231/tumblr_inline_nk1dmecg5d1s1hdxy.png"></figure><p>Since the buses have GPS it's not clear how this can happen, but it does.&nbsp;</p><p>It's possible to request a refund online. You can even explain why you should get one. Sort of.</p><figure><img src="https://31.media.tumblr.com/ef1455e47f44339c512b85da6eed072f/tumblr_inline_nk1dpzgoJF1s1hdxy.png"></figure><p>That is, you get about 70% as much space as Twitter gives you.&nbsp;</p><p>So, I fixed that.</p><figure><img src="https://31.media.tumblr.com/dc3f3f3a63b9c3604dcc1e608216e85b/tumblr_inline_nk1drikxbd1s1hdxy.png"></figure><p>Well, if I can't get a $1.60 refund I can at least whinge about it on my blog. That's what the internet is for. &nbsp;</p>]]></content:encoded>
    <wp:post_name>111496162326</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/111297543186</link>
    <pubDate>Wed, 18 Feb 2015 08:00:37 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/111297543186</guid>
    <!--<wp:post_id>111297543186</wp:post_id>-->
    <wp:post_date>2015-02-17 11:00:37</wp:post_date>
    <wp:post_date_gmt>2015-02-17 19:00:37</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Science and statistical inference</title>
    <description></description>
    <content:encoded><![CDATA[<p>Q: There have been a lot of papers recently about the spike in p-values just below 0.05, haven't there?</p><p>A: Yes. A bit depressing. But there's a new analysis that says it's ok.</p><p>Q: Really? That's great!</p><p>A: Yes, <a href="http://daniellakens.blogspot.nl/2015/02/a-peculiar-surge-of-incorrect.html?m=1">Daniel Lakens shows</a> that you can explain the recent increase in just-significant p-values, and that <i>"</i><b>data does not provide any indication of an increase in questionable research practices"</b></p><p>Q: And is his modelling correct?</p><p>A: It certainly looks plausible.</p><p>Q: So what does explain the 0.04 spike, if it's not 'questionable research practices'</p><p>A: Let's see...Ah. A decrease in power and increase in publication bias.</p><p><br></p><p><br></p><p><br></p><p>&nbsp; &nbsp; &nbsp; <i>** crickets **</i></p><p><br></p><p><br></p><p><br></p><p>Q: Sorry, I got nothing.</p><p>A: Well, better go, grant proposals aren't going to write themselves.</p>]]></content:encoded>
    <wp:post_name>111297543186</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/108190559231</link>
    <pubDate>Fri, 16 Jan 2015 09:28:00 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/108190559231</guid>
    <!--<wp:post_id>108190559231</wp:post_id>-->
    <wp:post_date>2015-01-15 12:28:00</wp:post_date>
    <wp:post_date_gmt>2015-01-15 20:28:00</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Assumptions and testing</title>
    <description></description>
    <content:encoded><![CDATA[<p>My attention was drawn on Twitter to an <a href="http://www.jstor.org/stable/2685616">old (1999) paper</a> in The American Statistician, <em>"Different Outcomes of the Wilcoxon-Mann-Whitney Test from Different Statistics Packages".&nbsp;</em>The authors looked at 11 statistics packages and found they didn't always give the same result for the Wilcoxon/Mann-Whitney test. The big problem was handling of tied observations.</p>
<p>Here are their example data:</p>
<p><img alt="image" src="https://31.media.tumblr.com/519a4fcce4acc3503929e45f44f0c180/tumblr_inline_ni8i9uj4qY1s1hdxy.png" /></p>
<p>The authors say <em>"It is obvious that the data resulting from the experiment could not be analyzed by the Student's t-test."&nbsp;</em>I would argue that with 19 of 24 data points identical, a test based on ranks isn't very attractive, either.</p>
<p>The experiment involved making rats balance on a rotating cylinder, and the treatment was a muscle relaxant. The values at 300s are really censored-- the rat hadn't fallen off after five minutes -- so the problem of ties is artificial.</p>
<p>At least this example doesn't have the transitivity problem: it makes sense that the treatment would only make the time shorter, not longer, so the distributions in the two groups should be stochastically ordered if they are different. &nbsp;But the large number of ties makes it harder to get an exact $p$-value for the Wilcoxon/Mann-Whitney test (which is the main point of the test) and also makes the mean rank a less interesting statistic.</p>
<p>It would make more sense either to treat the data as binary (300 or not) or, even better, as censored, which gets rid of the ties problem. With the Wilcoxon test on the measurements as given, the p-value is about 0.085 pretending it's a continuous distribution, or about 0.015 using a correction for ties.&nbsp;</p>
<p>Treating the censoring appropriately and using the $G^\rho$ weighted logrank test with $\rho=1$ (which reduces to the Wilcoxon test in uncensored data) gives a $p$-value of 0.014. No problem with ties; no problem with software.</p>
<p>And, while it might be "obvious" to the authors that a $t$-test cannot be used, the only reason I'd agree is that the data are censored. The permutation distribution of the Student $t$-statistic agrees very well with its nominal $t_{22}$ distribution. With groups of equal sizes, the $t$-test has excellent robustness of level. Its power is not robust to outliers, but in this example that's a feature, not a bug: you want a test that is able to notice how much smaller the values are in the treatment group.</p>
<p><img alt="image" src="https://31.media.tumblr.com/cc71b88af226dd2685932ac55ee498bf/tumblr_inline_ni8j873YO11s1hdxy.png" /></p>
<p>The paper is basically arguing for the permutation distribution in tests like this one, and says that Wilcoxon proposed his test to make permutation inference computationally simpler than it would be for, say, the difference in means. &nbsp;That was true in 1945. It wasn't true in 1999, and it certainly isn't true now: the advantage of a permutation approach is that you get to use whatever summary of the difference between groups you want; you don't need to choose it for mathematical convenience.</p>
<p></p>
<p></p>]]></content:encoded>
    <wp:post_name>108190559231</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/108126996681</link>
    <pubDate>Thu, 15 Jan 2015 15:03:00 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/108126996681</guid>
    <!--<wp:post_id>108126996681</wp:post_id>-->
    <wp:post_date>2015-01-14 18:03:00</wp:post_date>
    <wp:post_date_gmt>2015-01-15 02:03:00</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>A transitive test is a test for a univariate parameter</title>
    <description></description>
    <content:encoded><![CDATA[<p><a href="http://notstatschat.tumblr.com/post/63237480043/rock-paper-scissors-wilcoxon-test">As you know</a>, rank tests can be non-transitive: they can have the rock-paper-scissors property. Tests that are for a single real-valued summary statistic (eg a test comparing means or medians or variance) are always transitive, because they are just comparing a single number, and ordering on numbers is transitive.</p>
<p>The converse is almost obviously almost true: if you have a transitive test, it almost has to be a test for a single real-valued summary statistic. The test gives you an ordering on distributions, so there's a one-dimensional summary statistic in some ordered set saying where you are in the ordering. In any sane situation that one-dimensional summary statistic can be chosen to be real-valued.</p>
<p>What gets a little tricky is the definition of 'sane'. Econometricians have been looking at this problem since at least the 1960s, and there's one simple pathological case. &nbsp;Suppose your test statistic uses the lower quartile, and then breaks ties using the upper quartile. &nbsp;That is, you have a two-dimensional summary statistic and you flatten it into one dimension by using the second value just to break ties. &nbsp;This doesn't fit into the real numbers: the ordering has uncountably many copies of the real numbers, because the second number can be anything, for any value of the first number. You can't replace the pair of numbers with a single number.</p>
<p>A relatively recent (2002) <a href="https://ideas.repec.org/a/eee/mateco/v37y2002i1p17-38.html">paper</a> by Alan Beardon and co-workers characterises all the things that can go wrong. There are four of them</p>
<ol>
<li>The two-dimensional ordering, as above</li>
<li>Just Too Long: when you keep finding bigger and bigger elements you have to go on not just a countably infinite number of times, but an uncountably infinite number of times before you bound the whole set.</li>
<li>A weird thing called Aronszajn chains</li>
<li>A truly weird thing called a Souslin line</li>
</ol>
<p>The first two are standard counterexamples in topology: the <a href="http://en.wikipedia.org/wiki/Lexicographic_order_topology_on_the_unit_square">unit square under the dictionary ordering</a> on points, and the <a href="http://en.wikipedia.org/wiki/Long_line_%28topology%29">long line</a>.</p>
<p>Aronszajn chains need a delicate construction involving orderings on infinite sets of rational numbers. Not only is the whole ordering not representable by real &nbsp;numbers, not even any uncountable subset is representable.</p>
<p>It's impossible to say anything about useful what Souslin chains look like; their existence or otherwise is not decidable by basic axioms of set theory (ZFC$\pm$CH)</p>
<p>Because the counterexamples are weird, it's hard to come up with intuitive conditions that rule them out. However, I found a <a href="http://books.google.co.nz/books/about/Representations_of_preference_orderings.html?id=xZFFAAAAYAAJ&amp;redir_esc=y">book</a> by Bridges &amp; Mehta (and found that Douglas Bridges is <strike>just down the road in Hamilton</strike>,ok, at least in the same country), with basically the result I need (Theorem 4.6.3)</p>
<p><em>If $\preceq$ is a continuous, countably bounded, total preorder on a path-connected topological space, there is a continuous order isomorphism into $\mathbb{R}$.</em></p>
<p>Mathematicians can be useful, sometimes, even if it's just to solve problems basically created by mathematical formalism.</p>]]></content:encoded>
    <wp:post_name>108126996681</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/107969336091</link>
    <pubDate>Tue, 13 Jan 2015 20:31:00 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/107969336091</guid>
    <!--<wp:post_id>107969336091</wp:post_id>-->
    <wp:post_date>2015-01-12 23:31:00</wp:post_date>
    <wp:post_date_gmt>2015-01-13 07:31:00</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>New header picture</title>
    <description></description>
    <content:encoded><![CDATA[<p>As you can see, there's a new header picture to replace the generic tumblr theme. &nbsp;It's a pair of Superb Fairywrens, aka blue wrens (<em>Malurus cyaneus)</em>, one of my favourite Melbourne birds. They're now quite common along the Melbourne coastline from St Kilda down along the bay.</p>
<p>For people in the Northern hemisphere: these are completely unrelated to the wrens you are familiar with, and are also unrelated to the New Zealand wrens.&nbsp;The similarities in appearance and behaviour are due to convergent evolution; the fairywrens and emu-wrens of Australia are most closely related to other Australian birds such as wattlebirds and honeyeaters.&nbsp;The closest common ancestors for the three types of 'wren' are also common ancestors for all songbirds.&nbsp;</p>
<p>The picture was drawn by NZ artist&nbsp;<a href="http://www.giselledraws.com/">Giselle Clarkson</a>,who mostly does <a href="http://thewireless.co.nz/articles/up-with-the-birds-of-rnz">New Zealand birds</a>&nbsp;and fish, but was willing to branch out.&nbsp;</p>]]></content:encoded>
    <wp:post_name>107969336091</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/107961909786</link>
    <pubDate>Tue, 13 Jan 2015 18:31:58 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/107961909786</guid>
    <!--<wp:post_id>107961909786</wp:post_id>-->
    <wp:post_date>2015-01-12 21:31:58</wp:post_date>
    <wp:post_date_gmt>2015-01-13 05:31:58</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Tomato, tomato</title>
    <description></description>
    <content:encoded><![CDATA[<p>There are two<sup>*</sup> great commandments for conference session chairs</p>
<ol>
<li>You shall adhere to the schedule with all your heart and all your mind and all your strength</li>
<li>You shall pronounce the speakers' names approximately as they do themselves</li>
</ol>
<p>For show-biz award ceremonies the first commandment doesn't apply, but the second <a href="http://abcnews.go.com/Entertainment/2015-golden-globes-ricky-gervais-mispronounces-quvenzhane-walliss/story?id=28157683">still does</a>.</p>
<p>In order to pronounce someone's name correctly, you need to ask for the correct pronunciation and have some way of remembering it, such as writing it down phonetically. &nbsp;</p>
<p>Asking is necessary -- either asking the speaker, or in the case of someone famous, perhaps asking one of her friends or colleagues. &nbsp;The problem isn't so much with straightforward names such as&nbsp;<a href="http://en.wikipedia.org/wiki/Einojuhani_Rautavaara">Einojuhani Rautavaara</a>, or&nbsp;<a href="http://en.wikipedia.org/wiki/Mahela_Jayawardene">Mahela Jayawardene</a>, or&nbsp;<a href="http://en.wikipedia.org/wiki/Kamehameha_I">Kalani Paiʻea Wohi o Kaleikini Kealiʻikui Kamehameha o ʻIolani i Kaiwikapu kauʻi Ka Liholiho Kūnuiākea</a>. These&nbsp; you could maybe work out on your own, with a bit of time for research. The problem is names like MacKay (/mɨ'kai/ or&nbsp;/mə'kei/)&nbsp;and Cai (/kai/ or /tsʰai/) that are genuinely ambiguous.&nbsp;</p>
<p><sup>*</sup><small>and for the Joint Statistical Meetings, the third great commandment: you shall not chair any other session at the meeting, even in an emergency.</small></p>]]></content:encoded>
    <wp:post_name>107961909786</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/107772403471</link>
    <pubDate>Sun, 11 Jan 2015 21:12:01 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/107772403471</guid>
    <!--<wp:post_id>107772403471</wp:post_id>-->
    <wp:post_date>2015-01-11 0:12:01</wp:post_date>
    <wp:post_date_gmt>2015-01-11 08:12:01</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Different questions can have different answers</title>
    <description></description>
    <content:encoded><![CDATA[<p>The Slate Money <a href="http://www.slate.com/articles/podcasts/slate_money.html">podcast</a><sup>1</sup> had an item on Manhattan apartment prices. The mean price last quarter was \$1.7 million and the median was \$0.98 million.&nbsp;</p>
<p>Firstly, that's a lot of money. Secondly, the mean is a lot bigger than the median.&nbsp;The real point, though, is that the mean is a record, up on &nbsp;the previous peak (in 2008) by \$120,000. The median is down from 2008, by \$15,000.</p>
<p>Suppose someone asks whether Manhattan apartments are more expensive now than in 2008. There isn't a simple answer. The mean is up, the median is down, other summary statistics could also go either way.&nbsp;</p>
<p>You could try arguing that because the distribution is skewed, the mean is inappropriate. Unfortunately, this is just wrong. Non-normal distributions still have means, and there can be perfectly good reasons to be interested in them. For example, the total property tax and transfer tax collected by the city government are proportions of the market value and so depend on the mean, not the median.&nbsp;</p>
<p>There isn't any simple fact as to whether apartments are more or less expensive than in 2008. Some are more expensive. Some are less expensive. Some apartments that exist now didn't exist in 2008. Some that existed in 2008 don't exist now. &nbsp;</p>
<p>Different uses of the data call for summarising the prices in different ways. You can get different answers to different questions, and that's fine.&nbsp;But if there isn't a simple, unique, objective answer when your data are &nbsp;measured essentially without error on an interval scale, you've got no hope that there will be a&nbsp;simple, unique, objective answer for merely ordinal measurements. &nbsp;</p>
<p><span>An ordering on measurements is not enough to give you an ordering on distributions. That's true in theory, and it's even true in practice.&nbsp;</span></p>
<p></p>
<p></p>
<p><em>1. I know, but it's much better than it sounds. Honestly.</em></p>]]></content:encoded>
    <wp:post_name>107772403471</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/106956046171</link>
    <pubDate>Sat, 03 Jan 2015 14:07:00 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/106956046171</guid>
    <!--<wp:post_id>106956046171</wp:post_id>-->
    <wp:post_date>2015-01-02 17:07:00</wp:post_date>
    <wp:post_date_gmt>2015-01-03 01:07:00</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Variation explained and log transformation</title>
    <description></description>
    <content:encoded><![CDATA[<p>This post is technical details for one at <a href="http://www.statschat.org.nz/2015/01/03/cancer-isnt-just-bad-luck/">StatsCha</a>t on the Johns Hopkins "two-thirds of cancer is bad luck" paper.</p>
<p>I don't have any real opinions on the conclusion: it's clear that unforced errors &nbsp;in DNA copying will cause some cancers, and it's not obvious how many. &nbsp;The technical problem with the paper (or at least with its publicity) is that the 'proportion of variation explained' was estimated for log risk and quoted as "two-thirds of cancers are due to bad luck'.</p>
<p>This is the graph, with the impressive linear relationship,&nbsp;</p>
<p><img alt="image" src="https://31.media.tumblr.com/7cf5788e021fa9853e7295d8168468c6/tumblr_inline_nhkskorxQ81s1hdxy.jpg" />&nbsp;</p>
<p>I downloaded the supplementary material and extracted the data from the (ugh) multi-page PDF table (thanks, <a href="http://tabula.technology/">Tabula</a>), and I agree with the researchers</p>
<pre>&gt; with(vogel, cor(log(incidence),log(divcum)))
[1] 0.8039453 </pre>
<p>The proportion of variation explained in that graph is $0.804^2=0.65$, but the y-axis variable in that graph isn't cancer risk. It's log cancer risk. &nbsp;As you can see visually, the log transformation makes the variation about the same in rare cancers as common ones. However, from a <strong>population</strong> point of view, a two-fold variation in a common cancer is much more important than a two-fold variation in a rare cancer.</p>
<p>There are at least two ways around this. &nbsp;The first is to work out the implied power-curve model for risk. &nbsp;The slope in the log-log plot is very close to 0.5, so this suggests looking at the correlation between incidence and square root of stem cell divisions</p>
<pre>&gt; with(vogel, cor(incidence,sqrt(divcum)))
[1] 0.5968341
&gt; with(vogel, cor(incidence,sqrt(divcum)))^2
[1] 0.3562109
</pre>
<p>That's what I did on StatsChat. &nbsp;I also searched a bit to see if you could do better than a square root transformation, but without success.</p>
<p>The other thing you could do is a weighted analysis. &nbsp;From the delta method, we can see that the variance of $\log(\textrm{incidence})$ is the variance of $\textrm{incidence}$ divided by&nbsp;<span>$\textrm{incidence}^2$. &nbsp;If we weight the residuals by incidence, we approximately cancel out the effect of the transformation and get a proportion of incidence explained.</span></p>
<pre>&gt; summary(lm(log(incidence)~log(divcum),data=vogel,weights=incidence))

Call:
lm(formula = log(incidence) ~ log(divcum), data = vogel, weights = incidence)

Weighted Residuals:
     Min       1Q   Median       3Q      Max 
-0.59149 -0.22083 -0.05411 -0.01917  0.69178 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -14.8822     2.9132  -5.108 1.88e-05 ***
log(divcum)   0.5107     0.1063   4.806 4.35e-05 ***
---
Signif. codes:  0 &lsquo;***&rsquo; 0.001 &lsquo;**&rsquo; 0.01 &lsquo;*&rsquo; 0.05 &lsquo;.&rsquo; 0.1 &lsquo; &rsquo; 1

Residual standard error: 0.254 on 29 degrees of freedom
Multiple R-squared:  0.4433,	Adjusted R-squared:  0.4241 
</pre>
<p>This approach gives 44% rather than 36%, but that's reasonable agreement for the approximation over such a large range of values. Using $\sqrt(\textrm{cell divisions})$ as the weight gives 0.43.</p>
<p>[On top of this, as I said on StatsChat, the familial colorectal cancers shouldn't really be in the regression if you are interested in population risk explained.]</p>]]></content:encoded>
    <wp:post_name>106956046171</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/106114967946</link>
    <pubDate>Thu, 25 Dec 2014 17:30:18 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[photo]]></category>
		<category domain="category" nicename="photo"><![CDATA[photo]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/106114967946</guid>
    <!--<wp:post_id>106114967946</wp:post_id>-->
    <wp:post_date>2014-12-24 20:30:18</wp:post_date>
    <wp:post_date_gmt>2014-12-25 04:30:18</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title></title>
    <description></description>
    <content:encoded><![CDATA[<div class="figure"><figure>
  <img src="http://78.media.tumblr.com/0859a9abbd447bb759df6c1a8dfd37da/tumblr_nh4f6i3NTW1sueztxo1_1280.jpg" alt="">
</figure></div>

    <p>Christmas Day, Narrow Neck beach, Auckland</p>]]></content:encoded>
    <wp:post_name>106114967946</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/105949935391</link>
    <pubDate>Tue, 23 Dec 2014 22:13:35 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/105949935391</guid>
    <!--<wp:post_id>105949935391</wp:post_id>-->
    <wp:post_date>2014-12-23 1:13:35</wp:post_date>
    <wp:post_date_gmt>2014-12-23 09:13:35</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>How not to treat Ebola</title>
    <description></description>
    <content:encoded><![CDATA[<p>From the <em><a href="http://www.theguardian.com/world/2014/dec/22/ebola-untested-drug-patients-sierra-leone-uk-staff-leave">Guardian</a>,</em> via Mark Henderson of the Wellcome Trust</p>
<blockquote>
<p><em>Ebola patients at a treatment centre in Sierra Leone have been given a heart drug that is untested against the virus in animals and humans, a move that has been deemed reckless by one senior scientist and has prompted UK medical staff at the centre to leave.</em></p>
</blockquote>
<p>Ebola is a problem for drug testing. You don't want to leave people untreated, but you do want to find out as fast as possible what works.</p>
<p>I think the following are ethical</p>
<ul>
<li>If you have a candidate treatment with really good support from precliinical studies and that's available in volume, do a randomised-rollout (stepped-wedge) trial. The idea is that you can't get the drug to everyone at the same time, so you might as well choose the first recipients randomly rather than choosing the district where the Minister of Health's family lives.</li>
<li>If you have a really good candidate but can't supply it in volume (like the antibody-based treatments used on some Western medical personnel) you might as well randomise who gets the limited supply. This was the rationale for the first modern clinical trial (of streptomycin in tuberculosis) -- that a lottery is better than allocating the limited supply based on wealth or influence (assuming there isn't enough wealth around to have a useful effect on the supply). You could randomise all available patients or limit it to medical or nursing personnel, and I'm glad that isn't my decision.</li>
<li>If you have several candidates that are about equally good, randomise which ones go to which patient, so you learn which ones (if any) work.</li>
</ul>
<p>The drug in Guardian story is <a href="http://en.wikipedia.org/wiki/Amiodarone">amiodarone</a>, which is just being given to people at one clinic.&nbsp;Not only is this not tested in humans or animals, it's not primarily an antiviral and it's a seriously nasty drug. It stays in the body for weeks after you stop taking it; it inhibits the liver enzyme cytochrome P450 3A4, so it has interactions with basically <em>everything</em>; and it has numerous unpleasant side-effects.</p>
<p>Amiodarone is still on the market because it's effective for some heart rhythm problems that can't be treated any other way, but you'd want good evidence before you decided it was the preferred treatment in a population of people whose lives depend on flexible and innovative medical care.</p>
<p>Even if amiodarone <em>did</em> turn out to be the miracle anti-Ebola drug, the way it's being used would delay its widespread adoption and cost lives.&nbsp;</p>]]></content:encoded>
    <wp:post_name>105949935391</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/105251907836</link>
    <pubDate>Mon, 15 Dec 2014 20:18:00 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/105251907836</guid>
    <!--<wp:post_id>105251907836</wp:post_id>-->
    <wp:post_date>2014-12-14 23:18:00</wp:post_date>
    <wp:post_date_gmt>2014-12-15 07:18:00</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Citations: credit or blame</title>
    <description></description>
    <content:encoded><![CDATA[<p>Katie Hinde at 'Mammals Suck' <a href="http://mammalssuck.blogspot.co.nz/2013/12/mothers-milk-literature-sleuths-and.html">writes</a></p>
<blockquote>
<p><em>Only cite papers that you have read! DO NOT cite papers based on another publication&rsquo;s report of them. Because every time that happens, a science fairy dies.</em></p>
</blockquote>
<p><span>That's an excellent principle. So why do I have a paper in press that cites a paper I haven't read?</span></p>
<p><span>There are two reasons to cite a paper: as evidence for a claim, or to give credit to the authors for their research. In this case, I wanted to acknowledge that Takeuchi, in 1976, had come up with a modification of Akaike's Information Criterion for possibly misspecified models. Our proposed criterion reduces to Takeuchi's criterion under iid sampling.&nbsp;</span></p>
<p><span>Takeuchi wrote in Japanese. Not surprisingly, my campus library doesn't stock the journal. I could probably get it via interlibrary loan, but to say I don't read Japanese would be a massive understatement.&nbsp;</span></p>
<p><span>When you're only citing a paper for credit there's less risk. It's still not ideal -- eg, Lakatos pointed out in the second section of<em>&nbsp;Proofs and Refutations</em> that lots of people attribute the concept uniform convergence to mathematicians who really just failed to notice it was an assumption -- but it's not going to lead to huge gaps in your arguments. &nbsp;Our arguments rely on fairly simple properties of quadratic forms, and we developed them ourselves. I'm not citing to shift the blame to Takeuchi if he was wrong, I'm crediting him for having (partly) got there first.</span></p>
<p><span>I feel reasonably safe with the reference, since I've seen it cited in two publications by Japanese researchers, and my original source in English was a book co-authored by Nils Lid Hjort, who's careful about that sort of thing. </span></p>
<p><span>Still, I'm citing a paper I haven't read.&nbsp;</span></p>]]></content:encoded>
    <wp:post_name>105251907836</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/104662047986</link>
    <pubDate>Mon, 08 Dec 2014 22:25:35 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/104662047986</guid>
    <!--<wp:post_id>104662047986</wp:post_id>-->
    <wp:post_date>2014-12-08 1:25:35</wp:post_date>
    <wp:post_date_gmt>2014-12-08 09:25:35</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>What science should everyone know?</title>
    <description></description>
    <content:encoded><![CDATA[<p>In response to the question "<span>How much science knowledge should the average person have or should we just encourage people to ask questions?"</span></p>
<blockquote class="twitter-tweet">
<p><a href="https://twitter.com/NaomiShadbolt">@NaomiShadbolt</a> <a href="https://twitter.com/petergnz">@petergnz</a> Basics: Atoms; Evolution; "the lights in the sky are suns"; Randomisation; Conservation laws. And ask questions.</p>
&mdash; Thomas Lumley (@tslumley) <a href="https://twitter.com/tslumley/status/541879919567831042">December 8, 2014</a></blockquote>
<p>
<script charset="utf-8" src="//platform.twitter.com/widgets.js" type="text/javascript"></script>
</p>
<p>Expanding on this:</p>
<ul>
<li>Atoms: everything is made of a very large but not infinite number of definite, basically indivisible, pieces, and there are very few different types (about 100).</li>
<li>Evolution: Complex goal-oriented behaviour in the world mostly came by repeated competition to choose the best-performing of a random set of variations. That's how life developed from single cells: it isn't necessary to have intelligent choice or design.&nbsp;</li>
<li>"the lights in the sky are suns". The stars are the same kind of thing as the sun, but very, very far away, so they appear tiny.</li>
<li>Randomisation: The only really reliable way to get a fair comparison between two strategies is random choice of strategy for a large enough set of examples</li>
<li><span>Conservation laws: Many important facts about physics are in the for "the amount of X doesn't change", where X might be energy or mass or momentum or spin or charge. There are deep mathematical reasons for this.</span></li>
</ul>
<p>And, above all, the question "Why do you believe this?" is always in order.&nbsp;</p>]]></content:encoded>
    <wp:post_name>104662047986</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/104033804496</link>
    <pubDate>Mon, 01 Dec 2014 14:50:00 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/104033804496</guid>
    <!--<wp:post_id>104033804496</wp:post_id>-->
    <wp:post_date>2014-11-30 17:50:00</wp:post_date>
    <wp:post_date_gmt>2014-12-01 01:50:00</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>It depends on what you mean by 'cost'</title>
    <description></description>
    <content:encoded><![CDATA[<p>The Tufts Center for the Study of Drug Development has a new cost estimate out:&nbsp;<a href="http://csdd.tufts.edu/news/complete_story/pr_tufts_csdd_2014_cost_study">Cost to Develop and Win Marketing Approval for a New Drug Is $2.6 Billion</a>.</p>
<p>The figure is probably fairly accurate as an estimate of what it's trying to estimate, but it gets quoted in other contexts, so I think it's worth looking at &nbsp;the number a piece at a time.&nbsp;The Tufts researchers haven't provided enough information to do this, so I'm relying on estimates from <a href="http://lifescivc.com/2014/11/a-billion-here-a-billion-there-the-cost-of-making-a-drug-revisited/">Bruce Booth</a>&nbsp;(who also has a spreadsheet that you can use for sensitivity analyses).</p>
<p>There are three important issues. First is 'new drug'. This is for a genuinely new drug which needs Phase III testing on real clinical outcomes, with no short-cuts available.&nbsp;</p>
<p>The second issue is that the cost includes one successful drug and the average number of failures that go with it. The cost for one completely new drug is about \$400 million, but only about 4% of discovery projects produce a new drug. Fortunately, a lot of them fail relatively early and inexpensively, but adding in the cost of the 24 failures takes you from \$400 million to about \$1.4 billion.</p>
<p>The \$1.4 billion cost for a drug is like saying the cost of a lottery ticket is $15 million, because that's what it would cost, on average, to buy enough lottery tickets to get the big prize. &nbsp;I think it's reasonable to include the cost of failures, but you do then have to remember they are included. You can't say, for example, "It costs \$1.4 billion to develop a new drug, and most of them fail." The fact that most of them fail is already priced in.&nbsp;</p>
<p>Finally, the \$2.6 billion headline cost includes a cost of capital at 11%/year. This one, I think, is cheating when used in non-technical communication. Opportunity costs or borrowing costs are certainly real, but they are basically never included in the 'cost' figure by normal people. &nbsp;If you buy a house for \$500,000 (as you do in Auckland) you say the cost is \$500,000, not the \$1.25 million after mortgage interest or the even larger loss compared to not buying index funds with the money. If you're talking to investment bankers or governments you might want to include the cost of funds, but not when you are giving a number to be used by lay people.</p>
<p></p>]]></content:encoded>
    <wp:post_name>104033804496</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/102261453686</link>
    <pubDate>Mon, 10 Nov 2014 20:52:32 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/102261453686</guid>
    <!--<wp:post_id>102261453686</wp:post_id>-->
    <wp:post_date>2014-11-09 23:52:32</wp:post_date>
    <wp:post_date_gmt>2014-11-10 07:52:32</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Log determinants</title>
    <description></description>
    <content:encoded><![CDATA[<p>The Gaussian loglikelihood involves the log of the determinant of the covariance matrix. &nbsp;That's a problem in at least two settings</p>
<ul>
<li>the matrix is large and sparse, so the determinant dominates the computation</li>
<li>complex sampling, where it's not clear how to get the sampling probabilities into the matrix</li>
</ul>
<p>I learned by reading&nbsp;<a href="http://xcorr.net/2011/10/06/approximate-log-determinant-of-huge-matrices/">Patrick Mineault</a>&nbsp;that&nbsp;there's a power series for the log determinant, and, importantly, it is a power series where the terms are <em>expectations</em> of&nbsp;<em>quadratic forms</em>. We know how to put sampling weights into quadratic forms, so we should be able to estimate the log determinant using only pairwise sampling probabilities!</p>]]></content:encoded>
    <wp:post_name>102261453686</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/101996757051</link>
    <pubDate>Fri, 07 Nov 2014 21:29:23 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/101996757051</guid>
    <!--<wp:post_id>101996757051</wp:post_id>-->
    <wp:post_date>2014-11-07 0:29:23</wp:post_date>
    <wp:post_date_gmt>2014-11-07 08:29:23</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Being wrong isn't the worst part</title>
    <description></description>
    <content:encoded><![CDATA[<p>Stuff, the website for the Fairfax media in NZ, <a href="http://www.stuff.co.nz/national/quizzes/10711658/Quiz-Are-you-smarter-than-a-15-year-old">had a piece today</a> "<strong>Quiz: Are you smarter than a 15-year-old?"</strong></p>
<blockquote>
<p><em>For a taste of what the 143,000 teenagers sitting NCEA and scholarship tests are facing, we've prepared this quiz based on questions from previous years' level 1 maths, English, science, history, geography and classics papers.</em></p>
</blockquote>
<p>The first thing I noticed was that two of the physics/maths questions were wrong, because they mixed up the units for acceleration and speed. These were later changed, to be differently wrong, before finally being fixed. &nbsp;I initially missed the error in a third question, about Pythagoras' Theorem:<img src="https://31.media.tumblr.com/24945b9cc265fcf63ffb64c9221f1dea/tumblr_inline_nenszbKm0e1s1hdxy.png" /></p>
<p></p>
<p>and this is still wrong at the time of writing (<em>right-angled</em> triangles only). &nbsp;</p>
<p>I think the other 17 answers are correct, but that's not the biggest issue. The questions with correct answers are all like this one: simple memorisation. Only the two incorrect physics questions required even simple calculation. &nbsp;I don't know what the NCEA exams are like, but I really hope they aren't like this.</p>
<p>The optimistic hypothesis is that the simple memory questions are the result of the multiple-choice format. That still indicates a misunderstanding of multiple choice by the journalists -- there's no reason to avoid higher-level thinking in multiple-choice formats -- but it's a fairly common misunderstanding.</p>
<p>Back in 2007 I helped some biology education researchers with their statistical analysis. They were comparing the level of questions on the Medical College Admission Test (MCAT) with undergraduate and medical-school exams. Because MCAT is all multiple-choice, there's a myth that it's all memorisation -- as the researchers wrote (<a href="http://www.sciencemag.org/content/319/5862/414.full">paywalled</a>)</p>
<blockquote>
<p><em>The Medical College Admission Test (MCAT) has been accused of hindering efforts to introduce more critical thinking into introductory biology courses, and the Advanced Placement (AP) Biology course has come under fire for stressing rote memorization</em></p>
</blockquote>
<p>They had looked at a sample of questions from MCAT, AP Biology, undergraduate biology exams, and first-year medical school exams. The questions were graded according to <a href="http://en.wikipedia.org/wiki/Bloom's_taxonomy">Bloom's Taxonomy</a>, and this graph shows the results</p>
<p><img src="https://31.media.tumblr.com/72fe400c23858796c5dec77f74ec3341/tumblr_inline_nentocZpV01s1hdxy.gif" /></p>
<p>The MCAT, which is all multiple-choice, had the fewest pure-memorisation questions; the first-year medical school exams had the most.&nbsp;When questions were weighted in proportion to the number of marks, the average Bloom's Taxonomy rating was similar for the first four exam types and lower for first-year medical school.&nbsp;</p>
<p>Multiple choice questions don't have to be simple memorisation. They can test higher-order thinking skills, and for being "smarter than a 15-year old" to have any higher ambition than clickbait, they needed to.</p>
<p></p>]]></content:encoded>
    <wp:post_name>101996757051</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/101914240361</link>
    <pubDate>Thu, 06 Nov 2014 21:22:00 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/101914240361</guid>
    <!--<wp:post_id>101914240361</wp:post_id>-->
    <wp:post_date>2014-11-06 0:22:00</wp:post_date>
    <wp:post_date_gmt>2014-11-06 08:22:00</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>This is just to say</title>
    <description></description>
    <content:encoded><![CDATA[<p><em>The plums, which you stored there on ice,</em><br /><em>I have eaten; they went in a trice.</em><br /><em>If you meant them to last</em><br /><em>For a morning repast</em><br /><em>Then I'm sorry, but boy were they nice.</em></p>
<p>or</p>
<p><em>Some say the plums will end in tarts</em><br /><em>Some say on ice</em><br /><em>From what I've eaten 'round these parts</em><br /><em>I hold with those who favor tarts</em><br /><em>But if they had to vanish first</em><br /><em>I think I know enough of guilt</em><br /><em>To say that for consumption, chilled</em><br /><em>is also good, and would suffice</em></p>
<p>or</p>
<p><em><span>plums by here, my dear, plums by here</span><br /><span>plums by here, my dear, plums by here</span><br /><span>plums by here, my dear, plums by here</span><br /><span>In the icebox, plums by here</span><br /><br /><span>Someone's hungry, dear, plums by here</span><br /><span>Someone's hungry, dear, plums by here</span><br /><span>Someone's hungry, dear, plums by here</span><br /><span>In the icebox, plums by here</span><br /><br /><span>Cold and sweet, my dear, plums by here</span><br /><span>Cold and sweet, my dear, plums by here</span><br /><span>Cold and sweet, my dear, plums by here</span><br /><span>In the icebox, plums by here</span><br /><br /><span>Someone's sorry, dear, no plums here</span><br /><span>Someone's sorry, dear, no plums here</span><br /><span>Someone's sorry, dear, no plums here</span><br /><span>No breakfast, no plums here</span></em></p>
<p></p>
<p>[Update: after <a href="http://en.wikipedia.org/wiki/This_Is_Just_To_Say">Williams</a>, of course]</p>]]></content:encoded>
    <wp:post_name>101914240361</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/101910586331</link>
    <pubDate>Thu, 06 Nov 2014 19:46:31 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/101910586331</guid>
    <!--<wp:post_id>101910586331</wp:post_id>-->
    <wp:post_date>2014-11-05 22:46:31</wp:post_date>
    <wp:post_date_gmt>2014-11-06 06:46:31</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>A people set apart</title>
    <description></description>
    <content:encoded><![CDATA[<p>There's a <a href="http://www.deakin.edu.au/health/psychology/edconference/">conference</a> coming up at Deakin University in Melbourne, on energy drinks. The unusual aspect of the conference is that no-one who has received industry funding is welcome. Obviously the energy drink industry <a href="http://australianbeverages.org/industry-banned-attending-1st-international-energy-drinks-conference/">aren't happy</a> about this. &nbsp;I couldn't give a fsck about their hurt feelings, but I hope this sort of policy doesn't spread.</p>
<p>Now, I'm not completely na&iuml;ve about the sorts of things some industry groups will do when there's a lot of money at stake. People in New Zealand will have heard the allegations in Nicky Hager's <em>Dirty Politics</em>&nbsp;about smear campaigns against Doug Sellman and Mike Joy, but I knew about the problem long before that. I have<a href="http://www.cbgnetwork.org/1397.html"> friends and colleagues</a> from Seattle who were the target of pharmaceutical industry dirty tricks over a paper on the safety of some (then new and expensive) blood pressure drugs. At a less serious but more personal scale, I've heard reports of what some sales personnel for Mathsoft used to say about R and its developers. &nbsp;</p>
<p>Still, I hope this sort of exclusionary policy doesn't spread. &nbsp;It's fine to organise a workshop by invitation, for people who agree with you on some issue to work on the next step. I've been part of those myself. It's acceptable to exclude non-researchers from research meetings, and I'm entirely in favour of throwing out individuals who persist in being disruptive, but it's a big step to say that anyone who has received research funding from the energy drink industry is<em> ipso facto</em> a fraud.</p>
<p>The problem is particularly bad in research areas where the signal to noise ratio is very low, so that it's easy to miss important problems with your results. I rather like the approach for air pollution epidemiology represented by the<a href="http://www.healtheffects.org/"> Health Effects Institute</a>. It was a joint venture by the EPA and the polluting industries to come up with research that addressed the public policy issues and that people could believe, including the most useful peer-review processes I've encountered. It was necessary: although the polluters turned out to be wrong on all the substantive issues, they were right about the quality of some of the research. &nbsp;I met some first-class scientists in air pollution research (notably, the late <a href="http://deohs.washington.edu/sites/default/files/newsletter/EHnews-SprSum2011/memoriam.html">Sally Liu</a>), but there were also people who knew what the answer should be and wanted to get it.</p>
<p>I've never received or sought industry research funding, and have done a very limited amount of consulting for them (statistical software courses). I'd be allowed into meetings like this one, but I'd be reluctant to go.</p>]]></content:encoded>
    <wp:post_name>101910586331</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/100951740051</link>
    <pubDate>Sun, 26 Oct 2014 13:30:00 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/100951740051</guid>
    <!--<wp:post_id>100951740051</wp:post_id>-->
    <wp:post_date>2014-10-25 17:30:00</wp:post_date>
    <wp:post_date_gmt>2014-10-26 00:30:00</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Miasma and contagion</title>
    <description></description>
    <content:encoded><![CDATA[<p>Scientists have a nasty habit of taking ordinary English words, turning them into technical terms, and then insisting that the ordinary use is Just Wrong. 'Organic', which I've written about before, is a good example.</p>
<p>On the other hand, sometimes the scientists are right. I complained on Twitter last night about the phrase '<em>meningococcal virus'</em> in a Herald <a href="http://www.nzherald.co.nz/politics/news/article.cfm?c_id=280&amp;objectid=11347878">opinion piece</a> on state housing, and I have previously complained about the 'Psa virus' &nbsp;for the bacterium&nbsp;<em>Pseudomonas syringae </em>pv<em>. actinidiae.&nbsp;</em></p>
<p>I checked the Oxford English Dictionary, which reports a general meaning of 'virus', but says it's out of date</p>
<blockquote>
<p><em>a. Med. Originally: pus or other discharge produced by an ulcer or wound. Later: a substance produced within the body by a disease, esp. when contagious or infectious or used for vaccination; (also) any agent causing an infectious disease. Now hist. except in the restricted sense 2b.</em></p>
</blockquote>
<p>For the OED to decide a usage is <em>hist</em>, it's got to really be <em>hist.&nbsp;</em>It's not just sleeping; it's an ex-usage.&nbsp;</p>
<p>The distinction between viruses and bacteria seems to have become part of English in the 1940s, about the time antibiotics were becoming available. That's probably no coincidence: antibiotics kill bacteria but not viruses, they cure scarlet fever but not measles, so they made the bacteria/virus distinction relevant to everyone.&nbsp;</p>
<p>Antibiotics are the reason the distinction is even more important today. New Zealand has a problem of both underuse and overuse of antibiotics. Some kids aren't getting penicillin for strep throat, and a few of them end up with rheumatic fever. On the other hand, some people are still getting antibiotics for viral infections or other situations where they are not helpful.&nbsp;</p>
<p>'Meningococcal virus' is a particularly unfortunate confusion: meningococci are bacteria, and antibiotic treatment is a matter of the greatest possible urgency in suspected meningococcal disease.&nbsp;</p>
<p></p>]]></content:encoded>
    <wp:post_name>100951740051</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/100893932596</link>
    <pubDate>Sat, 25 Oct 2014 20:53:00 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/100893932596</guid>
    <!--<wp:post_id>100893932596</wp:post_id>-->
    <wp:post_date>2014-10-25 0:53:00</wp:post_date>
    <wp:post_date_gmt>2014-10-25 07:53:00</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Semiparametric efficiency and nearly-true models</title>
    <description></description>
    <content:encoded><![CDATA[<p>Suppose you have $N$ people with some variables measured, and you choose a subset of $n$ to measure additional variables. &nbsp;I'm going to assume the probability $\pi_i$ that you measure the additional variables on person $i$ is&nbsp;<strong>known</strong>, so it has to be a setting where non-response isn't an issue -- eg, choosing which frozen blood samples to analyse, or which free-text questionnaire responses to code, or which &nbsp;medical records to pull for abstraction.&nbsp;<span>As an example, if you have a binary outcome $Y$ you might take a case--control sample and measure $X$ on everyone with $Y=1$ and the same number of people with $Y=0$.&nbsp;</span></p>
<p>Suppose in addition that you want to fit a particular parametric or semiparametric model ${\cal P}_{\theta,\eta}$ to the data, where $\theta$ are parameters of interest and $\eta$ are nuisance parameters. For example, you might want to fit a logistic regression model where the coefficients are $\theta$ and the density of $X$ is $\eta$.</p>
<p>There are now two possible semiparametric models for the observed data. Let $R_i$ be the indicator that person $i$ is sampled. We could have</p>
<ul>
<li>Model D: $\pi_i=E[R_i|\textrm{variables available on everyone}]$</li>
<li>Model M: the submodel of $D$ that satisfies&nbsp;${\cal P}_{\theta,\eta}$</li>
</ul>
<p>Typically, estimation under model M will be more efficient. For example, in the case-control setting with a logistic regression model for $Y|X$ we know that the efficient estimator under model M is unweighted logistic regression &nbsp;(per Prentice &amp; Pyke 1979), and that the efficient estimator under model D is weighted logistic regression with weights $w_i=1/\pi_i$.</p>
<p>I want to consider slight misspecifications, where model M is 'nearly true'. Gross misspecifications aren't interesting: if the data don't look anything like a sample from&nbsp;${\cal P}_{\theta,\eta}$, a careful data analyst will notice and pick a different model. However, the difference between the efficient estimators under M and under D is $O_p(n^{-1/2})$, so a bias of the same order is enough to outweigh the precision gain. It's not obvious that we should expect to detect a misspecification of this size, so more precise investigation is needed.</p>
<p></p>
<p>The efficient estimator under $D$ is an Augmented Inverse Probability Weighted (AIPW) estimator (if you're a biostatistician) or a calibration estimator (if you're a survey statistician), and we can get reasonably close to it (Breslow et al, 2009). Write $\hat\theta_{\textrm{wtd}}$ for this estimator, and $\hat\theta_{\textrm{eff}}$ for the efficient estimator under $M$.&nbsp;</p>
<p><span>Models M and D agree when there is complete data, so I will&nbsp;</span><strong>define</strong><span>&nbsp;the true value $\theta_0$ of $\theta$ as the common limit of $\hat\theta_{eff}$ and $\hat\theta_{wtd}$ with complete data. Survey statisticians call this the 'census estimator.' Biostatisticians call it 'our next grant proposal'.</span></p>
<p>We now need a mathematical characterisation of 'nearly true'. I will use contiguity. &nbsp;A sequence of distributions $Q_n$ is contiguous to a sequence $P_n$ if for every event $A$, $P_nA\to0$ implies $Q_nA\to 0$. They are mutually contiguous if the implication goes both ways. Let $A$ be the event that a model diagnostic accepts model $M$, and let $P_n$ be a sequence of distributions in model M. &nbsp;If this is a useful diagnostic, &nbsp;$P_nA\not\to 0$, so for a mutually contiguous sequence of distributions $Q_n$ in model D but not in model M, $Q_nA\not\to 0$.</p>
<p>Now, under M</p>
<p>$\sqrt{n}(\hat\theta_{\textrm{eff}}-\theta_0) \stackrel{d}{\to}N(0,\sigma^2)$</p>
<p>and</p>
<p>$\sqrt{n}(\hat\theta_{\textrm{wtd}}-\theta_0) \stackrel{d}{\to}N(0,\sigma^2+\omega^2)$</p>
<p>By the Convolution Theorem, the extra variance for $\hat\theta_{\textrm{wtd}}$ under model M is pure noise, so&nbsp;</p>
<p>$\sqrt{n}(\hat\theta_{\textrm{eff}}-\hat\theta_{\textrm{wtd}}) \stackrel{d}{\to} N(0,\omega^2)$</p>
<p>Now, by LeCam's Third Lemma, if we switch from $P_n$ to $Q_n$ as the data distribution there is no change in variance, but there is bias</p>
<p>$\sqrt{n}(\hat\theta_{\textrm{eff}}-\hat\theta_{\textrm{wtd}}) \stackrel{d}{\to} N(-\kappa\rho\omega,\omega^2)$</p>
<p>where $\kappa$ is the limit of the log likelihood ratio $\log dQ_n/dP_n$, which governs the power of the Neyman--Pearson Lemma test, and $\rho$ measures whether the misspecification is in a direction that matters for $\theta$ or not.&nbsp;</p>
<p>Substituting back, under the contiguous misspecified model sequence $Q_n$,&nbsp;</p>
<p>$\sqrt{n}(\hat\theta_{\textrm{eff}}-\theta_0) \stackrel{d}{\to}N(-\kappa\rho\omega,\sigma^2)$</p>
<p>and</p>
<p>$\sqrt{n}(\hat\theta_{\textrm{wtd}}-\theta_0) \stackrel{d}{\to}N(0,\sigma^2+\omega^2)$</p>
<p>So, the mean squared error of $\hat\theta_{\textrm{wtd}}$ is lower if $\kappa^2\rho^2&gt;1$. &nbsp;If $\rho\approx 1$, this happens when $\kappa\approx 1$, at which point the most powerful test for $Q_n$ vs $P_n$ has power about 24%.&nbsp;</p>
<p>That is, the least-favourable misspecification of model M leads to worse mean squared error for&nbsp;$\hat\theta_{\textrm{eff}}$ than&nbsp;$\hat\theta_{\textrm{wtd}}$ before the most powerful test of misspecification is even moderately reliable, even if we (unrealistically) knew exactly the form of the misspecification.&nbsp;</p>
<p>Since the sense in which&nbsp;$\hat\theta_{\textrm{eff}}$ is optimal is precisely this local asymptotic minimax sense within&nbsp;${\cal P}_{\theta,\eta}$, it seems reasonable to use the same description of optimality outside the model. Under this description of optimality, the 'efficient' estimator's optimality is not robust to undetectable model misspecification.</p>]]></content:encoded>
    <wp:post_name>100893932596</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/100482894381</link>
    <pubDate>Mon, 20 Oct 2014 19:13:31 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/100482894381</guid>
    <!--<wp:post_id>100482894381</wp:post_id>-->
    <wp:post_date>2014-10-19 23:13:31</wp:post_date>
    <wp:post_date_gmt>2014-10-20 06:13:31</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Broman's Socks and the Nature of Scientific Reporting</title>
    <description></description>
    <content:encoded><![CDATA[<p><img src="https://31.media.tumblr.com/e519b76dfae7d7f7edc9901bc84f8185/tumblr_inline_ndqb1rEm0z1s1hdxy.jpg" /></p>
<p></p>
<p>Rasmus B&aring;&aring;th <a href="http://www.sumsar.net/blog/2014/10/tiny-data-and-the-socks-of-karl-broman/">wrote a post</a> using Approximate Bayesian Computation to estimate a posterior distribution for Karl's socks. What he didn't consider was the impact of publication bias. &nbsp;In order for us to see the tweet, it was not only necessary that Karl's first 11 socks were distinct, it was also necessary that he found this remarkable, and, probably, that no-one he follows on Twitter had made a similar laundry-related observation at any recent time. &nbsp;Now we've seen his socks, other laundry data will face a higher barrier to publication.&nbsp;</p>
<p>I <a href="http://notstatschat.tumblr.com/post/78570650254/my-likelihood-depends-on-your-frequency-properties">wrote about related issues</a> in March, but the basic point is that the likelihood principle doesn't save you, as a reader, from having to model the publication process. Your data are other people's reports, so your likelihood involves their reporting behaviour.</p>
<p>&nbsp;In this particular example, publication bias appears not to be very strong. Based on the actual number of socks involved (21 pairs, 3 singletons), the probability of the first 11 socks being distinct is about 0.23.&nbsp;The relative sparsity of published reports of the sock-uniqueness phenomenon imply that either not many people wash their socks, or that the decision to tweet about laundry is determined by factors other than the level of surprise in the data.&nbsp;</p>
<p></p>
<p></p>
<p>(the reference in the title is to <a href="https://cds.cern.ch/record/142461/files/198009299.pdf" title="Bertlmann's Socks and the Nature of Reality">a paper by John Bell</a> explaining &nbsp;Bell's Inequality and the incompatibility of local hidden variable theories with quantum mechanics. Since the paper was for a philosophy conference, it's fairly accessible)&nbsp;</p>]]></content:encoded>
    <wp:post_name>100482894381</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/98876253936</link>
    <pubDate>Wed, 01 Oct 2014 21:16:00 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/98876253936</guid>
    <!--<wp:post_id>98876253936</wp:post_id>-->
    <wp:post_date>2014-10-01 1:16:00</wp:post_date>
    <wp:post_date_gmt>2014-10-01 08:16:00</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Auckland Transport spy system</title>
    <description></description>
    <content:encoded><![CDATA[<p></p>
<p>[New: The paper <a href="https://www.diffchecker.com/8w6o0ndw">has silently changed its story</a>,&nbsp;It looks like they just went with a misleading Hewlett-Packard press release and that this isn't actually happening]</p>
<p>According to the <a href="http://www.stuff.co.nz/national/10567293/Sophisticated-surveillance-coming-to-Auckland">Fairfax papers</a>, Auckland Transport is about to spend lots of money on a spy system.</p>
<p>I like Auckland Transport. They're one of my favorite government agencies. All the people I've met from AT have been nerds who want to make people's lives better. <span>Their bus and train system has improved dramatically since I first encountered it in 2009.&nbsp;</span>&nbsp;I'm hoping to spend a noticeable fraction of my research time over the next three years working on improved Bayesian bus prediction modelling using their data (with the help of one of our best students). So, I <a href="https://www.google.co.nz/search?q=i+has+a+sad&amp;espv=2&amp;biw=1105&amp;bih=575&amp;tbm=isch&amp;tbo=u&amp;source=univ&amp;sa=X&amp;ei=0bYrVK3gMMek8AXsnIKYAw&amp;ved=0CBsQsAQ">has a sad</a>.&nbsp;</p>
<p>According to the<a href="http://www.campbellcollaboration.org/news_/CCTV_modest_impact_on_crime.php"> systematic review by the Campbell Collaboration</a>,&nbsp;surveillance systems (mostly with human monitoring) are only modestly effective, and mostly in reducing vehicle crimes in car parks. Some of their effect comes from just relocating crime. &nbsp;That doesn't seem a great use of resources.&nbsp;</p>
<p><span>The news story talked about face recognition, which I don't really believe (if you've finally got this to work using surveillance cameras, would you really have the first deployment in somewhere as boring as New Zealand?), but it also talked about number-plate recognition, which is routine technology now.&nbsp;</span></p>
<p>The &nbsp;story also talked about safeguards for the information. It didn't say, for example, that the police would need to get a warrant to use the data. It didn't describe a public consultation process that was used to decide on the purchase. It didn't talk about how the GCSB and other spy agencies would or would not have access.&nbsp;</p>
<p>Recently we heard that AT were going to have steep budget cuts over the next few years, to keep the Mayor's tax-rate promises. That's unfortunate, but perhaps unavoidable. Spending the money on surveillance systems instead of roads, trains, and buses, is entirely avoidable.&nbsp;</p>
<p></p>
<p></p>
<p>[Update: Auckland Transport has a <a href="http://m.nzherald.co.nz/nz/news/article.cfm?c_id=1&amp;objectid=11297578">different view</a> of what the system is for], and they are <a href="https://twitter.com/AklTransport/status/517227114722910208">saying on Twitte</a>r that the papers are completely wrong. It's unclear for now who is a lying liar who lies: neither possibility is all that encouraging]</p>]]></content:encoded>
    <wp:post_name>98876253936</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/98875275696</link>
    <pubDate>Wed, 01 Oct 2014 20:42:02 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/98875275696</guid>
    <!--<wp:post_id>98875275696</wp:post_id>-->
    <wp:post_date>2014-10-01 0:42:02</wp:post_date>
    <wp:post_date_gmt>2014-10-01 07:42:02</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Outsourcing your brain</title>
    <description></description>
    <content:encoded><![CDATA[<p>@Theincentive is the Twitter handle of a very good Irish science communication group. I encountered them through a well-intentioned recommendation from their report on Ireland's "Pint of Science" event</p>
<blockquote>
<p><em>If you think patents suck, follow&nbsp;<a href="https://twitter.com/onetruecathal">@onetruecathal</a></em></p>
</blockquote>
<p>Cathal Garvey is a homebrew biotechnologist, the sort of person who has a (very small) chance of ending up as the Richard Stallman of biotechnology.&nbsp;</p>
<p>The problem is that this is exactly the wrong recommendation. One of the things Twitter is really good for is learning from intelligent people who think differently from you. Unlike Facebook, Twitter is asymmetric: you can listen to people who have no interest in your existence. &nbsp;That's how 52 million people can follow Justin Bieber, but also how thousands or tens of thousand can follow people who give them something to think about.&nbsp;</p>
<p>if you think patents suck, you don't need to follow Cathal. You should follow someone like Derek Lowe, a pharmaceutical industry chemist. It's the people who like patents or who are worried about GMOs who should follow Cathal.&nbsp;</p>]]></content:encoded>
    <wp:post_name>98875275696</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/98337265736</link>
    <pubDate>Thu, 25 Sep 2014 10:00:40 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/98337265736</guid>
    <!--<wp:post_id>98337265736</wp:post_id>-->
    <wp:post_date>2014-09-24 15:00:40</wp:post_date>
    <wp:post_date_gmt>2014-09-24 22:00:40</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Is it good or bad when confounding adjustment makes no difference?</title>
    <description></description>
    <content:encoded><![CDATA[<p>There's a <a href="http://jech.bmj.com/content/early/2014/08/08/jech-2014-204274.abstract?sid=255c088d-fa9a-4538-b8d5-2686b5ca0358">new paper out</a> in <em>J Epi Community Health</em>, looking at the relationship between perceived job insecurity and incident asthma. NHS 'Behind the Headlines' <a href="http://www.nhs.uk/news/2014/09September/Pages/Job-insecurity-may-increase-adult-asthma-risk.aspx">covers it well</a>.</p>
<p>One of the interesting things about the paper is that the crude relative risk between above/below 50% estimated risk of &nbsp;losing your job is 1.61, and the relative risks after adjustment in three increasingly-complex models are 1.58, 1.62, and 1.61. That is, the adjustment for confounding has no impact at all.</p>
<p>I'm never sure whether this is a good or bad sign. In principle it might mean that there just wasn't any confounding, but it could also mean the control for confounding was completely ineffective. &nbsp;There are strong risk factors for incident asthma diagnosis (smoking, going to a doctor), but maybe these just aren't related to job insecurity in Germany?</p>]]></content:encoded>
    <wp:post_name>98337265736</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/96135813116</link>
    <pubDate>Sat, 30 Aug 2014 14:24:00 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/96135813116</guid>
    <!--<wp:post_id>96135813116</wp:post_id>-->
    <wp:post_date>2014-08-29 19:24:00</wp:post_date>
    <wp:post_date_gmt>2014-08-30 02:24:00</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Rhetorical sensitivity analysis</title>
    <description></description>
    <content:encoded><![CDATA[<p><em>&ldquo;The ethanol in alcohol is a group one carcinogen, like asbestos,&rdquo; </em>Prof. Doug Sellman, Otago University (<a href="http://www.stuff.co.nz/national/health/8862098/Ethanol-akin-to-asbestos-in-cancer-stakes">July 2013</a>)</p>
<p>Professor Sellman is correct, of course. &nbsp;<span>What's more, alcohol is even an important cause of cancer. From the viewpoint of rhetoric and risk communication i</span><span></span><span>t's still interesting to see how the effect of the sentence changes when other familiar IARC Group I carcinogens are substituted for 'asbestos'</span></p>
<ul>
<li><em>alcohol is a group one carcinogen, like sunlight</em></li>
<li><em><em>alcohol is a group one carcinogen, like birth-control pills</em></em></li>
<li><em><em>alcohol is a group one carcinogen, like plutonium</em></em></li>
<li><em><em><em>alcohol is a group one carcinogen, like tobacco</em></em></em></li>
<li><em><em><em><em>alcohol is a group one carcinogen, like arsenic,</em></em></em></em></li>
<li><em><em><em><em>alcohol is a group one carcinogen, like wood dust</em></em></em></em></li>
</ul>
<p>None of these really has the quite same rhetorical impact; the only one that comes close is &nbsp;'tobacco'. &nbsp;</p>
<p>Most of them&nbsp;<span>aren't scary enough: "Oh Noes! Beer is dangerous like sunshine!"&nbsp;</span>'Plutonium' and 'arsenic' are too scary: the (invalid) risk implication doesn't sound plausible. That's even though 'arsenic' in the IARC sense means low doses in drinking water, not the murder-mystery poison we tend to think of.</p>
<p></p>]]></content:encoded>
    <wp:post_name>96135813116</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/96072341641</link>
    <pubDate>Fri, 29 Aug 2014 20:26:00 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/96072341641</guid>
    <!--<wp:post_id>96072341641</wp:post_id>-->
    <wp:post_date>2014-08-29 1:26:00</wp:post_date>
    <wp:post_date_gmt>2014-08-29 08:26:00</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>A minor observation on dialect</title>
    <description></description>
    <content:encoded><![CDATA[<p>In New Zealand, 'radiata' and 'macrocarpa' are accepted common names for two widely planted non-native conifers: <em>Pinus radiata</em> and <em>Cupressus macrocarpa,</em><em>&nbsp;</em>known in their native US as 'Monterey pine' and 'Monterey cypress' respectively.</p>
<p>It's unusual for the specific epithet of a plant to become the common name. There are plenty of examples of the generic name becoming the common name, from 'bougainvillea' to 'wisteria'. &nbsp;There are even plenty of examples where a former generic name has stuck as the common name after the botanists have renamed the plant to, eg, &nbsp;<a href="http://en.wikipedia.org/wiki/Pelargonium"><em>Pelargonium</em></a>, <a href="http://en.wikipedia.org/wiki/Hippeastrum"><em>Hippeastrum</em></a>, or <a href="http://en.wikipedia.org/wiki/Corymbia"><em>Corymbia</em></a>.&nbsp;</p>
<p>I don't think I've ever heard another example of the specific name working this way, and I only know of one other example. &nbsp;Henry Reed's famous poem <a href="http://www.solearabiantree.net/namingofparts/namingofparts.html"><em>Naming of Parts</em></a> mentions 'japonica', which turns out to be the flowering quince <em>Chaenomeles japonica,</em> and by extension, other species and hybrids of <em>Chaenomeles</em>.&nbsp;</p>
<p>Are there others?</p>
<p></p>]]></content:encoded>
    <wp:post_name>96072341641</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/95980914906</link>
    <pubDate>Thu, 28 Aug 2014 18:15:00 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/95980914906</guid>
    <!--<wp:post_id>95980914906</wp:post_id>-->
    <wp:post_date>2014-08-27 23:15:00</wp:post_date>
    <wp:post_date_gmt>2014-08-28 06:15:00</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>O necessary sin</title>
    <description></description>
    <content:encoded><![CDATA[<p>The <a href="http://stat.ethz.ch/R-manual/R-devel/library/base/html/Trig.html">R help page</a> for <em>sin</em>, <em>cos</em>, and <em>tan</em>, mentions functions <em>sinpi, cospi, tanpi</em>, "accurate for <span class="s1">x</span> which are multiples of a half."&nbsp;This struck someone I know as strange. I've been thinking about this sort of thing recently while teaching Stat Computing, so here's some background.</p>
<p>If you're a <a href="http://gowers.wordpress.com/2014/03/02/how-do-the-power-series-definitions-of-sin-and-cos-relate-to-their-geometrical-interpretations/">mathematician</a>, $\sin x$&nbsp;is given by a power series<br />$$\sin x = x - \frac{x^3}{3!}+\frac{x^5}{5!} -\frac{x^7}{7!} +-\cdots$$</p>
<p>This series converges for all $x$, and so converges uniformly on any finite interval. In fact, it eventually converges faster than exponentially, since $n!\approx (n/e)^n$. At, say, $x=1$, 10 terms gives you 14 digits accuracy. Even better, it's an alternating series, so once the terms start to decrease, the error in truncating the series is smaller than the first omitted term.&nbsp;</p>
<p><span>The largest number of terms we could use without getting clever or working with logarithms is 85: at 86 terms, the factorial overflows double-precision storage. According to the real-number maths, that's still enough to get the error down to the 15th decimal place for $x=52,$ and logarithms are perfectly feasible.&nbsp;</span>It turns out that something else goes wrong first.</p>
<p>Consider $x=20$. We know $\sin x$ is between -1 and 1, but the first few terms of the series are huge: $20^{17}/17!$ is more than 35 million. &nbsp;For the series to converge, there must be almost perfect cancellation between large positive and negative terms. That's a recipe for massive inflation of rounding error when you're doing computations to finite precision.&nbsp;</p>
<p>The following R function compares the power series to the built-in $\sin$ function (which uses the one in the C standard library)</p>
<pre>culpa = function(x,N=85){
	n = 1:N
	terms = (-1)^(n+1)*x^(2*n-1)/factorial(2*n-1)
	sum(terms)-sin(x)
}
</pre>
<p>For $x=1$, or 2 or 3, the error is tiny, but it's creeping up. By the time we get to $x=38$, the error is larger than 2, which counts as completely inaccurate for a function bounded by $\pm 1.$ &nbsp;At $x=38$, the last term used is about $2\times 10^{-18}$, and so by standard results on alternating series, the error should be smaller than that. &nbsp;The real-numbers error bound is wrong by <strong>more than 18 orders of magnitude</strong> when applied to computer numbers -- and taking more terms will only make this worse.</p>
<p>So, how does<em> sin(x)</em> do it? The C standard, as is its habit, doesn't specify, but the basic idea is to reduce $x$ modulo $2\pi$ to get the argument into $[-\pi,\pi]$, and then use either the Taylor series or an approximating polynomial or ratio of polynomials. &nbsp;This works well for moderate $x$, but you still find</p>
<pre>&gt; sin((10^10)*pi)
[1] -2.239363e-06
</pre>
<p>In a sense, that's unavoidable. We've only got just under 16 digits of precision to work with, so $10^{10}\pi$ is accurate only to six digits after the decimal point. You can't do better.</p>
<p>Except, sometimes you can. If the formula you are trying to implement involves $\sin \pi x$ rather than $\sin x$, you don't need to waste time and accuracy multiplying by $\pi$ and then reducing modulo $2\pi$. You can reduce modulo 1 instead, which is faster, easier, and more accurate.&nbsp;The obvious use case is when $x$ is measured in degrees or cycles. If $x$ is in degrees, you need to evaluate $\sin (2\pi x/360)$. It's more accurate to use <em>sinpi(x/180)</em> than to use <em>sin(pi*x/180)</em>.&nbsp;</p>
<p>That's why&nbsp;ISO/IEC TS 18661 proposed <em>sinpi</em> and its siblings for a new C standard, and why R supplies an interface and an <a href="https://svn.r-project.org/R/trunk/src/nmath/cospi.c">implementation</a>.</p>]]></content:encoded>
    <wp:post_name>95980914906</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/95710047826</link>
    <pubDate>Mon, 25 Aug 2014 17:13:00 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/95710047826</guid>
    <!--<wp:post_id>95710047826</wp:post_id>-->
    <wp:post_date>2014-08-24 22:13:00</wp:post_date>
    <wp:post_date_gmt>2014-08-25 05:13:00</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Taking meta-analysis heterogeneity seriously</title>
    <description></description>
    <content:encoded><![CDATA[<p>In fixed-effects meta-analysis of a set of trials the goal is to find a weighted average of the true treatment effects in those trials (whatever they might be). The results are summarised by the weighted average and a confidence interval reflecting its sampling uncertainty.</p>
<p>In random-effects meta-analysis the trials are modelled as an exchangeable sample, implying that they can be treated as coming independently from some latent distribution of true treatment effects. That's attractive in some situations. What doesn't make sense to me is summarising the results just by the mean of this latent distribution and a confidence interval for that mean.&nbsp;</p>
<p>That is, the model for individual study estimates $\Delta_i$ is<br />$$\Delta_i\sim N(\mu_i,\sigma^2_i)$$<br />$$\mu_i\sim N(\mu_0, \tau^2)$$<br />and we usually report a confidence interval for $\mu_0.$</p>
<p>If you take seriously the idea of modelling heterogeneity in the true treatment effect, a confidence interval for the mean isn't enough. In order to make decisions you need a prediction interval for the the true treatment effect in a new population that might include you.&nbsp;</p>
<p>The difference between these intervals can be pretty large. Today, I saw <a href="http://www.nature.com/srep/2014/140822/srep06161/pdf/srep06161.pdf">a paper</a>&nbsp;(open-access) in the new Nature <em>Scientific Reports</em> journal, a meta-analysis of observational studies of vitamin C and lung cancer. &nbsp;Their Table 3 presents study-specific estimates and a random-effects meta-analysis for the risk ratio per extra 100mg/day vitamin C.&nbsp;</p>
<p>The point estimate is 0.93 and the confidence interval is 0.88-0.98, but the $I^2$ heterogeneity statistic is 75%. &nbsp;That is, the heterogeneity in the estimates is about three times the sampling uncertainty. &nbsp;Putting the data into my rmeta package in R I can reproduce their output (apart from their summary $p$-value, which I think must be a typo), and I get an estimate of $\tau=0.23$.</p>
<p>Combining that with the mean, the simple heterogeneity model says that the true effect on the relative risk scale of an extra 100mg/day vitamin C varies enormously depending on context, with 95% limits from 0.58 to 1.47. &nbsp;The true effect is beneficial in 62% of trials and harmful in 48%. This is without adding in the sampling uncertainty, which would expand the bounds slightly for a true prediction interval.&nbsp;</p>
<p>If we take the heterogeneity model seriously, this meta-analysis is telling us we have almost no clue about the effect of vitamin C on lung cancer in a new population that wasn't one of the studies that went into the analysis. &nbsp;Averaging over all populations, vitamin C is estimated to be slightly beneficial, but in <strong>your</strong> population we can't tell. &nbsp;Since the data are all observational and are visibly inconsistent, that's not terribly surprising, and is most likely due to different confounding patterns.</p>
<p>I think reporting suitable upper and lower quantiles of the latent treatment effect distribution in addition to a confidence interval for the mean would be an improvement for random-effects meta-analysis. &nbsp;In particular, it would help with the 'how much is too much' question about $I^2$, since a highly-heterogeneous set of studies would always end up with a wide treatment effect distribution.&nbsp;</p>
<p>It would be even better to report confidence intervals for the upper and lower quantiles, but that would take a little theoretical work, and the simple solution is probably good enough.&nbsp;</p>
<p><img alt="image" src="https://31.media.tumblr.com/f7215fe4f0ab3d0cc9c438344aa3e95d/tumblr_inline_naujsf5etE1s1hdxy.png" /></p>
<p></p>]]></content:encoded>
    <wp:post_name>95710047826</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/95358618851</link>
    <pubDate>Thu, 21 Aug 2014 21:02:00 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/95358618851</guid>
    <!--<wp:post_id>95358618851</wp:post_id>-->
    <wp:post_date>2014-08-21 2:02:00</wp:post_date>
    <wp:post_date_gmt>2014-08-21 09:02:00</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>A brief observation on shills</title>
    <description></description>
    <content:encoded><![CDATA[<p>By and large, it isn't cost-effective to bribe academics to say things they don't believe. &nbsp;</p>
<p>Academics who are sufficiently well-known to be worth bribing tend to be <em>(ipso facto</em>) both moderately affluent, and willing to have given up chances for greater wealth in exchange for freedom to do basically whatever they are interested in.&nbsp;The sensible strategy is to find academics who already believe what you want said, and to buy them bigger megaphones. &nbsp;Over time, cognitive dissonance may warp their views to be even better aligned with yours, but that's just a bonus.&nbsp;</p>
<p>Given that, it may be sensible to downweight the level of support for a position when it comes from sponsored research, but it is irrational to say&nbsp;an argument should be <em>dismissed</em> just because it comes from someone you view as a shill.&nbsp;</p>]]></content:encoded>
    <wp:post_name>95358618851</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/94896416376</link>
    <pubDate>Sat, 16 Aug 2014 21:12:44 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/94896416376</guid>
    <!--<wp:post_id>94896416376</wp:post_id>-->
    <wp:post_date>2014-08-16 2:12:44</wp:post_date>
    <wp:post_date_gmt>2014-08-16 09:12:44</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Upcoming talks</title>
    <description></description>
    <content:encoded><![CDATA[<p><em><em>"Statistics in the media: &lt;sigh&gt;".&nbsp;</em></em>Caf&eacute; Scientifique, Horse and Trap pub, Mount Eden, Auckland. <em>&nbsp;</em>August 27, evening.</p>
<p>Short course: "<em>Elements of R"</em>, aka the Ken and Thomas show, University of Lausanne, September 3-5.&nbsp;</p>
<p>Short course: <em>Complex sampling and epidemiologic designs.</em>&nbsp;University of Milan, September 9-10.</p>
<p><em>"Two million t-tests: lessons from GWAS"</em>, Australasian Epidemiology Association, Auckland, October 9.</p>
<p><em>"Measure Everything on Everyone? Substudies and Sampling in CVD </em><br /><em>Cohorts"&nbsp;</em>&nbsp;The Kronmal Symposium: Lessons Learned from Cardiovascular Epidemiology. November 24, University of Washington, Seattle.&nbsp;</p>]]></content:encoded>
    <wp:post_name>94896416376</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/94804517126</link>
    <pubDate>Fri, 15 Aug 2014 20:24:00 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/94804517126</guid>
    <!--<wp:post_id>94804517126</wp:post_id>-->
    <wp:post_date>2014-08-15 1:24:00</wp:post_date>
    <wp:post_date_gmt>2014-08-15 08:24:00</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Survey package update</title>
    <description></description>
    <content:encoded><![CDATA[<p>There's a new version, 3.30-3, of the 'survey' package for R. &nbsp;It's got quite a lot of new stuff:</p>
<ul>
<li>AIC and BIC for generalised linear models</li>
<li>Rank tests for more than two groups</li>
<li>Logrank and generalised logrank tests</li>
</ul>
<p>Since I'm known for a lack of enthusiasm about any of these techniques, why are they in the package? Am I just enabling?</p>
<p>Well, AIC and BIC are interesting, and I'll say more below. For all these techniques, though, the lack of versions that account for the sampling design hasn't stopped people using them. They're probably even worse off using versions that assume independent sampling than versions that handle weights and clustering.&nbsp;If we're going to give the Kruskal-Wallis test the unmarked grave it so richly deserves, we clearly aren't going to do it by restricting implementations. &nbsp;Instead, we need to teach people what it really does, and show them that isn't what they want.&nbsp;</p>
<p>AIC turns out to be relatively straightforward: Jon Rao and Alastair Scott worked out the sampling distribution of the likelihood ratio back when I was learning how to solve simultaneous equations. The expected value of the log likelihood ratio when the smaller model fits as well as the larger model is the trace of a matrix. That matrix would be the identity under iid sampling and correctly-specified models, giving the $p$ penalty in AIC; under complex sampling the matrix isn't the identity, but is easily estimated.</p>
<p>BIC is harder, because the likelihood can't just be a convenience. However, if all the models you consider can be nested in some Big Ugly Model and obtained by setting specific parameters to specific values in that Big Ugly Model, we can use the (approximate) Gaussian likelihood of the parameter estimates in the Big Ugly Model. &nbsp;Each submodel also has a Gaussian likelihood, and we can work out the honest, traditional, entirely Bayesian BIC for those Gaussian models. &nbsp;The result is a penalised Wald statistic, with a penalty that is the number of parametris times a sort of effective log sample size</p>
<p>I should also note that the generalised logrank tests are the work of Kevin Rader and Stuart Lipsitz, who presented them at the Joint Statistical Meetings in San Diego two years ago. &nbsp;They were on my to-do list, but the Harvard folks got to them first, and I just implemented what they presented.</p>]]></content:encoded>
    <wp:post_name>94804517126</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/94671124861</link>
    <pubDate>Thu, 14 Aug 2014 11:22:49 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/94671124861</guid>
    <!--<wp:post_id>94671124861</wp:post_id>-->
    <wp:post_date>2014-08-13 16:22:49</wp:post_date>
    <wp:post_date_gmt>2014-08-13 23:22:49</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Structure and interpretation of unspeakable tongues</title>
    <description></description>
    <content:encoded><![CDATA[<p>As a simple programming/probability exercise, I fitted a second-order Markov chain to <a href="https://mitpress.mit.edu/sicp/full-text/book/book.html"><em>"Structure and Interpretation of Computer Programs"</em></a> and the complete works of <a href="http://en.wikipedia.org/wiki/H._P._Lovecraft#Themes">H.P. Lovecraft</a>. Adding the King James Bible tends to improve the output, but even in NZ that seems inappropriate for a class example.&nbsp;</p>
<p>Here's some selected but unedited output: it always finishes at a full stop and starts on the word after one.</p>
<p><br /><em>Turing (1912-1954), whose 1936 paper laid the foundations of computing, but rather of a machine for each pair (i, j) that passes through the weed-choked fissure to that moment, I crouched on the floor in front of my strange new impulses seemed subtly connected with if and have to write programs as large-scale data-base-management systems.</em></p>
<p><em>This has important implications, both theoretical and practical, because we must distinguish different types as being defined in terms of opprobrium.</em></p>
<p><em>I reached for the special symbol that can be implemented as an element of dy, which will be the result of its midst, and before he could - he was unknown in Cairo, to know terror lives.</em></p>
<p><em>I turned again to the horror of its still greater was he thinking? To what extent could the expert test of the or expression, and we are given points a and b are equal? if they had a curiously fragmentary, intangible, and heterogeneous army of invasion.</em></p>
<p><em>Then, bracing himself, he added, had important business elsewhere, and which involved other French settlers of two cubes in two registers with those who inferred from the data base satisfying a specified location.</em></p>
<p><em>Unify-match is basically the right branch, a set of needed and modified by the generic arithmetic operations, we use assoc to see the rescue made by a sleeper is indeed the best course would involve many additional hazards.</em></p>
<p><em>Information Retrieval Logic programming excels in incidents and sensations of tense and heated discussion; and before the recursive definition which translates readily into the flat head and end with only general sense, memory, and the mockery of the defenders, and afterward the physician, unable to sing.</em></p>
<p><em>I could see that it signals an error if the evaluation rule for adding ``greats'' to a complex number, such as ``If x is 1 and the nameless scent was now shaking with their wives testify the bewilderment which their far too common Knowledge to be delayed, we must ensure that our programs at a distance from the all-pervading and almost dreaded the exertion, for his family and Dr.</em></p>
<p><em>De Marigny recalled the old room of my youth about the matter of duty, though in reverse order.</em></p>
<p><em>Binger saw me and bade me mark that all the other hand, even toy parsers can be extended to include some germ-tainted crocodile meat as food for the body of a century.</em></p>]]></content:encoded>
    <wp:post_name>94671124861</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/91516935421</link>
    <pubDate>Sat, 12 Jul 2014 16:05:12 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/91516935421</guid>
    <!--<wp:post_id>91516935421</wp:post_id>-->
    <wp:post_date>2014-07-11 21:05:12</wp:post_date>
    <wp:post_date_gmt>2014-07-12 04:05:12</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Feynman and the Suck Fairy</title>
    <description></description>
    <content:encoded><![CDATA[<p>There's been a bit of...discussion...about Richard Feynman recently. In one Twitter discussion, Richard Easther said he had been thinking of using Feynman's commencement address&nbsp;<a href="http://neurotheory.columbia.edu/~ken/cargo_cult.html">"Cargo Cult Science"</a>&nbsp;with a first-year physics class, and had decided against.</p>
<p>I was a bit surprised. It's been a long time since I read that piece, but I couldn't remember anything objectionable in it. So I re-read it. &nbsp;It's still really good in a lot of ways. But. Yeah. That.</p>
<p>The problems are more obvious in Feynman's book of autobiographical anecdotes <em>"Surely You're Joking, Mr Feynman"</em>. I read that when it came out, and loved it. I was in high school at the time. It wasn't that I loved the casual sexism; I just didn't really notice. I was interested in the science bits of the book, and I didn't care what he did in bars in Buffalo. It's harder to re-read it now without wincing.&nbsp;</p>
<p><span>If you find yourself feeling defensive about Feynman, you might like to read&nbsp;<a href="http://www.tor.com/blogs/2010/09/the-suck-fairy">Jo Walton on the Suck Fairy</a> in literature. The Suck Fairy goes through books you used to love, and edits them to make them suck when you read them again. The whole point of the name, of course, is that this isn't true; the book always did suck and you just didn't notice when you were younger. &nbsp;The book you remember is still good, it just isn't real.</span></p>
<p><span>The <em>"Surely You're Joking.."</em> I remember, and its author and hero, were great. There's no need to deny that. The problem is,</span><span>&nbsp;they weren't the actual book and the actual physicist. The actual person was a genius, but not really a role model.&nbsp;</span></p>
<p>The imaginary Feynman, the one who wrote the imaginary book I remember reading, would have wanted us to be honest about the faults of the actual Feynman.&nbsp;</p>]]></content:encoded>
    <wp:post_name>91516935421</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/88828431471</link>
    <pubDate>Sun, 15 Jun 2014 17:02:19 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/88828431471</guid>
    <!--<wp:post_id>88828431471</wp:post_id>-->
    <wp:post_date>2014-06-14 22:02:19</wp:post_date>
    <wp:post_date_gmt>2014-06-15 05:02:19</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Unacceptable Use</title>
    <description></description>
    <content:encoded><![CDATA[<p>The East-West Center at the University of Hawai'i has one of the most bizarre Internet Acceptable Use Policies I have ever seen. Among other things, it "strictly prohibits"</p>
<div class="page" title="Page 3">
<div class="layoutArea">
<div class="column">
<blockquote>
<p><em>The distribution, dissemination, storage, copying and/or sale of materials protected by copyright,&nbsp;trademark, trade secret or other intellectual-property right laws.&nbsp;</em></p>
</blockquote>
<p>That bans making this post, downloading the slides for the Summer Institute in Statistical Genetics that I want to revise, installing new R packages, and probably even reading email (unless it was sent by someone who is a US government employee as part of their work). &nbsp;You might think there's an implied "except with permission of copyright owner", except that in lots of other places in the policy they make that sort of exception explicit. Also, the policy goes on to say that this includes, but is not limited to, illegal copying and distribution.&nbsp;</p>
<p>More <em>practically</em> annoying is the fact that they block ssh and scp, so I can't upload the files for my course tomorrow. Maybe I'll go for a walk and see if I can get an eduroam connection.&nbsp;</p>
</div>
</div>
</div>]]></content:encoded>
    <wp:post_name>88828431471</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/87459685746</link>
    <pubDate>Sun, 01 Jun 2014 16:17:00 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/87459685746</guid>
    <!--<wp:post_id>87459685746</wp:post_id>-->
    <wp:post_date>2014-05-31 21:17:00</wp:post_date>
    <wp:post_date_gmt>2014-06-01 04:17:00</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Herd immunity simulations</title>
    <description></description>
    <content:encoded><![CDATA[<p>Especially for vaccines that are not 100% effective, a large chunk of the benefit comes from 'herd immunity', the fact that incomplete vaccination makes it harder for an epidemic to get started and spread. Increasing the proportion of people vaccinated helps those people, and it also helps the people who aren't vaccinated.</p>
<p>Here's a set of simulations (<a href="https://gist.github.com/anonymous/7e80257348c0a340b364">code</a>, needs FNN package and R) that show the effect. There is a simulated population of 10,000 people living on a square (actually, a doughnut, since it wraps around). &nbsp;Vaccinated people are green, unvaccinated are black.&nbsp;</p>
<p>Each day there is a 1/30 chance of a new disease case arriving.&nbsp;</p>
<p>If you are near a disease case you have a chance of being infected (red) which is lower, but still not zero, if you are vaccinated. The disease lasts four days and then you are immune (blue).&nbsp;Everyone moves slowly around, except that each day 1% of people get on a plane and move a long way.</p>
<p>With 10% vaccinated there is no herd immunity. As soon as an epidemic gets going, it spreads everywhere.</p>
<p>With 50% vaccinated, the epidemics still spread, but more slowly, and there's a lower chance of starting one when a case arrives.</p>
<p>With 70% vaccinated, the epidemics burn out before covering the population</p>
<p>With 90% vaccinated, the epidemics don't even get started.</p>
<p></p>
<p>&nbsp;<iframe frameborder="0" height="315" src="//www.youtube.com/embed/KkMD6KGgltU" width="420"></iframe></p>
<p></p>
<p></p>
<p><iframe frameborder="0" height="315" src="//www.youtube.com/embed/uw93SdC-ouo" width="420"></iframe></p>
<p></p>
<p></p>
<p><iframe frameborder="0" height="315" src="//www.youtube.com/embed/ivRBM03gPwM" width="420"></iframe></p>
<p><iframe frameborder="0" height="315" src="//www.youtube.com/embed/xTmHUegqcrA" width="420"></iframe></p>]]></content:encoded>
    <wp:post_name>87459685746</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/86487451016</link>
    <pubDate>Thu, 22 May 2014 20:12:09 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/86487451016</guid>
    <!--<wp:post_id>86487451016</wp:post_id>-->
    <wp:post_date>2014-05-22 1:12:09</wp:post_date>
    <wp:post_date_gmt>2014-05-22 08:12:09</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Monotonicity and smoothness</title>
    <description></description>
    <content:encoded><![CDATA[<p>Andrew Gelman has an<a href="http://andrewgelman.com/2014/05/21/models-constraints/"> interesting discussion</a> of monotonicity as a modelling constraint. &nbsp;I basically agree with what he says, but since my first real statistical research (my <a href="http://ideas.repec.org/a/eee/csdana/v20y1995i5p499-510.html">M.Sc. thesis</a>) was on order restrictions I thought I'd write about a related aspect of the problem.</p>
<p>Assuming that a relationship is monotone sounds like a very strong assumption, and therefore one that you'd expect to gain a lot by making. Asymptotically, this isn't true. &nbsp;If the relationship between $X$ and $Y$ is only known to be monotone, you get $E[Y|X=x]$ estimated to $O_p(n^{-1/3})$ where $X$ has non-zero density. By assuming smoothness you can get $O_p(n^{-2/5})$, which is better. That is, if you have a lot of data and you know a relationship is smooth, you don't gain anything by knowing it is monotone, but if you know it is monotone you do gain by knowing it is smooth.</p>
<p>I think that's non-intuitive, and I think the reason it's non-intuitive is the asymptotics. If you have relatively sparse data, knowing that the relationship is monotone is fairly powerful, but knowing it is smooth is pretty useless. If you have very dense data, knowing<em> a priori</em> the relationship is smooth is useful, but knowing <em>a priori</em>&nbsp;that it is monotone is not all that helpful, since it will be fairly obvious whether it's monotone or not.&nbsp;</p>
<p></p>
<p></p>]]></content:encoded>
    <wp:post_name>86487451016</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/86189983121</link>
    <pubDate>Mon, 19 May 2014 18:10:00 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/86189983121</guid>
    <!--<wp:post_id>86189983121</wp:post_id>-->
    <wp:post_date>2014-05-18 23:10:00</wp:post_date>
    <wp:post_date_gmt>2014-05-19 06:10:00</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Anchoring bias</title>
    <description></description>
    <content:encoded><![CDATA[<p>Anchoring bias: high school students asked to add up the digits in their phone number and to estimate how many countries there are in Africa.</p>
<p><img alt="image" src="https://31.media.tumblr.com/632d8738a3eb36f00b24c469a20855b7/tumblr_inline_n5t53cf16m1s1hdxy.png" /></p>
<p>(phew, it worked)</p>
<p></p>
<p>(I did delete one data point as non-responsive: estimated number of countries in Africa was 1)</p>
<p>(with adults I'd use last two digits of phone number, but with teenage girls I thought a bit more information-hiding was appropriate)</p>]]></content:encoded>
    <wp:post_name>86189983121</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/85802838191</link>
    <pubDate>Thu, 15 May 2014 20:09:07 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/85802838191</guid>
    <!--<wp:post_id>85802838191</wp:post_id>-->
    <wp:post_date>2014-05-15 1:09:07</wp:post_date>
    <wp:post_date_gmt>2014-05-15 08:09:07</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Fiction about science</title>
    <description></description>
    <content:encoded><![CDATA[<p>I recently discovered that Rosemary Kirstein's <em>Steerswoman</em> series of books has been reissued electronically and is available wherever good e-books are <strike>sold</strike> licensed for download.</p>
<p><span>These books are some of the best ever fictional characterisations of the scientific </span><em>process</em><span>, as opposed to engineering or technology. &nbsp;It's not obvious until you're reading them, because they have the formal structure of epic fantasy of the quest variety (powerful evil wizard subvariety). &nbsp;The protagonist, Rowan, is a wandering member of an order of, basically, scientists: people whose mission it is to learn and understand things and then tell people. &nbsp;Rowan has to discover what's actually going on at the same time the reader does, and Kirstein manages very well the trick of giving Rowan clues to what's going on in a way that she's sometimes ahead of the reader and sometimes behind.&nbsp;</span></p>
<p>Other incidental attractive features include the fact that nearly all the scientists are women, and that science communication is <em>explicitly</em> the price paid for public support and cooperation. If you ask a steerswoman a question, she must answer truthfully; if she asks you a question and you do not answer truthfully, you are excluded from the deal and they will not answer your questions in the future.</p>
<p>The biggest negative is that this is theoretically a five book series and Kirstein makes George R. R. Martin look like Barbara Cartland. The first book was published in 1989; the fourth, in 2004. The books are stand-alone novels, so they are still readable, but if you get annoyed waiting for an author, she is not an author to take up.&nbsp;</p>
<p>Some people won't be able to get past the genre fiction form, but for anyone who likes good genre fiction, or anyone that knows a teenage girl who is put off by popular depictions of science and scientists, should try them.</p>]]></content:encoded>
    <wp:post_name>85802838191</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/85709410221</link>
    <pubDate>Wed, 14 May 2014 21:53:00 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/85709410221</guid>
    <!--<wp:post_id>85709410221</wp:post_id>-->
    <wp:post_date>2014-05-14 2:53:00</wp:post_date>
    <wp:post_date_gmt>2014-05-14 09:53:00</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Randomisation without consent</title>
    <description></description>
    <content:encoded><![CDATA[<p>The issue of randomisation without consent has<a href="http://www.nzherald.co.nz/nz/news/article.cfm?c_id=1&amp;objectid=11254381"> come up in New Zealand</a>. Because I'm on the HRC Data Monitoring Core Committee, which monitors some NZ clinical trials I don't want to say much about any current NZ clinical trials, even ones we're not monitoring. I do want to talk about the principle.</p>
<p>The always-useful NZ Science Media Centre has rounded up a couple of <a href="http://www.sciencemediacentre.co.nz/2014/05/14/medical-ethics-and-unconscious-patients-experts-respond/">bioethicists</a> on the topic, and you should read what they say. I'm not a professional bioethicist, but I have been involved in discussions about the ethics and conduct of clinical trials since I was in high school, learning from people with internationally recognised expertise.&nbsp;</p>
<p>In contrast to physicians, who tend to start off from the doctor-patient relationship, my views have generally been that informed consent, if you could do it right, would be the only thing needed. If research participants understand all the issues and freely choose to take part, it doesn't matter what anyone else thinks.</p>
<p>In practice, though, you can't get perfect informed consent. &nbsp;Many people don't have the resources to get a really thorough understanding of the issues even when they are healthy, much less when they are sick. They will often go along with a recommendation from their physician, even if she doesn't think she's giving one.</p>
<p>Since you can't get perfect informed consent, you need other safeguards as well. Physicians need to decide whether a trial is suitable to suggest to patients; ethics committees need to decide whether it meets guidelines; paperwork needs to be filled in; whistleblowers need to be protected.&nbsp;</p>
<p>On the other hand, consent is still primary. New Zealand was home to one of the <a href="http://www.cartwrightinquiry.com/">dramatic examples</a> of what can happen when clinical research decisions are left to people who think they are on to something.&nbsp;</p>
<p>Given that I believe participant consent is the primary ethical principle in research and that the other principles are there as safeguards, does that mean I'm opposed to all research without consent? In fact, no. Any research that can be done with informed consent should be done that way. &nbsp;Any research that can be done with advance consent before people become incompetent should be done that way. Any research where the patient can't consent but a guardian or next-of-kin can consent should be done that way.&nbsp;Any research that can be done with even very limited patient assent (people with some types of mental illness, older children, people with dementia) should be done that way in addition to any other safeguards. And as with any vulnerable group, research should only be done on people who can't give consent if the treatment is <em>primarily</em> intended to benefit precisely that group of people.&nbsp;</p>
<p>However, I used to live in Seattle. Seattle has a long record of research and development in resuscitation medicine: treatments that are given to people who can't consent because they are basically dead at the time. Not unrelated to that, Seattle has one of the highest survival rates for out-of-hospital cardiac arrest.</p>
<p>There are three possibilities for resuscitation medicine</p>
<ol>
<li>No new treatments are ever introduced.</li>
<li>New treatments are introduced, but not evaluated</li>
<li>People are randomised without consent</li>
</ol>
<p>None of these is ideal, but I think the first two are worse than the third.&nbsp;</p>
<p>It's still important for randomisation without consent to have additional safeguards that aren't needed for normal clinical trials. &nbsp;In Seattle this included special ethics review, public consultation, public advertisement of how to opt-out of randomisation, and monitoring of how many people withdrew consent once they were in a state to be asked.</p>
<p>That is, the extra safeguards were intended to ensure that trials proceeded without consent only if there wasn't any other way, <em>and</em> if there was good reason to believe people would have consented given the opportunity. If the public consultation was negative or if lots of people withdrew consent for data use, consent to randomisation could no longer be assumed and you pull the plug on the trial.&nbsp;</p>
<p>In New Zealand there seems to be a term "retrospective consent" for when people wake up and you ask them how they feel about being randomised. &nbsp;I think this is the wrong way to phrase the issue. We need to recognise that participants are being randomised without consent, just as unconscious patients are routinely treated without consent if there is no alternative. We can, and must, ask patients whether they consent to their data being used, and whether they approve of having been randomised. But that's not consent to randomisation and treatment. It's too late for that.&nbsp;</p>]]></content:encoded>
    <wp:post_name>85709410221</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/85569565091</link>
    <pubDate>Tue, 13 May 2014 11:29:00 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/85569565091</guid>
    <!--<wp:post_id>85569565091</wp:post_id>-->
    <wp:post_date>2014-05-12 16:29:00</wp:post_date>
    <wp:post_date_gmt>2014-05-12 23:29:00</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Henchperson wanted</title>
    <description></description>
    <content:encoded><![CDATA[<p><strong>PhD Scholarship in Statistics: model-based and model-assisted analysis of complex samples</strong></p>
<p>This project is funded for three years by a grant from the Royal Society of New Zealand Marsden Fund. There are multiple possible PhD topics in analysis of complex samples and its interface with semiparametric mathematical statistics. The research group includes Thomas Lumley, Alan Lee, Alastair Scott, Chris Wild, and <strike>two</strike>three current PhD students (with one more having recently graduated).</p>
<p>Funding is available to start as soon as possible, but no later than September 2014. Applicants must have suitable qualifications for entry into the University of Auckland PhD, and should have significant training in at least two of complex sampling, statistical programming, mathematical statistics.</p>
<p>Contact:<br /> about the application process: stat.phdofficer@auckland.ac.nz<br /> about the research project: t.lumley@auckland.ac.nz</p>]]></content:encoded>
    <wp:post_name>85569565091</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/79561911881</link>
    <pubDate>Sat, 15 Mar 2014 05:12:00 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/79561911881</guid>
    <!--<wp:post_id>79561911881</wp:post_id>-->
    <wp:post_date>2014-03-14 9:12:00</wp:post_date>
    <wp:post_date_gmt>2014-03-14 16:12:00</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Einstein, Wikiquote, and fact checking</title>
    <description></description>
    <content:encoded><![CDATA[<p>It's not only Pi Day in the USA (3/14, they write dates backwards), it's Einstein's 135th birthday. Einstein, like Mark Twain, <span>孔夫子,&nbsp;</span>Churchill, Disraeli, and the <a href="http://www.nzherald.co.nz/nz/news/article.cfm?c_id=1&amp;objectid=11219648">Chinese proverbs</a>, is a quote magnet. He said many quotable things, and even more are attributed to him.</p>
<p>The NZ Herald <a href="http://www.nzherald.co.nz/world/news/article.cfm?c_id=2&amp;objectid=11219746">has a list of ten Einstein quotes</a>. Annoyingly, none of them say where or when they were said. So I did the absolutely minimal level of fact checking. I looked on Wikiquote's <a href="http://en.wikiquote.org/wiki/Albert_Einstein">Albert Einstein page</a> and the <a href="http://en.wikiquote.org/wiki/Talk:Albert_Einstein">talk page</a>.</p>
<p>All ten quotes appear on Wikiquote. Five have reliable sources originating with Einstein. One more is something Einstein said, but that he didn't claim was original. One has long been attributed to him but is disputed. Two look like recent inventions. One is definitely from someone else.</p>
<p>1. E. F. Schumacher "Small is Beautiful"<br />2. Quoted by Einstein but not original to him. A popular saying in French before his time <em>"La culture est ce qui reste lorsque l&rsquo;on a tout oubli&eacute;"</em> <br />3. Einstein. From a speech to the New History Society (14 December 1930), reprinted in "Militant Pacifism" in Cosmic Religion (1931)<br />4. No reliable source known, not attributed to him before 1990s<br />5. From Science and Religion (1941)<br />6. Not Einstein. Apparently first attributed to him by Ram Dass in 1970.<br />7. Einstein. Attributed in The Encarta Book of Quotations to an interview on the Belgenland (December 1930), which was the ship on which he arrived in New York that month.<br />8. Einstein. Speech made in honor of Thomas Mann in January 1939, when Mann was given the Einstein Prize given by the Jewish Forum. <br />9. Disputed. No reliable attributions. Similar quotes in French reliably sourced to 1880s<br />10. Einstein: said to Samuel J Woolf, Berlin, Summer 1929. Cited with additional notes in The Ultimate Quotable Einstein by Alice Calaprice and Freeman Dyson, Princeton UP (2010) p 230</p>
<p>Not a terribly good record for a serious newspaper.&nbsp;</p>]]></content:encoded>
    <wp:post_name>79561911881</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/78570650254</link>
    <pubDate>Wed, 05 Mar 2014 09:00:00 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/78570650254</guid>
    <!--<wp:post_id>78570650254</wp:post_id>-->
    <wp:post_date>2014-03-04 12:00:00</wp:post_date>
    <wp:post_date_gmt>2014-03-04 20:00:00</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>My likelihood depends on your frequency properties</title>
    <description></description>
    <content:encoded><![CDATA[<p><em>[note: you may need to click on a single post for the typesetting to work; it doesn't always work for the blog as a whole]</em></p>
<p>The likelihood principle states that given two hypotheses $H_0$ and&nbsp;$H_1$ and data $X$, all the evidence regarding which hypothesis is&nbsp;true is contained in the likelihood ratio<br />$$LR=\frac{P[X|H_1]}{P[X|H_0]}.$$</p>
<p>One of the fundamentals of &nbsp;scientific research is the idea of scientific publication, which allows other researchers to form their own conclusions based on your results and those of others. The data available to other researchers, and thus the likelihood on which they rely for inference, depends on your publication behaviour. In practice, and even in principle, publication behaviour for one hypothesis does depend on evidence you obtained for other hypotheses under study, so likelihood-based inference by other researchers depends on the operating characteristics of your inference.</p>
<p>Consider an idealised situation of two&nbsp;scientists, Alice and Bob (who are on sabbatical from the cryptography&nbsp;literature). Alice spends her life collecting, analysing, and&nbsp;reporting on data $X$ that are samples of size $n$ from $N_p(\mu, I)$ distributions, in order to make&nbsp;inference about $\mu$. Bob is also interested in $\mu$ but doesn't have&nbsp;the budget to collect his own $N_p(\mu,I)$ data. He assesses the&nbsp;evidence for various values of $\mu$ by reading the papers of Alice and&nbsp;other researchers and using their reported statistics $Y$. <a href="http://blogs.plos.org/everyone/2014/02/24/plos-new-data-policy-public-access-data/">In the future</a>, he might be able to get their raw data easily, but not yet.&nbsp;</p>
<p>Alice and&nbsp;Bob primarily care about $\mu_1$ which is obviously much more interesting than $\left\{\mu_i\right\}_{i=2}^p$, and more likely to be meaningfully far from zero,&nbsp;but they have some interest in the others.&nbsp;Alice bases her likelihood inference on the multivariate Normal&nbsp;distributions $f_X(X|\mu_i)$, Bob bases his on $f_Y(Y|\mu_i)$.</p>
<p>Compare Alice and Bob's&nbsp;likelihood functions for the hypotheses $\mu_i=0$ and $\mu_i=\delta$&nbsp;with $\delta$ meaningfully greater than $0$ in the following scenarios. In all of them, Alice <span>collects data on $\mu_1$ and reports the likelihood ratio&nbsp;</span><span>for $\mu_1=0$ versus $\mu_1=\delta$.</span>&nbsp;</p>
<ol>
<li><span>Alice collects only data on $\mu_1$ and reports the likelihood ratio&nbsp;</span><span>for $\mu_1=0$ versus $\mu_1=\delta$.</span></li>
<li><span>&nbsp;Alice also collects data on $\mu_2$ and reports whether she finds&nbsp;</span><span>strong evidence for $\mu_2=\delta$ over $\mu_2=0$ or not.</span></li>
<li><span>Alice also collects data on <span>&nbsp;$\mu_2\ldots\mu_q$ for some $q\leq p$.</span><span>&nbsp;</span>&nbsp;If she finds evidence worth mentioning in favour of $\mu_i=\delta$, she publishes her likelihood ratio, otherwise she reports that there wasn't enough evidence.&nbsp;</span></li>
<li><span>&nbsp;Alice also collects data on $\mu_2\ldots\mu_q$ for some $q\leq p$.</span><span>&nbsp;If she finds sufficient evidence for&nbsp;</span><span>$\mu_i=\delta$ for any $i&gt;1$ she reports the likelihood ratios for all&nbsp;$\mu_i$, otherwise only for $\mu_1$.&nbsp;</span></li>
</ol>
<p><span>Alice's likelihood ratio is the same in all scenarios. She obtains for each $i$</span></p>
<p><span></span>$$\frac{L_1}{L_0}=\frac{L(\mu_i=\delta)}{L(\mu_i=0)}=\frac{\exp(n^2(\bar X_i-\delta)^2/2)}{\exp(n^2\bar X_i^2/2)}.$$</p>
<p>and because she has been properly trained in decision theory her beliefs and her decisions about future research for any $\mu_i$ depend only on $\bar X_i$, not on $q$ or on other $\bar X_j$ or on how she decided to what to publish.</p>
<p><span>Bob's likelihood ratio for $\mu_1$ is always the same as Alice's. For the other parameters, things are more complicated.</span></p>
<ol>
<li><span>no other parameters</span></li>
<li><span>Bob's data is Alice's result, $Y_2=1$ for finding strong evidence, $Y_2=0$ for not. His likelihoods are $L_1=(1-\beta)^{Y_2}\beta^{1-Y_2}$ and $L_0=\alpha^{Y_2}(1-\alpha)^{1-Y_2}$, where $\alpha$ is the probability Alice finds strong evidence for $\mu_2=\delta$ when $\mu_2=0$ is true and $\beta$ is the probability Alice fails to find strong evidence for $\mu_2=\delta$ when $\mu_2=\delta$ is true.</span></li>
<li><span>Bob has a censored Normal likelihood, which depends on $\alpha$ and $\beta$. If he ignores this and just uses Alice's likelihood ratio when it's available, he will inevitably end up believing $\mu_i=\delta$ for all $i&gt;2$, regardless of the truth.</span></li>
<li><span>Bob's likelihood ratio for the other $\mu_i$ depends on $\alpha$, $\beta$, $q$ and on the values of $\mu_j$ for $j\neq i$.</span></li>
</ol>
<p>In scenarios 2-4, Bob's likelihood depends on Alice's criterion for strength of evidence and on how likely she is to satisfy it -- if Alice were a frequentist, we'd call $\alpha$ and $\beta$ her Type I and Type II error rates. But it's not a problem of&nbsp;misuse of $p$-values. Alice doesn't use $p$-values. She would never touch a $p$-value without heavy gloves. She doesn't even like being in the same room as a $p$-value. &nbsp;</p>
<p><span>In scenario 4, Bob also needs to know $q$ in order to interpret papers that do not include results for $i&gt;1$ -- he needs to know Alice's family-wise power and Type I error rate.&nbsp;</span><span>That's actually not </span><em>quite</em><span> true: if Bob knows Alice is following this rule he can ignore her papers that don't contain all the likelihood ratios, since he does know $q$ for the ones that do. &nbsp;His likelihood for $i&gt;1$ still depends on $\alpha$, $\beta$, $q$, and the other $\mu$s. &nbsp;</span></p>
<p><span>At least, if nearly all Alice's papers report results for all the $\mu$s, Bob knows that the bias from just using Alice's likelihood ratio when available will be small and he may be able to get by without all the detail and complication.&nbsp;</span></p>
<p><span>This isn't quite the same as publication bias, though it's related. At least if $q$ is given and we know Alice's criteria, &nbsp;she always publishes information about every analysis that would be sufficient for likelihood inference not only about $\mu_i=0$ vs $\mu_i=\delta$, but even for point and interval estimation of $\mu_i$. Alice isn't being evil here. She's not hiding negative results; they just aren't that interesting.</span></p>
<p><span>Of course, the problem would go away if Alice published, say, posterior distributions or point and interval estimates for all $\mu_i$, at least if $p$ isn't large enough that the complete set </span><a href="http://www.stat.cmu.edu/~fienberg/Fienberg-Slavkovic-Chance-2004.pdf" title="official-data example in CHANCE">could</a><span> be&nbsp;</span><a href="http://www.plosgenetics.org/article/info%3Adoi%2F10.1371%2Fjournal.pgen.1000665" title="Genetics example">sensitive</a><span>.&nbsp;</span></p>
<p><strong>tl;dr</strong>: &nbsp;If I can't get your data or at least (approximately) sufficient statistics, <em>my</em> conclusions may depend on details of your analysis and decision making that don't affect <em>your</em> conclusions.&nbsp;And if you ever just report&nbsp;<em>"was/wasn't significant,"</em> Bob will hunt you down and make you regret it.</p>]]></content:encoded>
    <wp:post_name>78570650254</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/77779938162</link>
    <pubDate>Tue, 25 Feb 2014 18:16:53 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/77779938162</guid>
    <!--<wp:post_id>77779938162</wp:post_id>-->
    <wp:post_date>2014-02-24 21:16:53</wp:post_date>
    <wp:post_date_gmt>2014-02-25 05:16:53</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Chemical nerdview</title>
    <description></description>
    <content:encoded><![CDATA[<p><a href="http://www.stephenjaygould.org/library/gould_house-ussher.html">One of Stephen J. Gould's essays</a> contains the admission</p>
<blockquote>
<p><em>I confess that I have always been greatly amused by the term&nbsp;primate, used in its ecclesiastical sense as "an archbishop ... holding the first place among the bishops of a province." My merriment must be shared by all zoologists, for primates, to us, are monkeys and apes--members of the order Primates.&nbsp;</em></p>
<p><em>...</em></p>
<p><em>But this amusement is silly, parochial, and misguided.</em></p>
</blockquote>
<p><span><span>Gould points out that the clerics had the term first, and that the etymology, from the Latin <em>primas</em>, first, is just as appropriate (actually, more appropriate) in the ecclesiastical usage as in the zoological one.</span></span></p>
<p>Chemists tend to find the concept of 'organic farming' amusing, because they think of the term 'organic' as meaning 'containing carbon atoms', not as indicating derivation from living things. &nbsp;The chemists are even more at fault than the zoologists, because this sense of organic is not only older than the 'carbon atoms' sense, but actually comes from chemistry, as does the semi-mystical distinction between synthesis in a lab and in a living cell.&nbsp;</p>
<p>Before 1828, there was a clear division in chemistry between ordinary compounds that could be made in the lab by ordinary chemical procedures and organic chemicals that were produced by living creatures. When Friedrick <a href="http://en.wikipedia.org/wiki/W%C3%B6hler_synthesis">W<span>&ouml;</span>hler</a> first made the organic compound urea from &nbsp;inorganic ammonium cyanate he virtually pissed himself in shock, writing to Berzelius</p>
<blockquote>
<p><span>"</span><em>I cannot, so to say, hold my chemical water and must tell you that I can make urea without thereby needing to have kidneys, or anyhow, an animal, be it human or dog</em><span>".</span></p>
</blockquote>
<p>Even then, the vitalist idea that organic compounds were special remained for a couple of decades, until <a href="http://en.wikipedia.org/wiki/Adolph_Wilhelm_Hermann_Kolbe">Kolbe</a> synthesized acetic acid step by step from precursors that were undeniably non-organic (in the old sense).</p>
<p><span>Chemists then had to adapt their terminology. Since nearly all of the compounds produced only by living cells contained carbon atoms, and vice versa, they could just make a slight shift in definition. &nbsp;This occasionally caused problems for small molecules without much carbon -- John Clarke's book <em>Ignition</em> mentions some confusion over organic vs non-organic nomenclature of rocket fuel compounds -- but worked well enough that many people seem to have forgotten the shift occurred.</span></p>
<p><span>The chemists got over vitalism; cells do just work by the same rules of chemistry and physics as test-tubes. Ascorbic acid from a Chinese chemical company really does fail to prevent cancer just as well as natural vitamin C. If complex mixtures of carotenoids turn out to have health benefits not provided by pure beta-carotene, these benefits will just depend on the actual molecules present, not on their origin. </span></p>
<p><span>Vitalism in chemistry was wrong, but the vitalist sense of 'organic' is older and was just as rooted in chemistry as the 'carbon atoms' sense. There are better things to sneer at Wholefoods about.</span></p>
<p></p>]]></content:encoded>
    <wp:post_name>77779938162</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/76183745465</link>
    <pubDate>Mon, 10 Feb 2014 16:19:00 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/76183745465</guid>
    <!--<wp:post_id>76183745465</wp:post_id>-->
    <wp:post_date>2014-02-09 19:19:00</wp:post_date>
    <wp:post_date_gmt>2014-02-10 03:19:00</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>WNAR Biometrics short course</title>
    <description></description>
    <content:encoded><![CDATA[<p class="p1">I'm giving the one-day short course at the <a href="http://www.wnar.org/">WNAR</a> Biometrics meeting in Honolulu this year. It's an introduction to large-scale genetic association studies. Here's the blurb. You'll soon be able to register for the conference, probably at a link <a href="http://www.wnar.org/">here</a></p>
<p class="p2"></p>
<p class="p2"><strong>Introduction to large-scale genetic association studies</strong></p>
<p class="p3">It is now feasible to measure hundreds of thousands of genetic variants on enough individuals for population-based epidemiology, and is becoming feasible to do the same for all or part of the complete DNA sequence.&nbsp; This course will cover the statistics and some of the computing needed to analyse large-scale genetic data on large numbers of unrelated individuals.&nbsp; Apart from a few brief mentions, it will not cover family-based studies or organisms other than humans, and will assume raw measurements have already been turned into genotypes.&nbsp;</p>
<p class="p2"></p>
<p class="p3"><em>Since 2008, Thomas has been a member of the Analysis Committee of the <a href="http://web.chargeconsortium.com/">Cohorts for Heart and Aging Research in Genomic Epidemiology </a>(CHARGE), which has done quite a lot of genome-wide SNP analysis and, more recently, DNA sequencing, in large cardiovascular cohort studies.&nbsp;</em></p>
<p class="p2"></p>
<p class="p2"><span>Outline.&nbsp;</span></p>
<p class="p3">1. <em>In which we meet DNA and learn how it is measured</em></p>
<p class="p2">DNA is very stable and is the same in essentially all cells in the body, so blood samples taken in the 1990s can easily be used to study disease and biology today (unlike RNA, protein, methylation, etc). How DNA varies, and how these variants are measured.&nbsp; What goes wrong with the measurements. &nbsp;</p>
<p class="p3">2<em>. In which we perform simple tests many times</em></p>
<p class="p2">The basic genome-wide association study consists of millions of simple linear or logistic regressions.&nbsp; The multiplicity leads to the resurrection of statistical issues that had previously been dismissed or settled.&nbsp;</p>
<p class="p3">3. <em>In which we go to the library</em></p>
<p class="p2">Annotation is the process of working out that your association between <a href="http://www.catb.org/jargon/html/T/TLA.html">TLA</a> levels and genotype at&nbsp; rs5551212 is uninteresting because that variant is in the TLA kinase gene and we already know about it.&nbsp; Or, occasionally, not.&nbsp;</p>
<p class="p2">4. <em>In which we may not be broken down by age and sex, but do discriminate based on ancestry</em></p>
<p class="p2">Confounding works differently in genetics, because your genome gets fixed before you are born.&nbsp; How model selection for main effects and interactions works in this context.&nbsp; How confounding by ancestry can cause problems, and what can be done about it.&nbsp;</p>
<p class="p3">5. <em>In which we make up data</em></p>
<p class="p2">You have measured a million SNPs, but that's a tiny fraction of all the known ones. How we impute unmeasured SNPs.&nbsp;</p>
<p class="p3">6. <em>In which we have friends</em></p>
<p class="p3">Genetic effects tend to be really, really small. A single cohort study typically isn't big enough to be useful, so we need to work together in larger consortia.&nbsp; Combining results without sharing individual data is important. So is playing nicely with the other children.&nbsp;</p>
<p class="p3">7.&nbsp;<em> In which we do not take one thing at a time</em></p>
<p class="p3">In principle, a meaningful genetic difference could involve multiple SNPs in a complicated non-additive way. How might we tell?</p>
<p class="p3">Also, when looking at very rare genetic variants there's no real point studying them individually. How can we group them to increase power?</p>
<p class="p3">&nbsp;8. <em>In which we worry about the future</em></p>
<p class="p3">If there is still time and energy, a brief lecture on some things we might have in the future, such as molecular haplotyping and reliable functional annotation.&nbsp;</p>]]></content:encoded>
    <wp:post_name>76183745465</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/76138681207</link>
    <pubDate>Mon, 10 Feb 2014 08:42:00 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/76138681207</guid>
    <!--<wp:post_id>76138681207</wp:post_id>-->
    <wp:post_date>2014-02-09 11:42:00</wp:post_date>
    <wp:post_date_gmt>2014-02-09 19:42:00</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>This is a wug. Now you have two of them.</title>
    <description></description>
    <content:encoded><![CDATA[<p>Three words that used to be plurals, and are changing in three different ways.</p>
<p><em>Candelabra</em>&nbsp;used to be the plural of <em>candelabrum</em>, a multiple-armed candlestick holder. There are very few other English words ending in 'brum', and most of the words ending in 'bra' are singular (e.g. <em>vertebra</em>, <em>penumbra</em>, <em>cobra</em>, <em>zebra</em>, <em>sabra, bra</em>). Over time, <em>candelabra</em>&nbsp;has been used more and more often as the singular, perhaps most famously in the biographical movie "Behind the Candelabra" about Liberace; the corresponding plural is <em>candelabras</em>. This change presumably started as a simple error; some authorities would say it still is.</p>
<p><em>Agenda</em>&nbsp;used to be the plural of <em>agendum</em>, from the Latin "which ought to be done". People would write lists of things which ought to be done, and write "Agenda" at the top of the lists rather than "Things to do", because they were pretentious and British. &nbsp;Over time, <em>agenda</em>&nbsp;became the name of the list itself, rather than the items, and specifically the name of a list of things to be done at a meeting; the corresponding plural is <em>agendas</em>. There's no error involved, just metonymy.</p>
<p><em>Data</em>&nbsp;used to be the plural of <em>datum</em>, from the Latin "was given". In statistics, you usually need a set of data, and the numbers are only really meaningful in context, not on their own. The new use has been to treat <em>data</em> as a mass noun, like <em>information</em>, something for which singular and plural are not relevant. There's no error involved; there is a change of meaning.</p>
<p><span>Mass nouns take the same verb forms as singular nouns, so underinformed pedant wannabes sometimes claim <em>data</em>&nbsp;is being treated as singular. The simplest way to see that this claim is wrong is that <em>data</em>&nbsp;has no plural. No-one regards <em>datas</em>&nbsp;as correct English, although new singular count nouns </span><span></span><a href="http://www.pbs.org/wgbh/nova/blogs/secretlife/blogposts/whats-a-wug/">very reliably form plurals with 's'</a><span>.</span></p>]]></content:encoded>
    <wp:post_name>76138681207</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/69955128874</link>
    <pubDate>Sat, 14 Dec 2013 19:55:00 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/69955128874</guid>
    <!--<wp:post_id>69955128874</wp:post_id>-->
    <wp:post_date>2013-12-13 22:55:00</wp:post_date>
    <wp:post_date_gmt>2013-12-14 06:55:00</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>At risk of vanishing</title>
    <description></description>
    <content:encoded><![CDATA[<p>A degree in science, in addition to specific facts about squid, neutrinos, or palladium-catalysed cross-couplings, should teach students what to do with questions about the world. In particular, they should learn to think about what the implications would be of each answer to the question, and know how we might use these implications to rule out some of the answers and reduce our uncertainty about others.</p>
<p>A degree in the humanities, in addition to specific facts about tenses in French, resource-allocation procedures in village societies, or the development of the Sangam literature,should teach students what to do with questions about the world. In particular, they should learn to think about what questions should be asked on a particular topic, the different ways these could be answered, and whose interests are served by systems that promote one question or answer over another.</p>
<p>Of course, there's some overlap in all disciplines, and a lot in some disciplines (such as, say, linguistics, statistics, geography, sociology), but I think a lot of academics would recognise these divisions, for good or bad.</p>
<p>So, when faced with <a href="http://www.nzherald.co.nz/nz/news/article.cfm?c_id=1&amp;objectid=11172286">a claim</a> that by adding extra tertiary places in science and engineering</p>
<blockquote>
<p><em>The country could lose "an informed and thoughtful citizenry which understands the history and cultures of a diverse nation and supports social and economic innovation and international engagement".</em></p>
</blockquote>
<p>we'd hope that someone with science training would ask if there's any empirical support for the idea that people with science degrees are less informed and thoughtful, or less supportive of social and economic innovation and international engagement. We'd also hope that they would have some idea how empirical support or refutation could be generated if it wasn't available.</p>
<p>We'd &nbsp;hope that someone with humanities training might wonder who was making this claim, and why the media thought it was the most important issue about current funding allocations to NZ universities, and which personal interests and <a href="http://en.wikipedia.org/wiki/The_Two_Cultures">historical</a> power structures this choice of issue tends to assume and reinforce.</p>
<p>Hey, I&nbsp;didn't start this. And it's not hard to find an argument for increased humanities funding that I'd support. But this wasn't it.</p>
<p><img alt="image" src="https://31.media.tumblr.com/6de39d4f4cf91450a5302555dc0b76fb/tumblr_inline_mxs9w6nSf71s1hdxy.png" /></p>
<p></p>]]></content:encoded>
    <wp:post_name>69955128874</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/67132955516</link>
    <pubDate>Sat, 16 Nov 2013 20:26:00 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/67132955516</guid>
    <!--<wp:post_id>67132955516</wp:post_id>-->
    <wp:post_date>2013-11-15 23:26:00</wp:post_date>
    <wp:post_date_gmt>2013-11-16 07:26:00</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Moving the goalposts?</title>
    <description></description>
    <content:encoded><![CDATA[<p>There's a <a href="http://www.pnas.org/content/early/2013/10/28/1313476110.full.pdf+html?with-ds=yes">paper in PNAS</a> suggesting that lots of published scientific associations are likely to be false, and that Bayesian considerations imply a p-value threshold of 0.005 instead of 0.05 would be good. It's had an impact outside the statistical world, eg, with a<a href="http://arstechnica.com/science/2013/11/is-it-time-to-up-the-statistical-standard-for-scientific-results/"> post on the blog Ars Technica</a>. &nbsp;The motivation for the PNAS paper is <a href="http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2009.00730.x/full">a statistics pape</a>r showing how to relate p-values to Bayes Factors in some tests.&nbsp;</p>
<p>Some people have asked me what I think. So.&nbsp;</p>
<p><span>1. I much prefer </span><a href="http://www.tandfonline.com/doi/abs/10.1198/tast.2010.09060#.UocYApT0-Fc">the other way</a><span>&nbsp;(</span><a href="http://faculty.washington.edu/kenrice/testingrev2a.pdf">non-paywalled tech report</a><span>) to get classical p-values as part of an optimal Bayesian decision, because it's based on estimation rather than on identifying arbitrary alternatives, and it seems to correspond better to what my scientific colleagues are trying to do with p-values. Ok, and because Ken is a friend.</span></p>
<p>2. The PNAS paper starts off by talking about reproducibility in terms of scientific fraud and slides into talking about publishing results that don't meet the proposed new p&lt;0.005. I'm not exaggerating: here's the complete first paragraph</p>
<div class="page" title="Page 1">
<div class="layoutArea">
<div class="column">
<blockquote>
<p><em>Reproducibility of scientific research is critical to the scientific endeavor, so the apparent lack of reproducibility threatens the credibility of the scientific enterprise (e.g., refs. 1 and 2). Unfortunately, concern over the nonreproducibility of scientific studies has become so pervasive that a Web site, Retraction Watch, has been established to monitor the large number of retracted papers, and methodology for detecting flawed studies has developed nearly into a scientific discipline of its own&nbsp;</em></p>
</blockquote>
<p>That's not a rhetorical device I'm happy with, to put it mildly.&nbsp;</p>
<p>3. If you don't use p-value thresholds as a publishing criterion, the change won't have any impact. And if you think p-value thresholds should be a publishing criterion, you've got <a href="http://www.alltrials.net/">worse problems than reproducibility</a>.</p>
<p>4. <a href="http://statistically-funny.blogspot.co.nz/2013/06/studies-of-cave-paintings-have-shown.html">False negatives are errors, too</a>. &nbsp;People already report "there was no association between X and Y " (or worse "there was no effect of X on Y") in subgroups where the p-value is greater than 0.05. &nbsp;If you have the same data and decrease the false positives you have to increase the false negatives.&nbsp;</p>
<p>5. The problem isn't the threshold so much as the really weak data in a lot of research, especially small-sample experimental research [large-sample observational research has <a href="http://en.wikipedia.org/wiki/Confounding">different problems</a>]. &nbsp;Larger sample sizes or better experimental designs would actually reduce the error rate; moving the threshold only swaps which kind of error you make.</p>
<p>6. Standards are valuable in scientific writing, but only to the extent that they reduce communication costs. That applies to statistical terminology as much as it applies to structured abstracts. Changing standards imposes substantial costs and is only worth it if there are substantial benefits.&nbsp;</p>
<p>7. And finally, why is it a disaster that a single study doesn't always reach the correct answer? Why would any reasonable person expect it to? It's not as if we have to ignore everything except the results of that one experiment in making any decisions. &nbsp;</p>
<p></p>
</div>
</div>
</div>]]></content:encoded>
    <wp:post_name>67132955516</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/66056322820</link>
    <pubDate>Tue, 05 Nov 2013 15:45:35 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/66056322820</guid>
    <!--<wp:post_id>66056322820</wp:post_id>-->
    <wp:post_date>2013-11-04 18:45:35</wp:post_date>
    <wp:post_date_gmt>2013-11-05 02:45:35</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>From labhacks: the $25 scrunchable scientific poster</title>
    <description></description>
    <content:encoded><![CDATA[<p><a class="tumblr_blog" href="http://labhacks.tumblr.com/post/62420107780/the-25-scrunchable-scientific-poster">labhacks</a>:</p>
<blockquote>
<p><img alt="image" src="http://media.tumblr.com/247357571238f63e85d5d48c5636b7e0/tumblr_inline_mtsgdeFxBE1rk6chf.jpg" /></p>
<p>Printed on <a href="http://www.spoonflower.com/performance">Spoonflower performance knit</a> at 300 dpi. 36&rdquo; x 56&rdquo;, vivid colors, no unraveling, and minimal wrinkling, even after being stuffed in a backpack. Hangs straight with about 8 pins. Print cost is $22 with $3 shipping.</p>
<p><img alt="image" src="http://media.tumblr.com/7fb4e3b1a7133dcee64254d1dbc0320b/tumblr_inline_mtsgzkXrSp1rk6chf.jpg" /></p>
<p></p>
</blockquote>
<p></p>]]></content:encoded>
    <wp:post_name>66056322820</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/65509406583</link>
    <pubDate>Wed, 30 Oct 2013 21:14:00 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/65509406583</guid>
    <!--<wp:post_id>65509406583</wp:post_id>-->
    <wp:post_date>2013-10-30 1:14:00</wp:post_date>
    <wp:post_date_gmt>2013-10-30 08:14:00</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>A diversity of gifts, but the same spirit </title>
    <description></description>
    <content:encoded><![CDATA[<p>Peter Green used this line (from I Corinthians) for his Royal Statistical Society Presidential Address in 2003, which anyone interested in the future of statistics <a href="http://www.maths.bris.ac.uk/~mapjg/papers/PresAddress.pdf">should read.</a>&nbsp;I've been planning to steal it ever since then, and the time seems right.</p>
<p>Roger, Jeff, and Rafa at Simply Statistics are holding an <a href="https://plus.google.com/events/cd94ktf46i1hbi4mbqbbvvga358">unconference on the future of statistics,</a> some time before dawn tomorrow morning New Zealand time. I probably won't be attending, but if you're in a more compatible time zone it promises to be interesting. It's also sparked some Twitter chatter on the future of statistics. As you'd expect given the promoters, the chatter has focused on the importance of computation and applications and argued that theory is overrated.&nbsp;To some extent I agree, but I'm writing this in defense of methodological pluralism.&nbsp;</p>
<p>I think it's unquestionably true that the academic statistical community overvalues mathematical formalism. It can be easier to publish sterile generalisations or pointless complications of mathematical statistics than useful simulation studies or high-quality applications. Much of the community has not really caught up with the fact that computation is thousands of times cheaper than it was three decades ago, and this has real implications for the best ways to solve problems. &nbsp;My colleague Alastair Scott (Chicago, 1965) tells the story of joining a discussion with other members of his generation about the most important advances in statistics over their careers. He suggested computing, which had not been brought up &nbsp;by anyone else and was received with some surprise.&nbsp;</p>
<p>On the other hand, some of the discussion reminds me of non-statisticians finding that, say, Andrew Gelman or Don Rubin is more knowledgeable and sensible than whoever taught them statistics in QMETH 101 and taking this as strong evidence for Bayesian statistics over frequentist statistics. It's certainly true that, say, Hadley Wickham or Roger Peng's research is of more benefit to humankind than the median piece of asymptotic statistics. But that's mostly because they are really smart and hardworking. If everyone learned lots of statistical computing and graphics and less theory it wouldn't turn everyone into Roger or Hadley. It would mean that useless papers on Edgeworth expansions of the overgeneralized beta distribution were replaced by useless simulations or ungeneralisable data analyses or pointless graphs. Beating <a href="http://en.wikipedia.org/wiki/Sturgeon's_Law">Sturgeon's Law</a> just isn't that easy -- as any journal editor can tell you.&nbsp;</p>
<p>The point of mathematical statistics is that it tells you how to simplify certain problems that are too hard to think about heuristically. That's only a minority of scientific problems, but it's an important minority. &nbsp;A huge amount of cognitive effort has gone into developing mathematical tools for thinking about inference, and these tools are valuable today. I've given some examples in past posts, and lots of people could give you others.</p>
<p>So, what would I recommend? Diversity. &nbsp;The heretics are right that we shouldn't have <strong>all</strong> PhD programs teaching two semesters from <a href="http://www.amazon.com/Testing-Statistical-Hypotheses-Springer-Statistics/dp/0387988645">TSH</a> and <a href="http://www.amazon.com/Theory-Point-Estimation-Springer-Statistics/dp/0387985026/ref=pd_sim_b_1">TPE</a>; but some programs should. Some of them should focus on computation and algorithms. Some should teach more modern theory from<a href="http://www.amazon.com/Asymptotic-Statistics-Statistical-Probabilistic-Mathematics/dp/0521784506/ref=pd_sim_b_3"> van der Vaart</a> instead. Some, like Santa Cruz and Duke, should focus on decision theory and Bayesian methods. Perhaps some should just concentrate on particular areas of application.</p>
<p>Statistics needs Savage Bayesians and moderate borrowing-strength Bayesians; we need applied statisticians who know the difference between DNA and RNA and probabilists who know the difference between l<sub>2</sub> and L<sub>2</sub>(P); we need Big Data and randomised experiements; and we need mathematical statisticians who understand why asymptotic approximations are useful.</p>
<p>I spent<a href="https://www.google.co.nz/webhp?sourceid=chrome-instant&amp;ion=1&amp;espv=2&amp;ie=UTF-8#q=r-alpha%20thomas%40biostat.washington.edu"> a lot of time and effort</a> on statistical computing before it became popular, against the advice of my seniors. &nbsp;I understand the attraction in elevating Chambers and Friedman and casting down Cramer and Kolmogorov. I can see the poetic justice in mathematical statistics becoming a peripheral subject in graduate programs. But I think it would be a terrible waste.&nbsp;</p>]]></content:encoded>
    <wp:post_name>65509406583</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/65316216749</link>
    <pubDate>Mon, 28 Oct 2013 19:09:04 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/65316216749</guid>
    <!--<wp:post_id>65316216749</wp:post_id>-->
    <wp:post_date>2013-10-27 23:09:04</wp:post_date>
    <wp:post_date_gmt>2013-10-28 06:09:04</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Interaction: 'real' and statistical</title>
    <description></description>
    <content:encoded><![CDATA[<p>Confounding is a model-independent property of nature: if doing A has a particular effect on Y, it is objectively either true or untrue that the conditional distributions of Y given A and not A match that particular effect.&nbsp;</p>
<p>Interaction or effect modification is scale-dependent: you ask <em>"is the effect of A on X in the presence of B the same as the effect of A on X in the absence of B."</em>&nbsp;This requires reducing "the effect" to a single number or other low-dimensional summary. &nbsp;If A and B both have an effect on X there must be summaries that show an interaction -- the effect can't be both exactly additive and exactly multiplicative, for example -- so interaction is intrinsically more statistical and model-based than confounding.</p>
<p>Scientists often dismiss mere 'statistical interaction' and say they are interested in 'real' interaction. As they should be. But it's not that simple.</p>
<p>Two real-world examples show that even when everything is known there may not be a good answer to whether there is "really" interaction or "really" effect modification.</p>
<p>1. Antifolate antibiotics. &nbsp;Folate is essential for cell growth. It acts as a co-enzyme, taking part in reactions and then being recycled. There are two classes of antibiotic that act on folate: the sulfonamides prevent bacteria from synthesizing folate, and trimethoprim and its relatives prevent folate from being recycled after use.&nbsp;</p>
<p>Do these drugs interact?</p>
<ul>
<li>A biochemist says 'No'. They inhibit completely different enzymes and have no effect on each other</li>
<li>A microbiologist says 'Yes'. Blocking availability of folate in two ways allows bacteria to be killed (in a Petri dish) with much lower doses of the two drugs when they are combined</li>
<li>A clinician says "Kinda, but not really". Because of different absorption and distribution in the body, the two drugs don't really act synergistically. They are sometimes given together, but mostly to avoid resistance.&nbsp;</li>
</ul>
<p></p>
<p>2. Hib vaccination. &nbsp;This one is even simpler.</p>
<p>In Australia before the <em>Haemophilus influenzae</em> type B (Hib) vaccine, the Hib meningitis rate was 4.5/100000/year in indigenous communities and 1.7/100000/year in the rest of the population.</p>
<p>After the vaccine was introduced, the rate was 0.5/100000/year in indigenous communities and 0.1/100000/year elsewhere.</p>
<p>Did the vaccination increase or decrease the disparity in meningitis risk? It depends how you measure: the relative risk is higher, the risk differences is lower.&nbsp;</p>
<p></p>
<p>In both cases there is ambiguity, but in neither case are there any facts whose addition would settle the question.&nbsp;</p>]]></content:encoded>
    <wp:post_name>65316216749</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/64556449200</link>
    <pubDate>Sun, 20 Oct 2013 20:25:46 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/64556449200</guid>
    <!--<wp:post_id>64556449200</wp:post_id>-->
    <wp:post_date>2013-10-20 0:25:46</wp:post_date>
    <wp:post_date_gmt>2013-10-20 07:25:46</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Barren proxies</title>
    <description></description>
    <content:encoded><![CDATA[<p>In causal inference it is often the case that you can't obtain a confounding variable directly, you can only measure something that it affects. &nbsp;Judea Pearl correctly points out the danger of conditioning on a 'barren proxy' for a confounder, in situations like this one:</p>
<p><img src="http://media.tumblr.com/2578eac1929368b1925969ddd1159ce1/tumblr_inline_muyh92BPuw1s1hdxy.png" /></p>
<p></p>
<p>A confounds the effect of B on C. D is affected by A but does not directly affect either B or C, so it is a 'barren proxy' for A.</p>
<p>It's easy to see that conditioning on D will not, in general, remove confounding by A. The problem, as so often with causal graphs, is where to draw the boundaries. Examined sufficiently closely, almost every variable in statistics is a barren proxy.&nbsp;</p>
<p>Suppose A is average particulate air pollution dose for people in a city and D is measured particulate air pollution concentration. &nbsp;The standard measurement technique is to force air through a filter and trap the pollution particles, which are then weighed. The particles that end up on the filter cannot have any effect on health; measured air pollution is a barren proxy for exposure.</p>
<p>Suppose A is blood glucose concentration. A blood drop is removed and fed into a testing device. The glucose in that drop of blood doesn't participate in future chemical reactions in the body; measured glucose is a barren proxy.</p>
<p>Suppose A is whether or not you have had a heart attack, and D is the conclusion from expert examination of your medical records. A subsequent examination of the medical records can't possibly have an impact on whether your heart muscle cells actually died during the event; diagnosis of heart attack is a barren proxy.&nbsp;</p>
<p>As a simple matter of fact, essentially all measured variables in medicine are barren proxies. That's not the important distinction, though. What actually matters is whether there are good causal reasons that the relationship between the true confounder and the measured variable is close, not just observationally but under intervention. &nbsp;That is, we should care whether D is a reliable measurement of A, not whether it is a barren proxy for A. Unfortunately, the criteria for being a reliable measurement are not simple and qualitative.&nbsp;</p>
<p></p>]]></content:encoded>
    <wp:post_name>64556449200</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/64459848750</link>
    <pubDate>Sat, 19 Oct 2013 19:59:44 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/64459848750</guid>
    <!--<wp:post_id>64459848750</wp:post_id>-->
    <wp:post_date>2013-10-18 23:59:44</wp:post_date>
    <wp:post_date_gmt>2013-10-19 06:59:44</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Google completions and sexism</title>
    <description></description>
    <content:encoded><![CDATA[<p>The <a href="http://mashable.com/2013/10/18/google-autocomplete-sexism/">new ads produced by UN Women</a> illustrating widespread sexism using Google autocomplete are pretty chilling, eg,&nbsp;</p>
<p><span><img src="http://media.tumblr.com/26a456e514ac98cfc3396e9bc222d7f8/tumblr_inline_muwlc78rP21s1hdxy.jpg" /></span></p>
<p></p>
<p><span>The ads are convincing and what they imply is true, but I'm less sure that they are actually good evidence for what they imply.</span></p>
<p><span>Typing whole phrases into Google is not how I or people I've watched usually search. I type key words. &nbsp;The only reason I would search for a phrase such as "Women should not speak in church" would be to find the source. I do this often when looking for scientific papers, and reasonably regularly when looking for quotes. And, in fact, I did just search for "Women should not speak in church" </span><span>-- I knew it was in one of the letters traditionally attributed to Paul, but I didn't remember which one (the Google tells me it's I Corinthians, and that this is one of the ones actually he did write).&nbsp;</span></p>
<p><span>If you think about it, what phrases starting "women should " would you get in an alternate universe where sexism had been mostly vanquished? Sentences starting off that way are just not going to end well.&nbsp;</span><span>The best you can hope for is that there aren't very many searches starting "women should ". One faint piece of support for this is that if I leave off the final space, one of the suggested completions is "women shoulder tattoos"</span></p>]]></content:encoded>
    <wp:post_name>64459848750</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/64280697576</link>
    <pubDate>Thu, 17 Oct 2013 19:43:25 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/64280697576</guid>
    <!--<wp:post_id>64280697576</wp:post_id>-->
    <wp:post_date>2013-10-16 23:43:25</wp:post_date>
    <wp:post_date_gmt>2013-10-17 06:43:25</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>What it says on the tin</title>
    <description></description>
    <content:encoded><![CDATA[<p>An interesting paper in <a href="http://www.biomedcentral.com/1741-7015/11/222">BMC Medicine</a>&nbsp;using DNA barcodes to see whether herbal medicine products claiming to have only a single plant ingredient actually included the plants that they claim to. &nbsp;More than half of the products had a problem: either filler from another plant such as alfalfa diluting the main ingredient, or contaminants from other plants, or in about 30%, &nbsp;just not including the main ingredient at all.</p>
<p>Here's their results broken down by company</p>
<p><img src="http://media.tumblr.com/622f37ab1c04f256a6f7f50614de464c/tumblr_inline_musv93Kwom1s1hdxy.jpg" /></p>
<p>The graph isn't ideal because they looked at an average of just under 4 products per company, and because the colour coding of 'contaminants' is very similar to 'substitution', when it should be more similar to 'authentic product', but it shows the problem.</p>
<p><a href="http://en.wikipedia.org/wiki/DNA_barcoding">DNA barcoding</a> involves sequencing of specific regions of the genome that vary between species enough to allow identification. It's about the only way to identify large numbers of plant species from ground leaf/root/stem material. &nbsp;For plants, the <a href="http://www.barcoding.si.edu/plant_working_group.html">barcoding standard</a> uses two coding regions of DNA. For animals, a mitochondrial gene is used, although this has problems in a few cases with <a href="http://en.wikipedia.org/wiki/Wolbachia">weird inheritance patterns</a>. For fungi, it's a non-functional chunk of mitochondrial DNA.</p>
<p>The same sort of approach has been used for testing <a href="http://phe.rockefeller.edu/barcode/sushigate.html">sushi</a>, and might be useful for quality assurance in specific types of honey, such as New Zealand's <a href="http://www.terrain.net.nz/friends-of-te-henui-group/table-1/manuka.html">manuka</a> honey, which comes with &nbsp;valuable (if largely unfounded) <a href="http://en.wikipedia.org/wiki/M%C4%81nuka_honey">claims of health benefits</a></p>
<p></p>]]></content:encoded>
    <wp:post_name>64280697576</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/63794571748</link>
    <pubDate>Sat, 12 Oct 2013 17:26:00 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/63794571748</guid>
    <!--<wp:post_id>63794571748</wp:post_id>-->
    <wp:post_date>2013-10-11 21:26:00</wp:post_date>
    <wp:post_date_gmt>2013-10-12 04:26:00</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title> Sense of entitlement</title>
    <description></description>
    <content:encoded><![CDATA[<p>Scientists get a lot of 'opportunities' to do stuff that helps other people and isn't directly part of our jobs. We get some credit for it under 'outreach' or 'service to the professional community', but often not enough to make up for time lost from research. Or sleep. &nbsp;Sometimes the opportunities come with money attached, whether for writing, consulting, or short-course teaching, other times not. &nbsp;Being able to decide which opportunities to take is part of the benefit (and risk) of the academic life.</p>
<p>But when I decide not to do something for free, the main risk is that I won't get asked again. &nbsp;D N Lee, who has a blog "<a href="http://blogs.scientificamerican.com/urban-scientist/">Urban Scientist</a>" at Scientific American, posted about an email exchange she had with 'Biology Online' about writing for them. When she turned them down, the email response said&nbsp;</p>
<blockquote>
<p>Because we don't pay for blog entries?</p>
<p>Are you an urban scientist or an urban whore?</p>
</blockquote>
<p>When Dr Lee posted about this on her blog at Scientific American, <a href="http://blogs.scientificamerican.com/urban-scientist/2013/10/11/give-trouble-to-others-but-not-me/">the post got taken down</a>, so far with no explanation. The exchange has been reposted at <a href="http://isisthescientist.com/2013/10/11/tell-someone-no-get-called-a-whore-standingwithdnlee-batsignal/">Isis the Scientist</a>.&nbsp;</p>
<p>My automatic reaction to anything like this is to want to hear the other side of the story first, but I can't imagine any other side of the story that would make much difference.&nbsp;</p>
<p>[update: some people are calling for the editor responsible to be fired. I'm not. The email was offensive, but not, for example, threatening.]</p>]]></content:encoded>
    <wp:post_name>63794571748</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/63674333781</link>
    <pubDate>Fri, 11 Oct 2013 10:33:00 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/63674333781</guid>
    <!--<wp:post_id>63674333781</wp:post_id>-->
    <wp:post_date>2013-10-10 14:33:00</wp:post_date>
    <wp:post_date_gmt>2013-10-10 21:33:00</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Do you know where it's been?</title>
    <description></description>
    <content:encoded><![CDATA[<p>Again this week on the bus I passed the annoying&nbsp;<a href="http://www.phoenixorganics.co.nz/content/why-we-do-it/default.aspx">Phoenix Organics</a> delivery van that says "<em>Don't drink science, you don't know where it's been</em>" (Phoenix are also notable for their <a href="http://www.donotlink.com/bF9">aspartame scare page</a>.)</p>
<p>One of the things they don't write up as glowingly is that they add the synthetic version of a natural antioxidant to their&nbsp;<a href="http://www.phoenixorganics.co.nz/what-we-do/browse/organic-juices.aspx">juices</a>&nbsp;and their (naturally high-fructose) <a href="http://www.phoenixorganics.co.nz/what-we-do/browse/organic-sparkling-juice-drinks.aspx">juice&nbsp;drinks</a>, in unnaturally high concentrations.&nbsp;That's perfectly legal by organic labelling rules, and the compound,&nbsp;ascorbic acid, is the same molecule as vitamin C -- but we're talking about an industry where 'the same molecule' doesn't usually cut it as an excuse.&nbsp;</p>
<p>And it's not just that the ascorbic acid is synthetic. Almost all<a href="http://en.wikipedia.org/wiki/Vitamin_C#Industrial_synthesis"> commercial vitamin C</a> is made by Chinese chemical companies,with just one <a href="http://www.dsm.com/corporate/home.html">Scottish chemical company</a> remaining in competition. Again, that's not something I have a problem with, but customers of Phoenix Organics might well feel differently if they knew.</p>]]></content:encoded>
    <wp:post_name>63674333781</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/63253329095</link>
    <pubDate>Sun, 06 Oct 2013 21:15:00 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/63253329095</guid>
    <!--<wp:post_id>63253329095</wp:post_id>-->
    <wp:post_date>2013-10-06 1:15:00</wp:post_date>
    <wp:post_date_gmt>2013-10-06 08:15:00</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title></title>
    <description></description>
    <content:encoded><![CDATA[<p><em>Background: <a href="http://www.salon.com/2013/05/17/revenge_ego_and_the_corruption_of_wikipedia/">Revenge, ego, and corruption in Wikipedia</a></em></p>
<p>Today we have shaming of prats. Yesterday,&nbsp;<br />we had unclear sources, and tomorrow morning<br />we may have original research. But today,&nbsp;<br />Today we have shaming of prats. <em>The <a href="http://en.wikipedia.org/wiki/Flame_robin">Flame Robin</a>&nbsp;</em><br /><em>is a small passerine bird native to Australia</em>,<br />and today we have shaming of prats</p>
<p>This is the anonymous editor. And this<br />is the revenge edit, whose use you will see<br />when you are snubbed by an author.<br /> And this is the appeals process.<br />Which in your case you have not got.<a href="http://en.wikipedia.org/wiki/Final_fantasy_XI"><em> Final Fantasy XI&nbsp;</em></a><br /><em>is a massively multiplayer online role-playing game,&nbsp;</em><br />which in my case I do not get. &nbsp; &nbsp;&nbsp;</p>
<p>This is the revision process,&nbsp;which maintains<br />the history of each page.&nbsp;And please do not let me<br />&nbsp;catch any user editing his own page.&nbsp;It is perfectly easy<br />&nbsp;if you sign in as a different user.<em><a href="http://en.wikipedia.org/wiki/A_Journey">A Journey</a>&nbsp;<br /></em><em>is a 2010 memoir by Tony Blair</em><em>&nbsp;</em><em>discussing&nbsp;<br /></em><em>his tenure as leader of the British Labour Party.<br /></em>You wouldn't catch&nbsp;him editing his own pages.</p>
<p></p>
<p>And this you can see is the talk page. The purpose of this<br />is to clear the air, as you see. We can argue<br />rapidly backwards and forwards: we say this&nbsp;<br />raises the standards. <em>When <a href="http://en.wikipedia.org/wiki/Oriflamme#Significance_on_the_battlefield">the Oriflamme</a> was&nbsp;</em><br /><em>displayed on the battlefield it indicated<br /> that no quarter was to be given</em>.&nbsp;<br />They called this raising the standard.</p>
<p>They call it raising the standard: it is perfectly easy&nbsp;<br />if you sign in as a different user: like the revenge edit,&nbsp;<br />and the anonymous editor, and the appeals process&nbsp;<br />(which in our case we have not got), and the massive<br /> online role-playing games,&nbsp;and the Oriflamme <br />displayed on the battlefield,&nbsp;for today<br /> we have shaming of prats.</p>
<p></p>
<p>(after <a href="http://www.solearabiantree.net/namingofparts/namingofparts.html">Henry Reed</a>)</p>]]></content:encoded>
    <wp:post_name>63253329095</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/63237480043</link>
    <pubDate>Sun, 06 Oct 2013 16:49:12 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/63237480043</guid>
    <!--<wp:post_id>63237480043</wp:post_id>-->
    <wp:post_date>2013-10-05 20:49:12</wp:post_date>
    <wp:post_date_gmt>2013-10-06 03:49:12</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Rock, paper, scissors, Wilcoxon test</title>
    <description></description>
    <content:encoded><![CDATA[<p><em>Based on my nerdnite talk last week.</em></p>
<p>Transitivity is a basic property of orderings: if A is better than B and B is better than C, then A must be better than C. &nbsp;For example, if the All Blacks beat Tonga and Tonga beats Japan, we would expect the All Blacks to beat Japan.</p>
<p>Rock-paper-scissors is interesting because it is the opposite: if A beats B and B beats C then A must lose to C. &nbsp;The 'winning' relationship is non-transitive -- as non-transitive as possible. &nbsp;In this case that's the whole point, since you want all the choices to be equally good if you don't know what your opponent is going to do.&nbsp;</p>
<p>In preparation for the talk I looked at the history of rock-paper-scissors, which largely meant following links from Wikipedia until they reached someone who looked as though they knew what they were talking about.</p>
<p>There's a very<a href="http://ir.minpaku.ac.jp/dspace/handle/10502/750"> interesting paper by a Finnish ethnologist</a> (!) on the history of the rock-paper-scissors games in Asia. They date back to the Han dynasty in China, and became popular in Japan, partly as a drinking game. There were lots of variants (frog-snake-snail, fox-mayor-hunter, hero-tiger-hero's mother) as well as the familiar rock-paper-scissors. The game apparently arrived in Europe in the early 20th century.</p>
<p>The paper also mentions the existence of five-way versions, eg one in Malaysia. These are independent of the more-familiar <a href="http://www.samkass.com/theories/RPSSL.html">rock-paper-scissors-lizard-Spock</a>, which was invented by Sam Kass and Karen Bryla, and popularised by the TV show "<a href="http://www.imdb.com/title/tt1256039/synopsis">The Big Bang Theory</a>".&nbsp;Of course, once you raise the possibility of more than three categories, <a href="http://www.sheldonsfans.com/quote-383-rock-paper-scissors-lizard-spock.html">people</a> <a href="http://www.umop.com/rps11.htm">get</a> <a href="http://www.umop.com/rps25.htm">creative</a>.</p>
<p>There are also examples of frequency-dependent selection preserving non-transitive games in evolution. The <a href="http://en.wikipedia.org/wiki/Common_side-blotched_lizard">common side-blotched lizard</a>&nbsp;comes in three colours with different mating strategies, and for these to all stay present in the population something strange must happen, <a href="http://www.pnas.org/content/97/26/14427.full">and it does</a>.</p>
<p>For statisticians, the most familiar example of non-transitive games is probably the <a href="http://plus.maths.org/content/non-transitiv-dice">non-transitive dice</a> first discovered by Brad Efron in the early 1970s. &nbsp;Efron's four dice have numbers</p>
<p></p>
<p></p>
<ul>
<li>4, 4, 4, 4, 0, 0</li>
<li>3, 3, 3, 3, 3, 3</li>
<li>6, 6, 2, 2, 2, 2</li>
<li>5, 5, 5, 1, 1, 1</li>
</ul>
<p>Each die beats the following one with probability 2/3, and the last one beats the first with probability 2/3.&nbsp;</p>
<p>It's surprising that numbers can behave this badly, but we're not really comparing single numbers; we're comparing whole probability distributions. &nbsp;Translating ordering of values into order of distributions turns out to be surprisingly hard.</p>
<p>The problem is related to design of voting systems: how do you turn a set of individual preferences into an ordering of candidates, so you can find the 'most preferred' candidate. &nbsp;Condorcet, in the late 18th century, noticed that this is hard -- if you have three candidates, it's possible that each candidate would beat one of their competitors and lose to the other one in two-way elections. That is, two-way voting comparisons can have the rock-paper-scissors property. &nbsp;Kenneth Arrow nailed down more of the details, coming up with a simple set of obviously necessary properties for a sane voting system and then<a href="http://en.wikipedia.org/wiki/Arrow's_impossibility_theorem"> showing that they were impossible to satisfy</a>.</p>
<p>However, for statisticians there is another important and much less well known aspect of the non-transitive dice. The dice show that when you compare probability distributions by 'probability that a random observation from A beats a random observation from B' you can get non-transitivity. This way of doing comparisons of probability distributions is better known in statistics as the Mann-Whitney U test or Wilcoxon rank-sum test. &nbsp;In fact, essentially all rank tests are non-transitive.&nbsp;</p>
<p>The reason rank tests are non-transitive is related to one of their apparent advantages: they don't depend on the scale of the data, and give the same result if you take an arbitrary (increasing) transformation. &nbsp;That's especially attractive for ordinal data where the appropriate scale may not be obvious. However, if you think in terms of disease and treatment it becomes clearer why scale independence is actually a bug, not a feature.&nbsp;</p>
<p>Suppose you have a treatment that makes some people better and other people worse, and you can't work out in advance which people will benefit. Is this a good treatment? The answer <strong>has to</strong> depend on the tradeoffs: <em>how much</em> worse and <em>how much</em> better, not just on how many people are in each group.&nbsp;</p>
<p>If you have a way of making the decision that doesn't explicitly evaluate the tradeoffs, it <strong>can't possibly</strong> be right. &nbsp;The rank tests make the tradeoffs in a way that changes depending on what treatment you're comparing to, and one extreme consequence is that they can be non-transitive. Much more often, though, they can just be misleading.</p>
<p>It's possible to prove that every transitive test reduces each sample to a single number and then compares those numbers [equivalent to Debreu's theorem in utility theory]. That is, if you want an internally consistent ordering over all possible results of your experiment, you can't escape assigning numerical scores to each observation. &nbsp;</p>
<p>In some scenarios, such as small-sample biological experiments where you expect the direction of effect to be the same for all experimental units, any test will give qualitatively the same true direction of change and rank tests may be sensible because they are exact in small samples.&nbsp;</p>
<p>In most settings, though, quite possible that different sets of scores will lead to different conclusions. For example, it's entirely plausible and does happen that a medical treatment can raise median medical costs (because it has to be paid for) but lower mean medical costs (by preventing expensive complications). That's sometimes used as an argument in favour of rank tests, but it's actually an argument against them.&nbsp;</p>]]></content:encoded>
    <wp:post_name>63237480043</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/63042673318</link>
    <pubDate>Fri, 04 Oct 2013 14:06:01 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/63042673318</guid>
    <!--<wp:post_id>63042673318</wp:post_id>-->
    <wp:post_date>2013-10-03 18:06:01</wp:post_date>
    <wp:post_date_gmt>2013-10-04 01:06:01</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Auckland's top news story</title>
    <description></description>
    <content:encoded><![CDATA[<p><em>Background: The Herald has had a whole <a href="http://www.nzherald.co.nz/nz/news/article.cfm?c_id=1&amp;objectid=11133832">sequence</a> of <a href="http://www.nzherald.co.nz/nz/news/article.cfm?c_id=1&amp;objectid=11134384">stories</a>, including two on the <a href="http://www.nzherald.co.nz/nz/news/article.cfm?c_id=1&amp;objectid=11133160">front</a> <a href="http://www.nzherald.co.nz/nz/news/article.cfm?c_id=1&amp;objectid=11133907">page</a>, about the decision of the city council to stop mowing the 'berms', the strips of grass between the road and sidewalk.&nbsp;</em></p>
<p><em>Additional background: the Mayor of Auckland is one Len Brown</em>.</p>
<p></p>
<p>O Lenny Boy, the berms, the berms need mowing<br />From Glen to Lynn, and down Mt Eden side<br />The winter's gone, and all the grass is growing<br />'Tis you, 'tis you must mow, and let us bide</p>
<p>And come ye back, at two or three week intervals<br />Or when the weeds and clover start to show<br />The grass will grow, in sunshine or in shadow<br />O Lenny Boy, we pay our rates, and you must mow</p>
<p></p>]]></content:encoded>
    <wp:post_name>63042673318</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/62048763550</link>
    <pubDate>Mon, 23 Sep 2013 21:03:00 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/62048763550</guid>
    <!--<wp:post_id>62048763550</wp:post_id>-->
    <wp:post_date>2013-09-23 2:03:00</wp:post_date>
    <wp:post_date_gmt>2013-09-23 09:03:00</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Statins and the causal Markov property</title>
    <description></description>
    <content:encoded><![CDATA[<p>A real example: there is developing uncertainty that the statin class of cholesterol-lowering drugs really works by lowering LDL cholesterol. This is partly because other drugs (eg, ezetimibe) that lower LDL cholesterol don't have the same impact on heart attacks, and also because statins seem to have beneficial effects on too many other conditions.&nbsp;</p>
<p>In principle, you could control for achieved cholesterol levels and see if statin use was then conditionally independent of heart disease [adjust and see if the effect goes away]. That's based on this causal graph</p>
<p><img alt="image" src="http://media.tumblr.com/3b45f6bb428b4195c3fb7f5c7daa2524/tumblr_inline_mtkm9ixrnO1s1hdxy.png" /></p>
<p></p>
<p></p>
<p>This doesn't really work. &nbsp;The big problem is that the actual mediating variable for <strong>all</strong> the potential effects of statins is the extent of inhibition of hydroxymethylglutaryl-coenzyme A reductase, so that most of the variation in effects will be common to all the mechanisms. &nbsp;This is combined with measurement error, so that measured cholesterol levels don't capture all the effect of statins and do introduce other variation. &nbsp;The causal graph actually looks more like this</p>
<p><img alt="image" src="http://media.tumblr.com/f690b03f5ae924afb6e380f254f174f8/tumblr_inline_mtkmf8Rwcr1s1hdxy.png" /></p>
<p></p>
<p>Much of the true variation in statin effect occurs upstream of the enzyme-inhibition bottleneck, and variation downstream of this is largely measurement error.&nbsp;</p>]]></content:encoded>
    <wp:post_name>62048763550</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/61934105353</link>
    <pubDate>Sun, 22 Sep 2013 20:09:00 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/61934105353</guid>
    <!--<wp:post_id>61934105353</wp:post_id>-->
    <wp:post_date>2013-09-22 1:09:00</wp:post_date>
    <wp:post_date_gmt>2013-09-22 08:09:00</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>PBRF consultation response consultation (edited)</title>
    <description></description>
    <content:encoded><![CDATA[<p><em>Friday week (October 4) is the deadline for providing input to the consultation on changes in the PBRF process (for foreigners: the national research evaluation program that allocates a chunk of long-term research funding to universities). Here's the <a href="http://www.minedu.govt.nz/NZEducation/EducationPolicies/TertiaryEducation/PolicyAndStrategy/~/media/MinEdu/Files/EducationSectors/TertiaryEducation/PBRF/PBRFConsultationDocument.pdf">consultation document</a>, if you haven't read it yet.</em></p>
<p></p>
<p><em>This is what I'm planning to say. It's also open for public feedback.</em></p>
<p><strong>Background</strong>: I was a member of the PBRF MIST review panel, and also submitted a portfolio. &nbsp;I moved to NZ from the USA in 2010, so this was my first involvement with the PBRF process.&nbsp;</p>
<p><strong>Observations</strong>.</p>
<p>0. Block funding that is not based on specific grant applications is tremendously beneficial to research in New Zealand. &nbsp;Evaluation and feedback is somewhat valuable.&nbsp;</p>
<p>1. In my (admittedly biased) opinion the PBRF process, at least in my field, produced good ratings of research. &nbsp;Before the panel process began, I was concerned that applied statistics, in particular, might be under-rated by the panel. This was not the case.</p>
<p>&nbsp;I'm not claiming the ratings are perfect -- there is not a well-defined 'right answer' and there may well have been some individuals who were misjudged -- but I cannot envisage a system that would do significantly better, and it is easy to see how it could be done much worse. Interdisciplinary fields such as statistics, in particular, can fare very badly under a bibliometric approach.&nbsp;</p>
<p>2. The system is very expensive. &nbsp;The consultation document quotes a figure of 4% of the total PBRF funds for the 6 year period, but since most of the effort was in a half-year period, that's nearly 50% in that 6 months.</p>
<p>&nbsp;The effort of compiling and optimising a research portfolio was required even for people who had no realistic prospect of a fundable rating. &nbsp;There was also a large burden on senior researchers both within the institutions and on the panels -- and if you believe the PBRF funding formula, their time is especially valuable.&nbsp;</p>
<p>3. The funding is tilted very strongly towards 'A's, a rating that even the best junior researchers will not be able to attain. &nbsp;This has implications for developing research excellence in New Zealand, and also for equity -- if our equity initiatives are working at all, top younger researchers will be more diverse than their seniors.&nbsp;</p>
<p>4. The main impact of the Christchurch earthquakes will be in the next round, not the last round. The quakes happened late enough that most research disrupted by them would not have been published in time to be eligible. &nbsp;</p>
<p>5. While 'Peer Esteem' could be conceptually distinct from 'Contribution to the Research Environment,' <em>evidence</em> of the latter is largely based on the former.&nbsp;</p>
<p><strong>Suggestions</strong>.</p>
<p>1. There should be a subcategory of the B grade for excellent but relatively junior researchers (eg, 5 or fewer years experience) which explicitly requires less substance beyond the Nominated Research Outputs and provides more funding than the standard B category. &nbsp;</p>
<p>2. Combine the 'Peer Esteem' and 'Contribution to the Research Environment' categories and reduce the number of examples allowed to, say, 20 for the combined category. &nbsp;Reducing to 8, as in the consultation document, is going too far, both in reducing information and in making researchers and institutions second-guess the panel's opinions.</p>
<p>3. Removing the Preliminary/Preparatory scores seems a serious mistake to me, though I expect panel chairs would ask panel members to pre-score applications and compare scores in any case. &nbsp;Committing to Independent scores before any discussion takes place, and requiring separate scoring for outputs and PE/CRE are important in ensuring careful evaluation, especially of senior people whose productivity has declined. Once the component scores exist, I can see no reason not to communicate them to the researcher.</p>
<p>4. Allow science outreach/science communication to qualify in the combined PE/CRE category. It is entirely appropriate for research funding to support science communication, and science communication should be recognised as a valuable product of the research process.</p>
<p>5. The 5:3:1 weighting for A:B:C grades is, in my opinion, excessive (and I'm one of the beneficiaries of it). It should be reduced either explicitly or by introducing extra grades for less senior researchers.&nbsp;</p>
<p>5. The TEC should consider carefully how to take into account the disruption in research in the Christchurch area for <em>at least</em> the first two years of the next PBRF cycle.&nbsp;</p>
<p>6. I do not think that drastically reducing the number of 'Other Research Outputs', as suggested in the consultation document, is helpful. &nbsp;This will slightly increase the workload on academics to select the appropriate subset, for relatively little benefit to the panels. There would be some reduction in validation effort for institutions and TEC, but the validation should be easier for new outputs than it has been for older ones.<span><br /></span></p>
<p><span>7. Mechanisms should be investigated for simply not submitting portfolios for staff who are clearly likely to receive 'R' grades. Compiling these portfolios is a pointless effort for the staff involved and evaluating them is a waste of time for the panels.&nbsp;</span></p>
<p><span>8. Special circumstances, in particular part-time work, should be allowed for even if it rarely makes a difference. There will sometimes be a difference, and it is important that there is some way for a researcher to be able to describe the problem and for the panel to be allowed to take it into account.</span></p>
<p><strong><span>Unrealistic dreams.</span></strong></p>
<p><span>A blbliometric system (including <a href="http://altmetrics.org/manifesto/">alternative metrics</a>) combined with panel evaluation of a stratified probability sample of research portfolios could give similar accuracy for funding allocations at the level of academic units, with enormously less work.&nbsp;<br /></span></p>
<p><span>I am not under the delusion that this has any chance of happening.&nbsp;</span></p>]]></content:encoded>
    <wp:post_name>61934105353</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/61099568913</link>
    <pubDate>Fri, 13 Sep 2013 21:06:41 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/61099568913</guid>
    <!--<wp:post_id>61099568913</wp:post_id>-->
    <wp:post_date>2013-09-13 2:06:41</wp:post_date>
    <wp:post_date_gmt>2013-09-13 09:06:41</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Upcoming talks</title>
    <description></description>
    <content:encoded><![CDATA[<p>"MonetDB and R for largish data"<a href="http://www.meetup.com/Auckland-R-Users-Group-AKLRUG/events/139579922/"> Auckland R User Group</a><span>: 25 September, basement of Science building, University of Auckland, 4pm</span></p>
<p><span>"Rock, paper, scissors: transitivity and statistical testing". Auckland Nerdnite, October 1, <a href="https://www.facebook.com/BarNectar">Nectar bar, Kingsland</a>, 7pm</span></p>
<p><span>Network meta-analysis.<a href="https://secure.orsnz.org.nz/conf47/"> New Zealand Statistics Association conference</a>, University of Waikato, November 25-27</span></p>
<p><span>R &amp; Bioconductor workshop (with Ken Rice). <a href="https://www.stat.auckland.ac.nz/uoa/home/events/template/event_item.jsp?cid=582324">University of Auckland</a>, November 28-29</span></p>
<p><span>"Semiparametric efficiency and nearly-true models" <a href="http://msor.victoria.ac.nz/Events/WWPMS2013/">Wellington Workshop in Probability and Mathematical Statistics</a>, December 2-4, Victoria University, Wellington.</span></p>
<p><span>&nbsp;</span></p>]]></content:encoded>
    <wp:post_name>61099568913</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/61098814157</link>
    <pubDate>Fri, 13 Sep 2013 20:40:00 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/61098814157</guid>
    <!--<wp:post_id>61098814157</wp:post_id>-->
    <wp:post_date>2013-09-13 1:40:00</wp:post_date>
    <wp:post_date_gmt>2013-09-13 08:40:00</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>An absolutely minimal way to increase invited speaker diversity</title>
    <description></description>
    <content:encoded><![CDATA[<p>The low proportion of women among invited speakers at conferences has finally become an issue in biology and computing and science fiction (at least for the people in my Twitter feed).</p>
<p>You might worry, if you were running a conference, that having some sort of minimum standard for diversity might lead to suboptimal speakers, or if you had a bunch of small sessions, might be difficult to ensure in each session. &nbsp;I think you'd be wrong on the first issue at least, but there are people of good will who feel that way.&nbsp;</p>
<p>A two-stage list gives you an absolutely minimal step forward. Suppose you want 10 invited speakers, and in a perfect world you'd like 5 to be women. Make a short list of at least 15 with at least 5 women, then select your speakers from the short list.</p>
<p>The point of this is that it stops you filling up the list and just not thinking of any of the suitable female speakers. You have to stop and think, and it occurs to you that Eve Rybody or Marge Inovera could go on the list. And then you remember that your mate Bruce liked the talk Dr Rybody gave at "Directions in Orientation 2011"</p>
<p>The same approach applies, <em>mutatis mutandis</em>, to any other under-represented group or groups -- perhaps you think increasing the number of talks by junior academics or by non-academic researchers or by people of colour is more important.&nbsp;</p>
<p>I'm not saying this will solve the problem or that it's a sufficient response, or even that it will save you from being first up against the wall when the revolution comes. But if you think under-representation of some group matters, I can't think of any principled reason ever to not do <em>at least</em> this much.&nbsp;</p>]]></content:encoded>
    <wp:post_name>61098814157</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/60268883295</link>
    <pubDate>Thu, 05 Sep 2013 02:06:11 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/60268883295</guid>
    <!--<wp:post_id>60268883295</wp:post_id>-->
    <wp:post_date>2013-09-04 7:06:11</wp:post_date>
    <wp:post_date_gmt>2013-09-04 14:06:11</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>What I said on StatsChat only shorter and with more swearing</title>
    <description></description>
    <content:encoded><![CDATA[<p>Stealing a <a href="http://publicaddress.net/onpoint/what-andrew-geddis-said-but-shorter-and-with/">Keith Ng</a>&nbsp;title, to do a post motivated by his criticism of my <a href="http://www.statschat.org.nz/2013/09/04/nz-taxbenefit-system-is-moderately-progressive/">StatsChat post</a>&nbsp;as a 'generous interpretation' of Bill English.</p>
<p>English said that households with income below $110000 collectively paid no net income tax. This assumes that all benefits are paid solely from income tax, not GST, and even then has to lump together people who receive more in benefits than they pay in income taxes with a lot of people who pay much more in income tax than they receive in benefits. Not fucking helpful when it comes to informed public debate.&nbsp;</p>
<p>You could do this lumping in other ways: I'm pretty sure all of these would be just as true if you ran the numbers:</p>
<ul>
<li><span>"MPs and unemployed collectively pay no net income tax"</span></li>
<li><span>"Famers and beneficiaries collectively pay no net income tax"&nbsp;</span></li>
<li><span>"Members of the National Party and beneficiaries collectively pay no net income tax"</span></li>
<li><span>"Beneficiaries and residents of </span><span></span><a href="http://en.wikipedia.org/wiki/Clutha-Southland">Clutha-Southland</a><span> collectively pay no net income tax"</span></li>
</ul>
<p></p>
<p></p>]]></content:encoded>
    <wp:post_name>60268883295</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/59059702266</link>
    <pubDate>Fri, 23 Aug 2013 13:10:00 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/59059702266</guid>
    <!--<wp:post_id>59059702266</wp:post_id>-->
    <wp:post_date>2013-08-22 18:10:00</wp:post_date>
    <wp:post_date_gmt>2013-08-23 01:10:00</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>On the persistence of variation in horn size among Soay sheep</title>
    <description></description>
    <content:encoded><![CDATA[<p><em>(<a href="http://www.bbc.co.uk/news/uk-scotland-highlands-islands-23769630">BBC News</a>)</em></p>
<p><em>The small-horned rams are fitter,&nbsp;<br />but the big-horned rams are phatter<br />And though we deemed it sweeter<br />To dally with the latter,<br />The small horns still stay with us<br />To Scottish boffins' wonder;<br />In flocks the nerds still pull the birds<br />When the jocks are six feet under</em></p>
<p><em>(After <a href="http://www.bartleby.com/246/103.html">Peacock</a>)</em></p>]]></content:encoded>
    <wp:post_name>59059702266</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/58580751706</link>
    <pubDate>Sun, 18 Aug 2013 19:26:30 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/58580751706</guid>
    <!--<wp:post_id>58580751706</wp:post_id>-->
    <wp:post_date>2013-08-18 0:26:30</wp:post_date>
    <wp:post_date_gmt>2013-08-18 07:26:30</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Why I'm not yet sure I oppose the new child protection orders</title>
    <description></description>
    <content:encoded><![CDATA[<p>There is a proposal to introduce new child protection orders that could be imposed by a judge based on evidence that the person had committed some serious form of child abuse and posed a serious risk in the future. These orders could last up to ten years and, based on Paula Bennett's statements could be very flexible in what they cover -- she even gave the example of banning someone from sitting next to a child on a bus.&nbsp;<span>Most significantly, perhaps, the evidence would only be assessed on a 'balance of probabilities' standard, not 'beyond reasonable doubt'. &nbsp;I wrote a StatsChat post on some issues related to the meaning of 'balance of probabilities'. In that post I deliberately didn't give my opinion about the proposals -- it's not a statistical issue, and I'm not an expert or even a parent or a child abuser, so I don't see why my views are especially relevant. However, a number of people wanted to know. So. &nbsp;</span></p>
<p>We don't yet know the text of the proposed bill, so a lot of the discussion is speculative. On purely procedural grounds this isn't as bad as the 'Skynet' copyright courts (where one of the first people fined <a href="http://www.stuff.co.nz/technology/digital-living/8419898/Soldier-pinged-by-Skynet-while-on-duty">produced evidence beyond any doubt</a> that he, personally, did not commit the copyright violations; this isn't enough under the law). &nbsp;On the other hand, the penalties are enormously more serious, as is the simple fact of being pronounced a probable child abuser.</p>
<p>We don't yet know what sort of evidence will be needed that someone poses a serious risk or what limitations are given on how the orders can be framed. &nbsp;Will the judge have discretion or be forced to impose an order if there is evidence? How much discretion will the judge have over the form of the order?&nbsp;Will it be possible to appeal the finding of a serious risk, will it be possible to appeal the appropriateness and necessity of the actual order, &nbsp;or will appeals be limited to errors in law? <em>[Some of this may be implied by other law, but I haven't seen it discussed. Why is Graeme Edgeler <a href="http://publicaddress.net/legalbeagle/">writing</a> about single transferable vote, which I already understand, instead of this?]</em></p>
<p>It's worth noting that the evidential standard is similar to that for restraining orders for stalking or domestic violence, which I don't have a problem with. &nbsp;The difference is that the scope of orders may be much greater -- domestic violence orders typically protect a single person and so can be narrowly framed; these new orders seem to be aimed at protecting all children everywhere and there's a real risk of writing them too broadly.&nbsp;</p>
<p>I'm probably going to be opposed to the law, but I do want to see it first.</p>
<p></p>]]></content:encoded>
    <wp:post_name>58580751706</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/58223740523</link>
    <pubDate>Wed, 14 Aug 2013 18:45:00 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/58223740523</guid>
    <!--<wp:post_id>58223740523</wp:post_id>-->
    <wp:post_date>2013-08-13 23:45:00</wp:post_date>
    <wp:post_date_gmt>2013-08-14 06:45:00</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>A layperson's view of a science communication problem</title>
    <description></description>
    <content:encoded><![CDATA[<p>There's <a href="http://www.stuff.co.nz/business/farming/dairy/9041155/Vet-links-botulism-to-farms-not-pipes">a story in one of the NZ papers</a>&nbsp;saying that Fonterra and the government are completely wrong about the source of the botulism contamination in milk products and about how to fix it.</p>
<p>This is a field I know very little about, so it's interesting to look at the story just from the point of view of an educated consumer.</p>
<p>There are some stylistic points that make the story look like it could be bogus: the claim that this one guy is right and everyone else is wrong, the reference to "sitting on material that will embarrass Fonterra further", blaming the problem on glyphosate (evil Monsanto's evil Roundup herbicide), the lack of any links or details for the research, and the lack of any independent scientific opinion. Also, the story is unclear over whether the claim is that the bacteria come from infected cows, or that the neurotoxin is already in the milk of infected cows.&nbsp;</p>
<p><span>I looked up 'botulism glyphosate' on PubMed, and found </span><span></span><a href="http://www.ncbi.nlm.nih.gov/pubmed/?term=botulism+glyphosate">one very new paper</a><span>&nbsp;that says diseases in cattle caused by the botulism bacterium have been increasing for the past 10-15 years, and showing that glyphosate suppressed the growth of one of the bacteria that competes with the botulism bacterium, and so might lead to more cattle being infected. That's supportive, but it's still only about diseases in cattle and is just test-tube research, so it's only modestly supportive.</span></p>
<p>The range of possibilities seems to be something like</p>
<ol>
<li>There really is this problem that Fonterra either doesn't know about or is covering up, that some of the milk is contaminated from the start</li>
<li><span>It's true but not all that important. &nbsp;Perhaps glyphosate really makes the bacteria more common, but there still had to be a failure of processing and bacteria growing in a pipe.</span></li>
<li><span>The guy's a crank</span></li>
</ol>
<p>What would be most helpful in ruling out some of these possibilities is a comment from an independent expert.&nbsp;<span>On a story of this importance, I would have expected the reporter to contact someone for a second opinion. Massey University has world-class vets and veterinerary epidemiologists, so they'd be the obvious source. Nigel French has already appeared in the media on this issue, so he'd be one place to start.</span></p>
<p></p>
<p><strong>Update</strong>: <a href="http://sciblogs.co.nz/guestwork/2013/08/16/a-dirty-pipe-a-vet-and-wild-claims-about-c-botulinum/?utm_source=CORRECTED+Corrected+HEADS-UP+16+-+22+AUG+2013&amp;utm_campaign=SMC+Heads-Up&amp;utm_medium=socialshare">Dr Heather Hendrickson</a>, from Massey, and<a href="http://sciblogs.co.nz/infectious-thoughts/2013/08/15/could-the-fonterra-botulism-scare-be-down-to-herbicide-use-and-gm-crops/?utm_source=CORRECTED+Corrected+HEADS-UP+16+-+22+AUG+2013&amp;utm_campaign=SMC+Heads-Up&amp;utm_medium=socialshare"> Dr Siouxsie Wiles</a>, from Auckland, who actually know about bacteria, have written posts on the issue, and it looks like this theory doesn't fly.&nbsp;</p>]]></content:encoded>
    <wp:post_name>58223740523</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/57747270796</link>
    <pubDate>Fri, 09 Aug 2013 12:34:01 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/57747270796</guid>
    <!--<wp:post_id>57747270796</wp:post_id>-->
    <wp:post_date>2013-08-08 17:34:01</wp:post_date>
    <wp:post_date_gmt>2013-08-09 00:34:01</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>SPEED sessions at JSM 2013</title>
    <description></description>
    <content:encoded><![CDATA[<p>This year, the Joint Statistical Meetings introduced a combined poster/short presentation session. &nbsp;The sessions took up a half-day, twice as long as the typical session. In the first half, each presenter gave a 5-minute talk, and the second session was an electronic poster session. &nbsp;</p>
<p>I signed up for one of these sessions primarily because I think any form of innovation at the Joint Statistical Meetings should be supported. For people who haven't experienced the JSM, it's the largest gathering of statisticians in the world, but it is also characterised by rigid and apparently inexplicable rules (eg, if the chair for your session doesn't show, you are supposed to find a replacement who isn't chairing any other session at the conference) and a significant minority of astoundingly awful talks.&nbsp;</p>
<p>Overall, I think the new sessions were a Good Thing. Some issues</p>
<ol>
<li>For static single-slide posters, paper is still far superior to a 42" TV screen. It would have been nice to have the option of a paper poster.</li>
<li>The real benefit of e-posters is interactivity. This requires the presenter to be allowed to use their own computer to drive the display.&nbsp;</li>
<li>Presenters need to be warned more clearly about the low effective resolution of the display -- I saw some posters where important chunks of text or maths were unreadable. An approximate but useful criterion is that the poster should be barely legible on an iPhone.&nbsp;</li>
<li>It wasn't clear what was supposed to happen in the oral session if a presenter was AWOL. &nbsp;The ideal solution would be to just call the next presenter.</li>
<li>The screens were identified by poster numbers that didn't appear in the online program, though they did correspond to the order in the online program.</li>
</ol>
<p>I expected there to be a problem with keeping to time in the 5-minute talks, but I was wrong. &nbsp;In two session (thirtysomething talks) only one presenter tried to go over time, and he was quickly stopped by the session chair.&nbsp;</p>]]></content:encoded>
    <wp:post_name>57747270796</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/57745243460</link>
    <pubDate>Fri, 09 Aug 2013 12:07:00 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/57745243460</guid>
    <!--<wp:post_id>57745243460</wp:post_id>-->
    <wp:post_date>2013-08-08 17:07:00</wp:post_date>
    <wp:post_date_gmt>2013-08-09 00:07:00</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>This is a test</title>
    <description></description>
    <content:encoded><![CDATA[<p>This is only a test. Had it been a real alarm ($e^{i\pi}$) everyone would have fled in panic</p>
<p>$$\textrm{panic}= \int_\infty^x \Phi(\eta)dP(\eta)$$</p>
<p>and you would not have been warned. (<a href="http://blog.angjookanazawa.com/post/15081007922/how-to-write-in-latex-use-mathjax-in-tumblr">thanks</a>)</p>]]></content:encoded>
    <wp:post_name>57745243460</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/57712446778</link>
    <pubDate>Fri, 09 Aug 2013 04:53:00 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/57712446778</guid>
    <!--<wp:post_id>57712446778</wp:post_id>-->
    <wp:post_date>2013-08-08 9:53:00</wp:post_date>
    <wp:post_date_gmt>2013-08-08 16:53:00</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>In defense of theory</title>
    <description></description>
    <content:encoded><![CDATA[<p>The statistical community, and even more so the statistical curriculum, hasn't yet adapted fully to the improvements in computing over the past few decades, and so still gives too much priority to mathematical approaches and too little to computational approaches to many problems. That's one reason for the popularity of the term 'data science', and for the mixed feelings about Nate Silver's comment at his JSM address that 'data scentist is just a sexed-up term for statistician'.</p>
<p>On the other hand, theory is important for people using statistics and computing to work on non-trivial scientific problems. The theory doesn't have to be developed by the same people that work on the scientific problems, but it does have to be developed.</p>
<p>Here are two simple examples where apparently pointless abstract theory has been relevant to applied medical research for me and my coworkers:</p>
<p>1. &nbsp;The case-crossover design and air pollution. One of the <a href="http://onlinelibrary.wiley.com/doi/10.1002/1099-095X(200011/12)11:6%3C689::AID-ENV439%3E3.0.CO;2-N/citedby">most-cited papers</a> in the journal <em>Environmetrics</em> is my work with Drew Levy on the case-crossover design. &nbsp;We showed that the standard approach to analysis, using a conditional likelihood based on analogies with matched case-control designs, was wrong. &nbsp;The objective function isn't actually a conditional likelihood, and for some popular designs the true conditional likelihood is free of the parameters of interest. &nbsp;</p>
<p>The key to our realisation of the problem was the fact that the score, the derivative of a log-likelihood always has zero mean under the true parameters. &nbsp;Not asymptotically zero. Zero. &nbsp;This let us be sure from finite-sample simulations that there was something wrong with the likelihood, by looking at the distribution of the score, where other researchers had looked at the distribution of the parameter estimates and found them inconclusive.</p>
<p>Once we knew that the 'conditional likelihood' couldn't possibly be right, it was much easier to find what was wrong with it.&nbsp;</p>
<p>2. The <a href="http://www.americanscientist.org/issues/pub/the-bootstrap/1">bootstrap</a> is a fantastically useful tool for data science, since it lets you generate uncertainty estimates for pretty much anything. It's easy to understand the bootstrap from a heuristic viewpoint -- so much so that it's now how interval estimation is introduced in the New Zealand high-school curriculum. &nbsp;</p>
<p><span>On the other hand, the bootstrap doesn't work for every statistic, and I've encountered two cases recently where people had proposed bootstrapping for a statistic where it doesn't work. &nbsp;</span><span>The first was the </span>I<a href="http://www.ncbi.nlm.nih.gov/pubmed/21673124">DI statistic</a><span> for improvements in predictive accuracy; the second was a proposed test for pleiotropy in genetic association analysis (ie, does a genetic variant affect more than one phenotype). </span></p>
<p><span>&nbsp;In both cases the problem was that the statistic was not regular at the null hypothesis. Simulations would show that the bootstrap failed (and led to the discovery of the problem in the IDI case), but it's useful to understand why it failed and to have a taxonomy of the possible reasons for failure. These come most reliably, I think, from the characterisation of the bootstrap as a plug-in estimator of the empirical CDF, and the use of the functional delta method.</span></p>]]></content:encoded>
    <wp:post_name>57712446778</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/57329928905</link>
    <pubDate>Mon, 05 Aug 2013 01:04:00 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[photo]]></category>
		<category domain="category" nicename="photo"><![CDATA[photo]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/57329928905</guid>
    <!--<wp:post_id>57329928905</wp:post_id>-->
    <wp:post_date>2013-08-04 6:04:00</wp:post_date>
    <wp:post_date_gmt>2013-08-04 13:04:00</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title></title>
    <description></description>
    <content:encoded><![CDATA[<div class="figure"><figure>
  <img src="http://78.media.tumblr.com/3404141cb34a94a092f583e40d121bbe/tumblr_mr0caweuay1sueztxo1_1280.png" alt="">
</figure></div>

    <p>My JSM2013 poster. Yes, it has too much text.&nbsp;</p>]]></content:encoded>
    <wp:post_name>57329928905</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/57329870050</link>
    <pubDate>Mon, 05 Aug 2013 01:02:00 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/57329870050</guid>
    <!--<wp:post_id>57329870050</wp:post_id>-->
    <wp:post_date>2013-08-04 6:02:00</wp:post_date>
    <wp:post_date_gmt>2013-08-04 13:02:00</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Some failure modes of statistics research talks</title>
    <description></description>
    <content:encoded><![CDATA[<p>Written before #JSM2013 actually starts, so it's not about your talk there.</p>
<p>Also, this is about deliberate choices by the presenter, and specifically about statistics research talks.&nbsp;</p>
<ol>
<li><span><em>"The Overgeneralized Beta Distribution"</em>. There is a place for new parametric distributions, but it's a fairly small place and mostly occupied by distributions derived from underlying substantive knowledge.</span></li>
<li><span><span><em>"Asymptotics of an uninteresting estimator"</em>. If there were a novel mathematical idea this would be fine, but otherwise we know its asymptotic behavior and roughly why it happens, and we can't read your notation fast enough anyway.</span></span></li>
<li><span><span><span><em>"A simple mathematical solution to a complex non-mathematical problem"&nbsp;</em>Includes, but is not limited to, straw-man Bayesian/Frequentist talks.&nbsp;<br /></span></span></span></li>
<li><span><span><span><em>"Small improvements from heroic assumptions"</em>. Yes, you can do second-order Cornish-Fisher expansions, but do you believe the distributional assumptions hold <em>that</em> accurately?&nbsp;</span></span></span></li>
<li><span><span><span><span>"<em>My model takes five pages!</em>" Predominantly, but not exclusively, a Bayesian problem. &nbsp;If you're solving a real problem don't fill all your slides with model and proposal distributions. If you're not? Eh. &nbsp;</span></span></span></span></li>
<li><span><span><span><span><em>"Implausible results from inadequate data"</em>&nbsp;You battled strong confounding, non-classical measurement error, and 90% missing data, and used clever statistical techniques to demonstrate that the conventional wisdom on health and exercise was completely wrong.</span></span></span></span></li>
<li><em><span><span><span><span><span>"Uninteresting results from inadequate data"</span></span></span></span></span></em>You battled strong confounding, non-classical measurement error, and 90% missing data, and&nbsp;and used clever statistical techniques to demonstrate&nbsp;that the conventional wisdom on health and exercise was completely correct.</li>
<li><span><span><span><span><span><em>"I did an analysis."</em> That's good for your clients or collaborators, but unless it helps us do one, this isn't the right venue.&nbsp;</span></span></span></span></span></li>
<li><span><span><span><span><span><span><em>"Mine is faster than yours" </em> Useful if it's true and the problem is computation-constrained, but it's not, and it's not.</span></span></span></span></span></span></li>
<li><span><span><span><span><span><span><span><em>"Small-sample efficiency comparisons"</em> These can't be comprehensive, so they are only useful when the scope of the real question is very narrow. Is there a reason you know the treatment has exactly the same effect on everyone?</span></span></span></span></span></span></span></li>
<li><span><span><span><span><span><span><span><em>"You need little teeny eyes for reading little teeny print"</em> And I left my opera glasses behind.</span></span></span></span></span></span></span></li>
<li><span><span><span><span><span><span><span><em>"It worked for Dr Ishihara"</em> He was actually&nbsp;<em>trying</em> to make his slides into vision test.&nbsp;<br /></span></span></span></span></span></span></span></li>
</ol>
<p><span><span><span><span><span><span><span><em>"I did an analysis"</em> is the least annoying of these, since the background is often interesting and the analysis sensible. It's also one of the few that would be a good talk in the right setting.&nbsp;</span></span></span></span></span></span></span></p>
<p>My own contribution to #3 is&nbsp;<a href="http://faculty.washington.edu/tlumley/multilevel.pdf">here</a>, but in partial defense (a) it was on a web page, not at a conference, (b) I was a student, and (c) it's less over-the-top and less incorrect than typical for the genre.</p>
<p><span>It's possible that my JSM poster will be a #9 failure, but I think it's a setting where users actually are computationally constrained and there isn't an easier way. &nbsp;</span></p>]]></content:encoded>
    <wp:post_name>57329870050</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/55617573015</link>
    <pubDate>Wed, 17 Jul 2013 06:16:00 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/55617573015</guid>
    <!--<wp:post_id>55617573015</wp:post_id>-->
    <wp:post_date>2013-07-16 11:16:00</wp:post_date>
    <wp:post_date_gmt>2013-07-16 18:16:00</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Phonetics</title>
    <description></description>
    <content:encoded><![CDATA[<p>No, not phonics, phonetics. In particular the existence of writing systems that represent the sounds in different languages and relate them to how your mouth moves.&nbsp;</p>
<p>Andrew Gelman <a href="http://andrewgelman.com/2013/07/14/learning-how-to-speak/">wrote about this</a> recently, and today Julian Wolfson tweeted a helpful guide to people attending the Joint Statistical Meetings (in Montreal):</p>
<p><span>Palais des Congr&egrave;s (convention center) = pah-LAY day cone-GRAY</span></p>
<p><span>That's a big step in the right direction, but for most US English speakers, 'ay' is a diphthong, and it doesn't match either of the French vowel sounds. You can't write French in English (as one of my former conductors used to say "Pretend this is a foreign language that's not pronounced like English")</span></p>
<p><span>In the International Phonetic Alphabet you'd have, I believe</span></p>
<ul>
<li>pa'l&epsilon; de kɔ̃'gʀ&epsilon; for the <a href="http://learn-foreign-language-phonetics.com/french-phonetic-transcription-converter.php?site_language=english">French</a></li>
<li>pa'leɪ̯ deɪ̯ koʊ̯n'gɹeɪ̯ for Julian's suggested transliteration</li>
<li>'paleɪ̯s d<a href="http://en.wikipedia.org/wiki/Open-mid_front_unrounded_vowel" title="Open-mid front unrounded vowel">ɛ</a>z 'koʊ̯<a href="http://en.wikipedia.org/wiki/Velar_nasal" title="Velar nasal">ŋ</a>gɹ<a href="http://en.wikipedia.org/wiki/Open-mid_front_unrounded_vowel" title="Open-mid front unrounded vowel">ɛ</a>ɪ̯z for 'pronounce this like English', but loudly and slowly</li>
</ul>
<p><span>&nbsp;</span></p>
<p><span>&nbsp;</span></p>]]></content:encoded>
    <wp:post_name>55617573015</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/55528427421</link>
    <pubDate>Tue, 16 Jul 2013 06:26:16 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/55528427421</guid>
    <!--<wp:post_id>55528427421</wp:post_id>-->
    <wp:post_date>2013-07-15 11:26:16</wp:post_date>
    <wp:post_date_gmt>2013-07-15 18:26:16</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Welfare as an addictive drug</title>
    <description></description>
    <content:encoded><![CDATA[<p>From the <a href="http://www.nzherald.co.nz/lifestyle/news/article.cfm?c_id=6&amp;objectid=10898616">NZ Herald today</a>&nbsp;</p>
<p><em>Doctors have been told that putting patients on welfare is akin to putting them on "an addictive debilitating drug ... not dissimilar to smoking".</em></p>
<p>Smoking is a really, really bad analogy here, since doctors would absolutely never recommend a patient starts smoking. &nbsp;It's hard to imagine how someone with a medical degree could come up with that analogy.&nbsp;</p>
<p>Welfare is hard to get off and probably bad for your health, but a better comparison would be something like sleeping pills or opioid analgesics: drugs that are risky, potentially dependence-inducing, and should be taken for the shortest possible time period, but that are absolutely medically necessary at times.</p>
<p>As with opioids, there probably are people on welfare who shouldn't be, and people not able to get on welfare who should be. And, as with opioids, most people don't like being on welfare.&nbsp;</p>
<p>And if you're going to put this sort of emphasis on the risk of welfare dependence, you need to worry more about the financial penalties for starting work. Today's Herald also has<a href="http://www.nzherald.co.nz/nz/news/article.cfm?c_id=1&amp;objectid=10898604"> a story </a>about someone facing effective marginal tax rates of over 100% for starting work.&nbsp;</p>]]></content:encoded>
    <wp:post_name>55528427421</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/55478947401</link>
    <pubDate>Mon, 15 Jul 2013 15:16:39 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/55478947401</guid>
    <!--<wp:post_id>55478947401</wp:post_id>-->
    <wp:post_date>2013-07-14 20:16:39</wp:post_date>
    <wp:post_date_gmt>2013-07-15 03:16:39</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Graphs and counterfactuals</title>
    <description></description>
    <content:encoded><![CDATA[<p>The two main ways of reasoning about cause and effect in statistics are causal graphs and counterfactuals.</p>
<p>With causal graphs, you write down variables and draw arrows representing direct effects of one variable on another, and then work with a set of axioms that summarise what it means for one variable to affect another.&nbsp;</p>
<p>With counterfactuals, you talk about the effect of a variable in terms of the difference between the actual outcome with the variable set one way and the 'potential outcome' if it had been set another way. &nbsp;This is a fairly natural way to think about cause and effect -- saying you have a hangover because you got drunk is saying that you have a hangover, and if you had not got drunk you would not have had a hangover. &nbsp;Reasoning about potential outcomes requires assumptions about conditional independence between variables.&nbsp;</p>
<p>Thomas Richardson (of UW) and James Robins (of Harvard) have <a href="http://www.csss.washington.edu/Papers/wp128.pdf">a new &nbsp;working paper</a> showing that graph and potential outcome approaches to causal inference are equivalent, and that the equivalence can be constructed in a much technically simpler and metaphysically more straightforward way than was previously available. &nbsp;</p>]]></content:encoded>
    <wp:post_name>55478947401</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/54900416402</link>
    <pubDate>Mon, 08 Jul 2013 19:42:28 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/54900416402</guid>
    <!--<wp:post_id>54900416402</wp:post_id>-->
    <wp:post_date>2013-07-08 0:42:28</wp:post_date>
    <wp:post_date_gmt>2013-07-08 07:42:28</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Sparse linear systems and calibration of weights</title>
    <description></description>
    <content:encoded><![CDATA[<p>Diego Zardetto (Italian national stats agency) wants to be able to calibrate sampling weights to population totals for regions. &nbsp;This leads to a very large number of calibration variables and solving large linear systems.</p>
<p>Using the Matrix package in R, we can compute sparse QR decompositions instead of the dense ones used in the survey package. &nbsp;Alternatively, using block-diagonal sparse matrices from the bdsmatrix package we can represent the linear system as a set of separate systems for each region.&nbsp;</p>
<p>Which approach works best is an empirical question, but both should be fairly easy to implement (when I get a spare day to do it).&nbsp;</p>]]></content:encoded>
    <wp:post_name>54900416402</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/54900159212</link>
    <pubDate>Mon, 08 Jul 2013 19:36:00 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/54900159212</guid>
    <!--<wp:post_id>54900159212</wp:post_id>-->
    <wp:post_date>2013-07-08 0:36:00</wp:post_date>
    <wp:post_date_gmt>2013-07-08 07:36:00</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Big data linear models</title>
    <description></description>
    <content:encoded><![CDATA[<p>The biglm package for R currently uses incremental QR decomposition, which fits linear models to big data in linear time and bounded memory, but doesn't parallelize.</p>
<p>It turns out that parallel computation is easy (and has been studied by Dongarra and the LAPACK folks). &nbsp;If you have two data chunks reduced to R_1 and Q_1^TY_1, and R_2 and Q_2^TY_2, just treat each R as an X and each Q^TY as a Y to merge the QR decompositions.</p>
<p>So, if you have a file system that can feed M separate QR decomposition processes, you can do the QR decomposition in M parallel processes and then log(M) sets of parallel merges. &nbsp;</p>
<p>Coming to the biglm package Real Soon Now.&nbsp;</p>]]></content:encoded>
    <wp:post_name>54900159212</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/54743961034</link>
    <pubDate>Sat, 06 Jul 2013 22:05:13 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/54743961034</guid>
    <!--<wp:post_id>54743961034</wp:post_id>-->
    <wp:post_date>2013-07-06 3:05:13</wp:post_date>
    <wp:post_date_gmt>2013-07-06 10:05:13</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Problems with faithfulness and the causal Markov property (II)</title>
    <description></description>
    <content:encoded><![CDATA[<p>This one I got from reading Nancy Cartwright's "<em>Hunting Causes, and using them"</em>, though it isn't exactly the point she's making. It's also related to points made by Hofstadter, Dennett, and others about reductionist reasoning.&nbsp;</p>
<p>The idea of causal graphs is that you have variables and some prior knowledge of possible causal relationships between them -- the prior knowledge could be as weak as 'future cannot cause past' or could incorporate a lot of domain-specific knowledge. &nbsp;Given enough data, you can observe conditional-independence relationships between the variables. &nbsp;Assuming that there are no conditional independence relationships beyond those implied by direct causal connections, you can often figure out the causal relationships.&nbsp;</p>
<p>The problem is that the level of explanation at which the graph assumptions are true may not be the same as the level of explanation where your variables and domain knowledge operate. &nbsp;If you measure clouds and the colour of sunsets you can predict some things about weather, but there's no reason to expect the graph assumptions to hold. If you measure temperature, pressure, and humidity at enough weather stations, the causal graph assumptions will hold, but the variables in the graph won't include things like clouds and sunsets.</p>
<p>In the same way 'poverty' or 'racism' or even 'dietary fat' or 'oxidative stress' may be too high-level a variable to satisfy simple causal relations.&nbsp;</p>]]></content:encoded>
    <wp:post_name>54743961034</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/54495541868</link>
    <pubDate>Wed, 03 Jul 2013 18:41:33 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/54495541868</guid>
    <!--<wp:post_id>54495541868</wp:post_id>-->
    <wp:post_date>2013-07-02 23:41:33</wp:post_date>
    <wp:post_date_gmt>2013-07-03 06:41:33</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Problems with faithfulness and the causal Markov property (I)</title>
    <description></description>
    <content:encoded><![CDATA[<p>The causal Markov property says that you can write down causal relationships between variables in a directed acyclic graph so that each variable is affected only by its parents in the graph. &nbsp;The faithfulness property says that the variables will have exactly the conditional independence properties required by the graph.</p>
<p>The first problem with these properties is measurement error. &nbsp;If the only causal relations are that A affects B and C, then B and C are conditionally independent given A. &nbsp;If instead of A we have A* measured with error then B and C will not be independent given A*. &nbsp;Note that 'measurement error' here is used in the statistician's sense, meaning any difference between A and A*. It includes measurement error <em>sensu stricto</em>, but also the error in representing a long-term average by a single measurement (eg, blood pressure measurement), and the error due to A* not being quite the right variable (eg, C-reactive protein as a marker for inflammation)</p>
<p></p>]]></content:encoded>
    <wp:post_name>54495541868</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/54323239710</link>
    <pubDate>Mon, 01 Jul 2013 17:48:58 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/54323239710</guid>
    <!--<wp:post_id>54323239710</wp:post_id>-->
    <wp:post_date>2013-06-30 22:48:58</wp:post_date>
    <wp:post_date_gmt>2013-07-01 05:48:58</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Upcoming talks and stuff</title>
    <description></description>
    <content:encoded><![CDATA[<p>Two modules (on intermediate and advanced R) with Ken Rice at the Seattle Summer Institute in Statistical Genetics</p>
<p>Joint Statistical Meetings: analyzing large data with SQL generated by R, with Hannes Muhleisen. MonetDB is a database optimised for analysis tasks, and controlling it from R gives more flexibility and programmability.&nbsp;</p>
<p></p>]]></content:encoded>
    <wp:post_name>54323239710</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/54012436360</link>
    <pubDate>Fri, 28 Jun 2013 01:20:36 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/54012436360</guid>
    <!--<wp:post_id>54012436360</wp:post_id>-->
    <wp:post_date>2013-06-27 6:20:36</wp:post_date>
    <wp:post_date_gmt>2013-06-27 13:20:36</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Two simple notes on 'error' in regression models</title>
    <description></description>
    <content:encoded><![CDATA[<p>1. In regression, we often talk about the difference between the population line and the observations as "errors." &nbsp;In some introductory texts these are even called "measurement errors" in Y. &nbsp;Sometimes they are errors in Y, and sometimes they are even measurement errors in Y, but much more often Y is the truth and the 'error' is the error in predicting Y by a straight line. As Dan Davies observed (from memory) <em>"The Great Depression really happened; it wasn't just an unusually inaccurate observation of an underlying 4% return on equities"</em></p>
<p>2. Why do we assume errors have zero mean? &nbsp;If 'errors' actually are measurement errors this is genuinely an assumption and could be falsified empirically. &nbsp;One example is pulmonary function measurements FEV1 and FVC, which are defined to be the maximum attainable by the individual, and so by definition do not have zero measurement error.&nbsp;More often, though, the mean of the residuals is not identifiable separately from the intercept, and we just <em>choose</em> the parametrization that has mean-zero residuals. In that situation it's not an assumption and couldn't be falsified empirically.</p>
<p></p>]]></content:encoded>
    <wp:post_name>54012436360</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/54011641155</link>
    <pubDate>Fri, 28 Jun 2013 01:01:55 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/54011641155</guid>
    <!--<wp:post_id>54011641155</wp:post_id>-->
    <wp:post_date>2013-06-27 6:01:55</wp:post_date>
    <wp:post_date_gmt>2013-06-27 13:01:55</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Where is Bayesian introductory statistics better?</title>
    <description></description>
    <content:encoded><![CDATA[<p>For the sort of statistics taught in introductory courses, competent Bayesian and frequentist analysis are going to agree -- point and interval estimates will be similar, and similar conclusions will be drawn. &nbsp;Computation isn't seriously hard for either approach, though prepackaged pointy-clicky software is more available for frequentist inference.&nbsp;</p>
<p>There are going to be pedagogical differences. &nbsp;The big one, in favour of Bayesian statistics, is not having to explain p-values. Another benefit of the Bayesian approach is that it gives you a good reason not to talk about rank tests. Against this are some disadvantages.</p>
<p>Firstly, you are more or less forced to work with simple parametric models for the data -- you could just do what we do with the t-test and ANOVA and say that group means are Normally distributed, but I've never seen this approach used. Of course, a lot of frequentist introductory courses use simple parametric models for the data, but that doesn't stop it being a somewhat dodgy idea, and it is well known to lead to students behaving as if non-Normal distributions don't really have means.&nbsp;</p>
<p>Secondly, and more importantly, it's harder to explain why random sampling and randomization are important. The reason is to do with independence between exposures and <strong>unmeasured</strong> confounders, but you usually wouldn't include the unmeasured confounders in your model at an introductory level.&nbsp;</p>
<p>In practice, though, the introductory course is a service course and the deciding issues is that the consumers wouldn't like it to be Bayesian.&nbsp;</p>]]></content:encoded>
    <wp:post_name>54011641155</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/52445567432</link>
    <pubDate>Sat, 08 Jun 2013 19:30:11 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/52445567432</guid>
    <!--<wp:post_id>52445567432</wp:post_id>-->
    <wp:post_date>2013-06-08 0:30:11</wp:post_date>
    <wp:post_date_gmt>2013-06-08 07:30:11</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>My setup</title>
    <description></description>
    <content:encoded><![CDATA[<p>For <a href="http://usesthis.com/">UsesThis.com</a>, prompted by <a href="http://www.quantumforest.com/2012/04/my-setup/">Luis Apiolaza</a>.</p>
<p><strong>Who are you, and what do you do?</strong></p>
<p>I'm a statistics professor at the University of Auckland. &nbsp;I teach, do research in statistics and in epidemiology, and contribute to <a href="www.r-project.org">R</a>.</p>
<p><strong>What hardware do you use?</strong></p>
<p>I've been using Mac laptops since 2001. I currently have an aluminium MacBook from 2009 and am waiting on delivery of an 11in MacBook Air. They are nicely solid, have reasonable keyboards, run Unix, and support the Microsoft software that my collaborators use. They are a bit expensive, but worth it.&nbsp;</p>
<p>Currently, I also use an iMac as a desktop, with the university's Linux-based cluster for serious computing, but I have also recently used Linux on a cheap white-box PC.&nbsp;</p>
<p>My new phone is from Sony, an older-generation Android phone. &nbsp;It was the least expensive phone I could find that had a good-enough display to read e-books properly. &nbsp;My previous phone was an iPhone -- at that time, the cheap competitors weren't really good enough. The phone mostly gets used for reading, Twitter, and maps.&nbsp;</p>
<p><strong>And what software?</strong></p>
<p>For statistics, R and occasionally Stata, and Stan or JAGS for Bayesian modelling. &nbsp;For scripting, usually R. For coding things that need to be fast, C (gcc) or Java. It's been 10 years since I learned a new language, so I plan to learn JavaScript on my sabbatical, and one of Julia, Haskell, or ocaml.&nbsp;</p>
<p>Text editor: the Aquamacs version of Emacs, or Apple's TextEdit, or an embedded editor (eg R.app or TeXshop)</p>
<p>Typesetting: TeXLive LaTeX, via TeXshop.&nbsp;</p>
<p>For collaboration, I use Microsoft Office even though I don't really like wordprocessors -- there are more important things to use my persuasive powers on than choice of software.&nbsp;</p>
<p>Web browser: varies by device, but preferably Chrome or Firefox.</p>
<p>For presentations: &nbsp;LaTeX/beamer or PowerPoint or Keynote. &nbsp;It tends to be PowerPoint when talking to medical researchers (when in Rome...) or trying to be funny, and LaTeX when talking to statisticians.&nbsp;</p>
<p>For posters: PowerPoint, but I really should learn Illustrator.</p>
<p>Email: various web-based systems. Currently Gmail and (unfortunately, because it's what the university provides) Outlook. &nbsp;I like the Gmail concept of keeping all your email and searching the full text when you need to find something.&nbsp;</p>
<p>I use plain paper and gel ink pens, or a whiteboard, for roughing out mathematical ideas, and the whiteboard for explaining things to students.</p>]]></content:encoded>
    <wp:post_name>52445567432</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/52369593489</link>
    <pubDate>Fri, 07 Jun 2013 21:27:19 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/52369593489</guid>
    <!--<wp:post_id>52369593489</wp:post_id>-->
    <wp:post_date>2013-06-07 2:27:19</wp:post_date>
    <wp:post_date_gmt>2013-06-07 09:27:19</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Talks in the near future</title>
    <description></description>
    <content:encoded><![CDATA[<p>"Filtering for rare variant tests" &nbsp;CHARGE consortium workshop, Rotterdam. &nbsp;Basic message: it's total count of rare alleles that matters.</p>
<p>"R: an environment for statistical computing and graphics". &nbsp;CWI, Amsterdam. &nbsp;Talking about the history, design, and applications of R to a scary computer-science audience.</p>
<p>"Testing rare DNA variants in unrelated individuals: experience from the CHARGE consortium", IARC, Lyon. &nbsp;On unidirectional and omnidirectional 'burden of mutation' tests using rare DNA variants.</p>
<p>"Analysing very large surveys with SQL generated from R" ITACOSM conference, Milan. &nbsp;Message: use <a href="http://www.monetdb.org/">MonetDB</a> for the big-data parts, controlled from R, which handles the complicated parts.&nbsp;</p>
<p>"Information criteria under complex sampling" ITACOSM conference, Milan. &nbsp;How to construct analogues of AIC and BIC for complex survey samples. &nbsp;Yes, I am aware this could be characterised as filling a much-needed gap.&nbsp;</p>]]></content:encoded>
    <wp:post_name>52369593489</wp:post_name>
  </item>
  <item>
    <link>http://notstatschat.tumblr.com/post/52369180811</link>
    <pubDate>Fri, 07 Jun 2013 21:12:00 +0000</pubDate>
    <dc:creator><![CDATA[post_author]]></dc:creator>
    <category><![CDATA[regular]]></category>
		<category domain="category" nicename="regular"><![CDATA[regular]]></category>
    <guid isPermaLink="false">http://notstatschat.tumblr.com/post/52369180811</guid>
    <!--<wp:post_id>52369180811</wp:post_id>-->
    <wp:post_date>2013-06-07 2:12:00</wp:post_date>
    <wp:post_date_gmt>2013-06-07 09:12:00</wp:post_date_gmt>
    <wp:comment_status>closed</wp:comment_status>
    <wp:ping_status>closed</wp:ping_status>
    <wp:status>publish</wp:status>
    <wp:post_parent>0</wp:post_parent>
    <wp:menu_order>0</wp:menu_order>
    <wp:post_type>post</wp:post_type>
    <wp:post_password></wp:post_password>
    <title>Hello, world</title>
    <description></description>
    <content:encoded><![CDATA[<p>This will be my not-<a href="http://www.statschat.org.nz">StatsChat</a> blog, for things that are too technical, too political, or simply not relevant to StatsChat.&nbsp;</p>]]></content:encoded>
    <wp:post_name>52369180811</wp:post_name>
  </item>
</channel>
</rss>